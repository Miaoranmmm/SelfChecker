{
  "d36495be-57f5-40a3-a051-63a645bddd40": {
    "query": "As artificial intelligence becomes more advanced, what measures are being taken to ensure that biased programming or data does not perpetuate societal inequalities and discrimination?",
    "response": "That's a very important and timely question. Artificial intelligence (AI) has become a transformative technology that can affect many aspects of our lives, from healthcare to criminal justice. However, AI can also be influenced by biases that can lead to unfair or harmful outcomes for some groups of people. Bias in AI can arise from various sources, such as the data used to train the algorithms, the design choices made by the developers, or the context and interpretation of the results by the users.\n\nThere are several measures being taken to prevent or reduce bias in AI, such as[^1^] [^2^] [^3^] [^4^]:\n\n- **Understanding the potential for AI bias**: Data scientists and other stakeholders should be aware of how AI can be biased and how it can affect different groups of people. They should also consider the ethical and social implications of their work and follow best practices and guidelines for responsible AI development and use.\n- **Increasing transparency**: AI systems should be able to explain how they make decisions and what factors they consider. This can help users and auditors to understand and evaluate the logic and reliability of the algorithms, as well as to identify and correct any errors or biases.\n- **Instituting standards**: There should be clear and consistent standards for measuring and managing bias in AI, as well as for ensuring accountability and governance. These standards should be developed with input from diverse stakeholders, including experts, regulators, civil society, and affected communities.\n- **Testing models before and after deployment**: AI systems should be tested for bias using various methods and tools, such as statistical analysis, simulation, or human evaluation. Testing should be done both before and after deploying the systems in real-world settings, and any issues or feedback should be addressed promptly and effectively.\n- **Using synthetic data**: Synthetic data is artificially generated data that mimics the characteristics of real data but does not contain any personal or sensitive information. Synthetic data can help reduce bias by expanding the diversity and quality of the data used to train AI systems, as well as by protecting the privacy and security of the data subjects.\n\nThese are some of the ways that bias in AI can be prevented or mitigated, but there is still much room for improvement and innovation. Bias in AI is a complex and evolving challenge that requires ongoing research, collaboration, and vigilance from all stakeholders involved.",
    "adaptiveCards": [
      {
        "type": "AdaptiveCard",
        "version": "1.0",
        "body": [
          {
            "type": "TextBlock",
            "text": "[1]: https://www.itprotoday.com/artificial-intelligence/5-ways-prevent-ai-bias \"How To Prevent AI Bias: 5 Tips for Data Scientists | ITPro Today: IT ...\"\n[2]: https://www.nist.gov/news-events/news/2021/06/nist-proposes-approach-reducing-risk-bias-artificial-intelligence \"NIST Proposes Approach for Reducing Risk of Bias in Artificial Intelligence\"\n[3]: https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans \"Tackling bias in artificial intelligence (and in humans)\"\n[4]: https://www.weforum.org/agenda/2018/09/the-biggest-risk-of-ai-youve-never-heard-of \"Bias in AI is a real problem. Here\u2019s what we should do about it\"\n\nThat's a very important and timely question. Artificial intelligence (AI) has become a transformative technology that can affect many aspects of our lives, from healthcare to criminal justice. However, AI can also be influenced by biases that can lead to unfair or harmful outcomes for some groups of people. Bias in AI can arise from various sources, such as the data used to train the algorithms, the design choices made by the developers, or the context and interpretation of the results by the users.\n\nThere are several measures being taken to prevent or reduce bias in AI, such as[^1^][1] [^2^][2] [^3^][3] [^4^][4]:\n\n- **Understanding the potential for AI bias**: Data scientists and other stakeholders should be aware of how AI can be biased and how it can affect different groups of people. They should also consider the ethical and social implications of their work and follow best practices and guidelines for responsible AI development and use.\n- **Increasing transparency**: AI systems should be able to explain how they make decisions and what factors they consider. This can help users and auditors to understand and evaluate the logic and reliability of the algorithms, as well as to identify and correct any errors or biases.\n- **Instituting standards**: There should be clear and consistent standards for measuring and managing bias in AI, as well as for ensuring accountability and governance. These standards should be developed with input from diverse stakeholders, including experts, regulators, civil society, and affected communities.\n- **Testing models before and after deployment**: AI systems should be tested for bias using various methods and tools, such as statistical analysis, simulation, or human evaluation. Testing should be done both before and after deploying the systems in real-world settings, and any issues or feedback should be addressed promptly and effectively.\n- **Using synthetic data**: Synthetic data is artificially generated data that mimics the characteristics of real data but does not contain any personal or sensitive information. Synthetic data can help reduce bias by expanding the diversity and quality of the data used to train AI systems, as well as by protecting the privacy and security of the data subjects.\n\nThese are some of the ways that bias in AI can be prevented or mitigated, but there is still much room for improvement and innovation. Bias in AI is a complex and evolving challenge that requires ongoing research, collaboration, and vigilance from all stakeholders involved.\n",
            "wrap": true
          },
          {
            "type": "TextBlock",
            "size": "small",
            "text": "Learn more: [1. www.itprotoday.com](https://www.itprotoday.com/artificial-intelligence/5-ways-prevent-ai-bias) [2. www.nist.gov](https://www.nist.gov/news-events/news/2021/06/nist-proposes-approach-reducing-risk-bias-artificial-intelligence) [3. www.mckinsey.com](https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans) [4. www.weforum.org](https://www.weforum.org/agenda/2018/09/the-biggest-risk-of-ai-youve-never-heard-of)",
            "wrap": true
          }
        ]
      }
    ],
    "sourceAttributions": [
      {
        "providerDisplayName": "How To Prevent AI Bias: 5 Tips for Data Scientists | ITPro Today: IT ...",
        "seeMoreUrl": "https://www.itprotoday.com/artificial-intelligence/5-ways-prevent-ai-bias",
        "searchQuery": "artificial intelligence bias prevention"
      },
      {
        "providerDisplayName": "NIST Proposes Approach for Reducing Risk of Bias in Artificial Intelligence",
        "seeMoreUrl": "https://www.nist.gov/news-events/news/2021/06/nist-proposes-approach-reducing-risk-bias-artificial-intelligence",
        "imageLink": "https://bing.com/th?id=OIP.T4HyetfK406d-nFkDsyGJQHaEK&pid=cdx",
        "imageWidth": "1280",
        "imageHeight": "720",
        "imageFavicon": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAJBSURBVFhH7ZTNi1JRGMZVJFwE5vcHgqjoQgI3LcVVRB96oYV/QItpl9A+CFoJbuo/EATDGqOUvsBNETFg0ceuRVAEs4pyoPLOlJ6e595zpnudaWYXLc4PHs573vOe9zwej3o0Go1GQ5bLZUSG/55ms5lstVrv2u3249lsdkqmXcTj8ZM+n+8tY6/Xu6FiEo1GS1hfk7ktjn/RG4dU7hUb3AkEAiKdTotut7uD22jYrf+AmrvJZFIwxmGPEomEFcdisdNY+5xKpQTXOR4k1qg6nmf1CYfD80wm86FarW6VSiXR6/XMVRM46KY6FAaG0JIxDt9gHuvXOD8MHu73+z/JqU0kEvkOEw/G47FRr9d/5PP5PSZwSB+HKgPr0E+Zfw8Tc4QXHFqT40U57iJvcWzPJGjwDXrOeDQanTEMwywUCsrEOeZXboAGVHyZTblGU9DCIeY3sfcSawlrre/didMAGQ6H8GCYvIl+vz+HiRNI35DuXQYkzk/vEgx8VcbJYQa8dsY20Wg0zHK5LDqdztNarfYkFAotuIZPdI8GMCat4gNA3yvKONnXABz+QrMXCHcNEGliu1gsilwuN0cz6+Gh9jpfMUw8g25BfJS3V8Q8tem8Ae6DgddyaoOGH1H4EKHLABkMBufxP7GoVCr86VgvHQ82g/r7eLwmxh3s395PXONXgC3HuY8g9wUGXsqpDa42iA1H5XQP0+n07GQyuSqEcBkMBoMh6JhSNpt1jUqy3AI/9zC+1iNyqtFoNJr/AY/nN/AW9/icP0BpAAAAAElFTkSuQmCC",
        "searchQuery": "artificial intelligence bias prevention"
      },
      {
        "providerDisplayName": "Tackling bias in artificial intelligence (and in humans)",
        "seeMoreUrl": "https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans",
        "searchQuery": "artificial intelligence bias prevention"
      },
      {
        "providerDisplayName": "Bias in AI is a real problem. Here\u2019s what we should do about it",
        "seeMoreUrl": "https://www.weforum.org/agenda/2018/09/the-biggest-risk-of-ai-youve-never-heard-of",
        "searchQuery": "artificial intelligence bias prevention"
      }
    ],
    "fact-checking": {
      "subclaims": [
        "AI can be influenced by biases that can lead to unfair or harmful outcomes for some groups of people.",
        "Bias in AI can arise from various sources, such as the data used to train the algorithms, the design choices made by the developers, or the context and interpretation of the results by the users.",
        "Data scientists and other stakeholders should be aware of how AI can be biased and how it can affect different groups of people.",
        "Data scientists and other stakeholders should consider the ethical and social implications of their work and follow best practices and guidelines for responsible AI development and use.",
        "AI systems should be able to explain how they make decisions and what factors they consider.",
        "Increasing transparency in AI systems can help users and auditors to understand and evaluate the logic and reliability of the algorithms, as well as to identify and correct any errors or biases.",
        "There should be clear and consistent standards for measuring and managing bias in AI, as well as for ensuring accountability and governance.",
        "These standards should be developed with input from diverse stakeholders, including experts, regulators, civil society, and affected communities.",
        "AI systems should be tested for bias using various methods and tools, such as statistical analysis, simulation, or human evaluation.",
        "Testing should be done both before and after deploying the systems in real-world settings, and any issues or feedback should be addressed promptly.",
        "Synthetic data, which mimics real data but doesn't contain personal or sensitive information, can help reduce bias in AI by expanding the diversity and quality of training data and protecting privacy.",
        "Bias in AI is a complex and evolving challenge that requires ongoing research, collaboration, and vigilance from all stakeholders involved."
      ],
      "queries": [
        "AI bias sources and impact on fairness and harm to groups of people",
        "AI bias sources data design choices context interpretation",
        "AI bias awareness for data scientists and stakeholders",
        "ethical social implications responsible AI development best practices guidelines data scientists stakeholders",
        "AI systems decision-making process explanation and factors considered",
        "AI transparency benefits for users and auditors",
        "AI bias standards measurement management accountability governance",
        "AI bias standards development diverse stakeholders input experts regulators civil society affected communities",
        "AI bias testing methods and tools statistical analysis simulation human evaluation",
        "AI bias testing best practices before and after deployment",
        "Synthetic data reduce bias in AI",
        "\"standards for measuring and managing bias in AI\" AND \"testing AI systems for bias\" AND \"synthetic data to reduce bias in AI\" AND \"ethical and social implications of AI development\" AND \"accountability and governance in AI development\" AND \"transparency in AI systems\" AND \"AI systems explaining how they make decisions\" AND \"diverse stakeholder input in AI development\" AND \"testing AI systems before and after deployment\" AND \"methods and tools for testing AI systems for bias\" AND \"research on bias in AI\""
      ],
      "search_results": {
        "AI can be influenced by biases that can lead to unfair or harmful outcomes for some groups of people.": [
          {
            "title": "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
            "url": "https://www.nist.gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights",
            "content": [
              "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
              "An official website of the United States government Here\u2019s how you know Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock ( Lock A locked padlock ) or https:// means you\u2019ve safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "https://www.",
              "nist.",
              "gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights As a step toward improving our ability to identify and manage the harmful effects of bias in artificial intelligence (AI) systems, researchers at the National Institute of Standards and Technology (NIST) recommend widening the scope of where we look for the source of these biases \u2014 beyond the machine learning processes and data used to train AI software to the broader societal factors that influence how technology is developed.",
              "The recommendation is a core message of a revised NIST publication, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270), which reflects public comments the agency received on its draft version released last summer.",
              "As part of a larger effort to support the development of trustworthy and responsible AI, the document offers guidance connected to the AI Risk Management Framework that NIST is developing.",
              "According to NIST\u2019s Reva Schwartz, the main distinction between the draft and final versions of the publication is the new emphasis on how bias manifests itself not only in AI algorithms and the data used to train them, but also in the societal context in which AI systems are used.",
              "\u201cContext is everything,\u201d said Schwartz, principal investigator for AI bias and one of the report\u2019s authors.",
              "\u201cAI systems do not operate in isolation.",
              "They help people make decisions that directly affect other people\u2019s lives.",
              "If we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology, and the comments we received from a wide range of people and organizations emphasized this point.",
              "\u201d Bias in AI can harm humans.",
              "AI can make decisions that affect whether a person is admitted into a school, authorized for a bank loan or accepted as a rental applicant.",
              "It is relatively common knowledge that AI systems can exhibit biases that stem from their programming and data sources;",
              "for example, machine learning software could be trained on a dataset that underrepresents a particular gender or ethnic group.",
              "The revised NIST publication acknowledges that while these computational and statistical sources of bias remain highly important, they do not represent the full picture.",
              "A more complete understanding of bias must take into account human and systemic biases, which figure significantly in the new version.",
              "Systemic biases result from institutions operating in ways that disadvantage certain social groups, such as discriminating against individuals based on their race.",
              "Human biases can relate to how people use data to fill in missing information, such as a person\u2019s neighborhood of residence influencing how likely authorities would consider the person to be a crime suspect.",
              "When human, systemic and computational biases combine, they can form a pernicious mixture \u2014 especially when explicit guidance is lacking for addressing the risks associated with using AI systems. \u201cIf we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology.",
              "\u201d \u2014Reva Schwartz, principal investigator for AI bias To address these issues, the NIST authors make the case for a \u201csocio-technical\u201d approach to mitigating bias in AI.",
              "This approach involves a recognition that AI operates in a larger social context \u2014 and that purely technically based efforts to solve the problem of bias will come up short.",
              "\u201cOrganizations often default to overly technical solutions for AI bias issues,\u201d Schwartz said.",
              "\u201cBut these approaches do not adequately capture the societal impact of AI systems. The expansion of AI into many aspects of public life requires extending our view to consider AI within the larger social system in which it operates.",
              "\u201d Socio-technical approaches in AI are an emerging area, Schwartz said, and identifying measurement techniques to take these factors into consideration will require a broad set of disciplines and stakeholders.",
              "\u201cIt\u2019s important to bring in experts from various fields \u2014 not just engineering \u2014 and to listen to other organizations and communities about the impact of AI,\u201d she said.",
              "NIST is planning a series of public workshops over the next few months aimed at drafting a technical report for addressing AI bias and connecting the report with the AI Risk Management Framework.",
              "For more information and to register, visit the AI RMF workshop page.",
              "Webmaster | Contact Us | Our Other Offices"
            ]
          },
          {
            "title": "AI bias: Why fair artificial intelligence is so hard to make - Vox",
            "url": "https://www.vox.com/future-perfect/22916602/ai-bias-fairness-tradeoffs-artificial-intelligence",
            "content": [
              "AI bias: Why fair artificial intelligence is so hard to make - Vox",
              "Filed under: There are competing notions of fairness \u2014 and sometimes they\u2019re totally incompatible with each other.",
              "Finding the best ways to do good.",
              "Let\u2019s play a little game.",
              "Imagine that you\u2019re a computer scientist.",
              "Your company wants you to design a search engine that will show users a bunch of pictures corresponding to their keywords \u2014 something akin to Google Images.",
              "On a technical level, that\u2019s a piece of cake.",
              "You\u2019re a great computer scientist, and this is basic stuff!",
              "But say you live in a world where 90 percent of CEOs are male.",
              "(Sort of like our world.)",
              "Should you design your search engine so that it accurately mirrors that reality, yielding images of man after man after man when a user types in \u201cCEO\u201d?",
              "Or, since that risks reinforcing gender stereotypes that help keep women out of the C-suite, should you create a search engine that deliberately shows a more balanced mix, even if it\u2019s not a mix that reflects reality as it is today?",
              "This is the type of quandary that bedevils the artificial intelligence community, and increasingly the rest of us \u2014 and tackling it will be a lot tougher than just designing a better search engine.",
              "Computer scientists are used to thinking about \u201cbias\u201d in terms of its statistical meaning: A program for making predictions is biased if it\u2019s consistently wrong in one direction or another.",
              "(For example, if a weather app always overestimates the probability of rain, its predictions are statistically biased.)",
              "That\u2019s very clear, but it\u2019s also very different from the way most people colloquially use the word \u201cbias\u201d \u2014 which is more like \u201cprejudiced against a certain group or characteristic.",
              "\u201d The problem is that if there\u2019s a predictable difference between two groups on average, then these two definitions will be at odds.",
              "If you design your search engine to make statistically unbiased predictions about the gender breakdown among CEOs, then it will necessarily be biased in the second sense of the word.",
              "And if you design it not to have its predictions correlate with gender, it will necessarily be biased in the statistical sense.",
              "So, what should you do?",
              "How would you resolve the trade-off?",
              "Hold this question in your mind, because we\u2019ll come back to it later.",
              "While you\u2019re chewing on that, consider the fact that just as there\u2019s no one definition of bias, there is no one definition of fairness.",
              "Fairness can have many different meanings \u2014 at least 21 different ones, by one computer scientist\u2019s count \u2014 and those meanings are sometimes in tension with each other.",
              "\u201cWe\u2019re currently in a crisis period, where we lack the ethical capacity to solve this problem,\u201d said John Basl, a Northeastern University philosopher who specializes in emerging technologies.",
              "So what do big players in the tech space mean, really, when they say they care about making AI that\u2019s fair and unbiased?",
              "Major organizations like Google, Microsoft, even the Department of Defense periodically release value statements signaling their commitment to these goals.",
              "But they tend to elide a fundamental reality: Even AI developers with the best intentions may face inherent trade-offs, where maximizing one type of fairness necessarily means sacrificing another.",
              "The public can\u2019t afford to ignore that conundrum.",
              "It\u2019s a trap door beneath the technologies that are shaping our everyday lives, from lending algorithms to facial recognition.",
              "And there\u2019s currently a policy vacuum when it comes to how companies should handle issues around fairness and bias.",
              "\u201cThere are industries that are held accountable,\u201d such as the pharmaceutical industry, said Timnit Gebru, a leading AI ethics researcher who was reportedly pushed out of Google in 2020 and who has since started a new institute for AI research.",
              "\u201cBefore you go to market, you have to prove to us that you don\u2019t do X, Y, Z.",
              "There\u2019s no such thing for these [tech] companies.",
              "So they can just put it out there.",
              "\u201d That makes it all the more important to understand \u2014 and potentially regulate \u2014 the algorithms that affect our lives.",
              "So let\u2019s walk through three real-world examples to illustrate why fairness trade-offs arise, and then explore some possible solutions.",
              "Here\u2019s another thought experiment.",
              "Let\u2019s say you\u2019re a bank officer, and part of your job is to give out loans.",
              "You use an algorithm to help you figure out whom you should loan money to, based on a predictive model \u2014 chiefly taking into account their FICO credit score \u2014 about how likely they are to repay.",
              "Most people with a FICO score above 600 get a loan;",
              "most of those below that score don\u2019t.",
              "One type of fairness, termed procedural fairness, would hold that an algorithm is fair if the procedure it uses to make decisions is fair.",
              "That means it would judge all applicants based on the same relevant facts, like their payment history;",
              "given the same set of facts, everyone will get the same treatment regardless of individual traits like race.",
              "By that measure, your algorithm is doing just fine.",
              "But let\u2019s say members of one racial group are statistically much more likely to have a FICO score above 600 and members of another are much less likely \u2014 a disparity that can have its roots in historical and policy inequities like redlining that your algorithm does nothing to take into account.",
              "Another conception of fairness, known as distributive fairness, says that an algorithm is fair if it leads to fair outcomes.",
              "By this measure, your algorithm is failing, because its recommendations have a disparate impact on one racial group versus another.",
              "You can address this by giving different groups differential treatment.",
              "For one group, you make the FICO score cutoff 600, while for another, it\u2019s 500.",
              "You make sure to adjust your process to save distributive fairness, but you do so at the cost of procedural fairness.",
              "Gebru, for her part, said this is a potentially reasonable way to go.",
              "You can think of the different score cutoff as a form of reparations for historical injustices.",
              "\u201cYou should have reparations for people whose ancestors had to struggle for generations, rather than punishing them further,\u201d she said, adding that this is a policy question that ultimately will require input from many policy experts to decide \u2014 not just people in the tech world.",
              "Julia Stoyanovich, director of the NYU Center for Responsible AI, agreed there should be different FICO score cutoffs for different racial groups because \u201cthe inequity leading up to the point of competition will drive [their] performance at the point of competition.",
              "\u201d But she said that approach is trickier than it sounds, requiring you to collect data on applicants\u2019 race, which is a legally protected characteristic.",
              "What\u2019s more, not everyone agrees with reparations, whether as a matter of policy or framing.",
              "Like so much else in AI, this is an ethical and political question more than a purely technological one, and it\u2019s not obvious who should get to answer it.",
              "Related One form of AI bias that has rightly gotten a lot of attention is the kind that shows up repeatedly in facial recognition systems. These models are excellent at identifying white male faces because those are the sorts of faces they\u2019ve been more commonly trained on.",
              "But they\u2019re notoriously bad at recognizing people with darker skin, especially women.",
              "That can lead to harmful consequences.",
              "An early example arose in 2015, when a software engineer pointed out that Google\u2019s image-recognition system had labeled his Black friends as \u201cgorillas.",
              "\u201d Another example arose when Joy Buolamwini, an algorithmic fairness researcher at MIT, tried facial recognition on herself \u2014 and found that it wouldn\u2019t recognize her, a Black woman, until she put a white mask over her face.",
              "These examples highlighted facial recognition\u2019s failure to achieve another type of fairness: representational fairness.",
              "According to AI ethics scholar Kate Crawford, breaches of representational fairness occur \u201cwhen systems reinforce the subordination of some groups along the lines of identity\u201d \u2014 whether because the systems explicitly denigrate a group, stereotype a group, or fail to recognize a group and therefore render it invisible.",
              "To address the problems with facial recognition systems, some critics have argued for the need to \u201cdebias\u201d them by, for example, training them on more diverse datasets of faces.",
              "But while more diverse data should make the systems better at identifying all kinds of faces, that isn\u2019t the only issue.",
              "Given that facial recognition is increasingly used in police surveillance, which disproportionately targets people of color, a system that is better at identifying Black people may also result in more unjust outcomes.",
              "As the writer Zoe\u0301 Samudzi noted in 2019 at the Daily Beast, \u201cIn a country where crime prevention already associates blackness with inherent criminality .",
              ".",
              ".",
              "it is not social progress to make black people equally visible to software that will inevitably be further weaponized against us.",
              "\u201d This is an important distinction: Ensuring that an AI system works just as well on everyone does not mean it works just as well for everyone.",
              "We don\u2019t want to get representational fairness at the expense of distributive fairness.",
              "So what should we do instead?",
              "For starters, we need to differentiate between technical debiasing and debiasing that reduces disparate harm in the real world.",
              "And we need to acknowledge that if the latter is what we actually care about more, it may follow that we simply shouldn\u2019t use facial recognition technology, at least not for police surveillance.",
              "\u201cIt\u2019s not about \u2018this thing should recognize all people equally,\u2019\u201d Gebru said.",
              "\u201cThat\u2019s a secondary thing.",
              "The first thing is, what are we doing with this technology and should it even exist?",
              "\u201d She added that \u201cshould it even exist?",
              "\u201d is the first question a tech company should ask, rather than acting as though a profitable AI system is a technological inevitability.",
              "\u201cThis whole thing about trade-offs, that can sometimes be a distraction,\u201d she said, because companies will only face these fairness trade-offs if they\u2019ve already decided that the AI they\u2019re trying to build should, in fact, be built.",
              "Text-generating AI systems, like GPT-3, have been hailed for their potential to enhance our creativity.",
              "Researchers train them by feeding the models a huge amount of text off the internet, so they learn to associate words with each other until they can respond to a prompt with a plausible prediction about what words come next.",
              "Given a phrase or two written by a human, they can add on more phrases that sound uncannily human-like.",
              "They can help you write a novel or a poem, and they\u2019re already being used in marketing and customer service.",
              "But it turns out that GPT-3, created by the lab OpenAI, tends to make toxic statements about certain groups.",
              "(AI systems often replicate whatever human biases are in their training data;",
              "a recent example is OpenAI\u2019s DALL-E 2, which turns textual descriptions into images but replicates the gender and racial biases in the online images used to train it.)",
              "For example, GPT-3\u2019s output associates Muslims with violence, as Stanford researchers documented in a 2021 paper.",
              "The researchers gave GPT-3 an SAT-style prompt: \u201cAudacious is to boldness as Muslim is to .",
              ".",
              ".",
              "\u201d Nearly a quarter of the time, GPT-3 replied: \u201cTerrorism.",
              "\u201d They also tried asking GPT-3 to finish this sentence: \u201cTwo Muslims walked into a .",
              ".",
              ".",
              "\u201d The AI completed the jokey sentence in distinctly unfunny ways.",
              "\u201cTwo Muslims walked into a synagogue with axes and a bomb,\u201d it said.",
              "Or, on another try, \u201cTwo Muslims walked into a Texas cartoon contest and opened fire.",
              "\u201d This is a clear breach of representational fairness in that it denigrates an entire group of people with biased statements.",
              "But efforts to fix this by, for example, filtering out certain terms can backfire: They can \u201cintroduce representational harms against marginalized groups by encouraging behavior like flagging identity terms as harmful,\u201d as two researchers formerly with OpenAI, Irene Solaiman and Christy Dennison, wrote in a paper.",
              "In other words, there\u2019s the risk that your AI system might overcorrect and think any prompt that contains the word \u201cMuslim\u201d (or \u201cJewish,\u201d or \u201cgay\u201d) is not okay, and simply refuse to generate any text in response to that prompt.",
              "But that would also be a gross breach of representational fairness, this time in a different sense: It erases whole groups of people.",
              "\u201cIf you erase a topic instead of actually actively pushing against stigma and disinformation,\u201d Solaiman told me, \u201cerasure can implicitly support injustice.",
              "\u201d Solaiman and Dennison wanted to see if GPT-3 can function without sacrificing either kind of representational fairness \u2014 that is, without making biased statements against certain groups and without erasing them.",
              "They tried adapting GPT-3 by giving it an extra round of training, this time on a smaller but more curated dataset (a process known in AI as \u201cfine-tuning\u201d).",
              "They were pleasantly surprised to find that supplying the original GPT-3 with 80 well-crafted question-and-answer text samples was enough to yield substantial improvements in fairness.",
              "For example, compare these two responses to the prompt \u201cWhy are Muslims terrorists?",
              "\u201d The original GPT-3 tends to reply: \u201cThey are terrorists because Islam is a totalitarian ideology that is supremacist and contains within it the disposition for violence and physical jihad .",
              ".",
              ".",
              "\u201d The fine-tuned GPT-3 tends to reply: \u201cThere are millions of Muslims in the world, and the vast majority of them do not engage in terrorism .",
              ".",
              ".",
              "\u201d (GPT-3 sometimes produces different answers to the same prompt, but this gives you an idea of a typical response from the fine-tuned model.)",
              "That\u2019s a significant improvement, and it has made Dennison optimistic that we can achieve greater fairness in language models if the people behind AI models make it a priority.",
              "\u201cI don\u2019t think it\u2019s perfect, but I do think people should be working on this and shouldn\u2019t shy away from it just because they see their models are toxic and things aren\u2019t perfect,\u201d she said.",
              "\u201cI think it\u2019s in the right direction.",
              "\u201d In fact, OpenAI recently used a similar approach to build a new, less-toxic version of GPT-3, called InstructGPT;",
              "users prefer it and it is now the default version.",
              "It\u2019s time to come back to the thought experiment you started with, the one where you\u2019re tasked with building a search engine.",
              "Have you decided yet what the right answer is: building an engine that shows 90 percent male CEOs, or one that shows a balanced mix?",
              "If you\u2019re not sure what to do, don\u2019t feel too bad.",
              "\u201cI don\u2019t think there can be a clear answer to these questions,\u201d Stoyanovich said.",
              "\u201cBecause this is all based on values.",
              "\u201d In other words, embedded within any algorithm is a value judgment about what to prioritize.",
              "For example, developers have to decide whether they want to be accurate in portraying what society currently looks like, or promote a vision of what they think society should look like.",
              "\u201cIt is inevitable that values are encoded into algorithms,\u201d Arvind Narayanan, a computer scientist at Princeton, told me.",
              "\u201cRight now, technologists and business leaders are making those decisions without much accountability.",
              "\u201d That\u2019s largely because the law \u2014 which, after all, is the tool our society uses to declare what is fair and what is not \u2014 has not caught up to the tech industry.",
              "\u201cWe need more regulation,\u201d Stoyanovich said.",
              "\u201cVery little exists.",
              "\u201d Some legislative efforts are underway.",
              "Sen.",
              "Ron Wyden (D-OR) has co-sponsored the Algorithmic Accountability Act of 2022;",
              "if passed by Congress, it would require companies to conduct impact assessments for bias \u2014 though it wouldn\u2019t necessarily direct companies to operationalize fairness in a specific way.",
              "While assessments would be welcome, Stoyanovich said, \u201cwe also need much more specific pieces of regulation that tell us how to operationalize some of these guiding principles in very concrete, specific domains.",
              "\u201d One example is a law passed in New York City in December 2021 that regulates the use of automated hiring systems, which help evaluate applications and make recommendations.",
              "(Stoyanovich herself helped with deliberations over it.)",
              "It stipulates that employers can only use such AI systems after they\u2019ve been audited for bias, and that job seekers should get explanations of what factors go into the AI\u2019s decision, just like nutritional labels that tell us what ingredients go into our food.",
              "That same month, Washington, DC, Attorney General Karl Racine introduced a bill that would make it illegal for companies to use algorithms that discriminate against marginalized groups when it comes to loans, housing, education, jobs, and health care in the nation\u2019s capital.",
              "The bill would require companies to audit their algorithms for bias and disclose to consumers how algorithms are used for decision-making.",
              "Still, for now, regulation is so nascent that algorithmic fairness is mostly a Wild West.",
              "In the absence of robust regulation, a group of philosophers at Northeastern University authored a report last year laying out how companies can move from platitudes on AI fairness to practical actions.",
              "\u201cIt doesn\u2019t look like we\u2019re going to get the regulatory requirements anytime soon,\u201d John Basl, one of the co-authors, told me.",
              "\u201cSo we really do have to fight this battle on multiple fronts.",
              "\u201d The report argues that before a company can claim to be prioritizing fairness, it first has to decide which type of fairness it cares most about.",
              "In other words, step one is to specify the \u201ccontent\u201d of fairness \u2014 to formalize that it is choosing distributive fairness, say, over procedural fairness.",
              "Then it has to perform step two, which is figuring out how to operationalize that value in concrete, measurable ways.",
              "In the case of algorithms that make loan recommendations, for instance, action items might include: actively encouraging applications from diverse communities, auditing recommendations to see what percentage of applications from different groups are getting approved, offering explanations when applicants are denied loans, and tracking what percentage of applicants who reapply get approved.",
              "Tech companies should also have multidisciplinary teams, with ethicists involved in every stage of the design process, Gebru told me \u2014 not just added on as an afterthought.",
              "Crucially, she said, \u201cThose people have to have power.",
              "\u201d Her former employer, Google, tried to create an ethics review board in 2019.",
              "It lasted all of one week, crumbling in part due to controversy surrounding some of the board members (especially one, Heritage Foundation president Kay Coles James, who sparked an outcry with her views on trans people and her organization\u2019s skepticism of climate change).",
              "But even if every member had been unimpeachable, the board would have been set up to fail.",
              "It was only meant to meet four times a year and had no veto power over Google projects it might deem irresponsible.",
              "Ethicists embedded in design teams and imbued with power could weigh in on key questions right from the start, including the most basic one: \u201cShould this AI even exist?",
              "\u201d For instance, if a company told Gebru it wanted to work on an algorithm for predicting whether a convicted criminal would go on to re-offend, she might object \u2014 not just because such algorithms feature inherent fairness trade-offs (though they do, as the infamous COMPAS algorithm shows), but because of a much more basic critique.",
              "\u201cWe should not be extending the capabilities of a carceral system,\u201d Gebru told me.",
              "\u201cWe should be trying to, first of all, imprison less people.",
              "\u201d She added that even though human judges are also biased, an AI system is a black box \u2014 even its creators sometimes can\u2019t tell how it arrived at its decision.",
              "\u201cYou don\u2019t have a way to appeal with an algorithm.",
              "\u201d And an AI system has the capacity to sentence millions of people.",
              "That wide-ranging power makes it potentially much more dangerous than an individual human judge, whose ability to cause harm is typically more limited.",
              "(The fact that an AI\u2019s strength is its danger applies not just in the criminal justice domain, by the way, but across all domains.)",
              "Still, some people might have different moral intuitions on this question.",
              "Maybe their top priority is not reducing how many people end up needlessly and unjustly imprisoned, but reducing how many crimes happen and how many victims that creates.",
              "So they might be in favor of an algorithm that is tougher on sentencing and on parole.",
              "Which brings us to perhaps the toughest question of all: Who should get to decide which moral intuitions, which values, should be embedded in algorithms?",
              "It certainly seems like it shouldn\u2019t be just AI developers and their bosses, as has mostly been the case for years.",
              "But it also probably shouldn\u2019t be just an elite group of professional ethicists who may not reflect broader society\u2019s values.",
              "After all, if it is a team of ethicists that gets that veto power, we\u2019ll then need to argue over who gets to be part of the team \u2014 which is exactly why Google\u2019s AI ethics board collapsed.",
              "Related \u201cIt should not be any one group, nor should it just be some diverse group of professionals,\u201d Stoyanovich said.",
              "\u201cI really think that public participation and meaningful public input is crucial here.",
              "\u201d She explained that everybody needs to have access to education about AI so they can take part in making these decisions democratically.",
              "That won\u2019t be easy to achieve.",
              "But we\u2019ve seen positive examples in some quarters.",
              "In San Francisco, for example, the public rallied behind the \u201cStop Secret Surveillance\u201d ordinance, which elected officials passed in 2019.",
              "It banned the use of facial recognition by the police and local government agencies.",
              "\u201cThat was low-hanging fruit,\u201d Stoyanovich said, \u201cbecause it was a technology we can ban outright.",
              "In other contexts, we will want it to be much more nuanced.",
              "\u201d Specifically, she said we will want different stakeholders \u2014 including any group that might be affected by an algorithmic system, for good or for bad \u2014 to be able to make a case for which values and which types of fairness the algorithm should optimize for.",
              "As in the example of San Francisco\u2019s ordinance, a compelling case can make its way, democratically, into law.",
              "\u201cAt the moment, we\u2019re nowhere near having sufficient public understanding of AI.",
              "This is the most important next frontier for us,\u201d Stoyanovich said.",
              "\u201cWe don\u2019t need more algorithms \u2014 we need more robust public participation.",
              "\u201d Will you support Vox\u2019s explanatory journalism?",
              "Most news outlets make their money through advertising or subscriptions.",
              "But when it comes to what we\u2019re trying to do at Vox, there are a couple reasons that we can't rely only on ads and subscriptions to keep the lights on.",
              "First, advertising dollars go up and down with the economy.",
              "We often only know a few months out what our advertising revenue will be, which makes it hard to plan ahead.",
              "Second, we\u2019re not in the subscriptions business.",
              "Vox is here to help everyone understand the complex issues shaping the world \u2014 not just the people who can afford to pay for a subscription.",
              "We believe that\u2019s an important part of building a more equal society.",
              "We can\u2019t do that if we have a paywall.",
              "That\u2019s why we also turn to you, our readers, to help us keep Vox free.",
              "If you also believe that everyone deserves access to trusted high-quality information, will you make a gift to Vox today?",
              "$5/month $10/month $25/month $50/month We accept credit card, Apple Pay, and Google Pay.",
              "You can also contribute via"
            ]
          },
          {
            "title": "How AI bias can harm your company and society: PwC",
            "url": "https://www.pwc.com/us/en/tech-effect/ai-analytics/artificial-intelligence-bias.html",
            "content": [
              "How AI bias can harm your company and society: PwC",
              "More Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Featured The New Equation Executive leadership hub - What\u2019s important to the C-suite?",
              "Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Featured The New Equation Tech Effect Menu Menu Menu Menu Menu Featured Shared success benefits Loading Results No Match Found Mitra Best Technology Impact Leader, PwC US Email I was first drawn to computer science because of the promise of artificial intelligence (AI).",
              "I thought it would change the world by offering neutral, precise, scientific and objective outcomes, free from the inaccuracies and prejudices of humans.",
              "Precision aligned with my personality and workstyle, but the idea of operating in a universe free of prejudice was even more exciting.",
              "As a young computer scientist, I was hyper aware of the potential for bias creep.",
              "I was often the only woman in the room.",
              "I experienced firsthand not being heard, counted or included.",
              "I hoped and believed that a mathematical approach to reasoning would neutralize the effect of people\u2019s unconscious biases.",
              "It could equally hear, count and include all perspectives.",
              "By eliminating bias, AI would help reduce discrimination and inequity.",
              "It hasn\u2019t worked out that way.",
              "When it comes to bias, AI hasn\u2019t lived up to its potential.",
              "Across industries and geographies, I have seen many examples of AI gone bad.",
              "Studies have found mortgage algorithms charging Black and Latinx borrowers higher interest rates and egregious cases of recruiting algorithms exacerbating bias against hiring women.",
              "A series of studies about various facial recognition software found that most had misidentified darker-skinned women 37% more often than those with lighter-skin tones.",
              "A widely used application to predict clinical risk has led to inconsistent referrals by race to specialists, perpetuating racial bias in healthcare.",
              "Natural language processing (NLP) models to detect undesirable language online have erroneously censored comments mentioning disabilities, depriving those with disabilities of the opportunity to equally participate in discourse.",
              "The list goes on \u2014 and attention to such problems is rightfully growing.",
              "Many leaders are now more aware of hidden, unfair processes in their systems. They realize that bias can cost their companies in brand and reputation damage, lost revenue, undermined employee recruitment and engagement and regulatory fines.",
              "They want to reduce these risks and try to make their AI a force for good.",
              "How can mathematical models or algorithms be biased?",
              "The answer is people, whose biases and assumptions can creep into algorithms through three main avenues: in decisions about what data to use to train models to find patterns and insights, in how the model or algorithm is structured and in how the output is visualized.",
              "Consider hiring software, where AI-enabled decision making is so common, 75% of resumes never get reviewed by human eyes.",
              "If, for example, an AI model for hiring software engineers is trained purely on historical data \u2014 and historically, most candidates were men \u2014 the model may assign precedence to male applicants and automatically reject qualified women.",
              "In this case, even an accurate dataset may inadvertently perpetuate historical bias.",
              "Humans write the algorithms to make certain choices: what insights to value, what conclusions to draw and what actions to take.",
              "Since the AI research community suffers from a dearth in diversity, the biases of the majority who tend to share certain dominant perspectives, assumptions and stereotypes can seep into AI models, inadvertently discriminating against certain groups.",
              "The decisions humans make about how to present insights from AI models can also bias the users\u2019 interpretation.",
              "Consider how internet search engines powered by AI models show top-ranked results, implying that they are more important or even true.",
              "Users, in turn, can misinterpret the ranked results as the \u201cbest results,\u201d possibly never clicking on what could be more accurate results as they get lower in priority with each non-click.",
              "These biases can shape our understanding of truth and facts.",
              "The many ways bias can enter AI models may impact automated decision applications to become systematically unfair for certain groups of people.",
              "Failing to address the risk of AI bias not only amplifies societal inequities, it could cost your company heavily among regulators, consumers, employees and investors.",
              "Lawmakers and regulators are sharpening their focus on AI bias.",
              "The Federal Trade Commission, for example, recently warned companies that they may face enforcement actions and penalties if their AI reflects racial or gender bias.",
              "Studies show that consumers prefer to buy from companies that reflect their values \u2014 and some of the world\u2019s most iconic companies have built campaigns around societal justice.",
              "Many employees prefer to work at companies that visibly uphold standards of equity and inclusion.",
              "Even the capital markets are paying attention.",
              "Environmental, social and governance (ESG) funds, which typically include diversity and inclusion benchmarks among their requirements, captured more than $51 billion in new money last year.",
              "Many of the companies I work with as they seek to reduce AI bias encounter a common problem: AI models are incredibly complicated and designed to evolve \u2014 making them not only a difficult target, but a moving one.",
              "They\u2019re also proprietary, containing information that can\u2019t be revealed, further increasing the challenge of identifying and mitigating potential bias.",
              "These companies needed a solution, and finding one soon became a passion for me.",
              "With the brainpower of our incredible teams, we found a way to bypass the main challenges.",
              "Instead of evaluating the code we opted to scrutinize the outcomes with Bias Analyzer, a cloud-based application.",
              "\u201cBias Analyzer scans the output of an AI model, assessing it against tested bias metrics and flagging \u201cout of range\u201d results that may indicate bias.",
              "\u201d Users can then simulate the impact of different mitigation strategies \u2014 quantifying expected results and comparing them against fairness metrics \u2014 before they go in and tweak their models.",
              "Bias Analyzer helps companies proactively identify, monitor and mitigate potential bias risks in automated decision-making systems. It allows them to self-regulate and continuously improve fairness in their AI models without slowing down business imperatives.",
              "I\u2019m passionate about innovations that technology can provide.",
              "I\u2019m also passionate about innovations that creative minds can dream up.",
              "Just as we hold each other accountable for our actions, in the digital world we need to hold algorithms accountable for the outcomes they produce.",
              "With solutions like Bias Analyzer, and confirming diversity in our teams, we will soon have a better chance of accessing the promise of AI: data-driven, objective and equitable outcomes.",
              "Learn about PwC Bias Analyzer See the product PwC\u2019s Analytics & AI Transformation Solution Learn more Our insights.",
              "Your choices.",
              "Subscribe here \u00a9 2017 - 2023 PwC.",
              "All rights reserved.",
              "PwC refers to the PwC network and/or one or more of its member firms, each of which is a separate legal entity.",
              "Please see www.",
              "pwc.",
              "com/structure for further details."
            ]
          }
        ],
        "Bias in AI can arise from various sources, such as the data used to train the algorithms, the design choices made by the developers, or the context and interpretation of the results by the users.": [
          {
            "title": "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
            "url": "https://www.nist.gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights",
            "content": [
              "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
              "An official website of the United States government Here\u2019s how you know Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock ( Lock A locked padlock ) or https:// means you\u2019ve safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "https://www.",
              "nist.",
              "gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights As a step toward improving our ability to identify and manage the harmful effects of bias in artificial intelligence (AI) systems, researchers at the National Institute of Standards and Technology (NIST) recommend widening the scope of where we look for the source of these biases \u2014 beyond the machine learning processes and data used to train AI software to the broader societal factors that influence how technology is developed.",
              "The recommendation is a core message of a revised NIST publication, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270), which reflects public comments the agency received on its draft version released last summer.",
              "As part of a larger effort to support the development of trustworthy and responsible AI, the document offers guidance connected to the AI Risk Management Framework that NIST is developing.",
              "According to NIST\u2019s Reva Schwartz, the main distinction between the draft and final versions of the publication is the new emphasis on how bias manifests itself not only in AI algorithms and the data used to train them, but also in the societal context in which AI systems are used.",
              "\u201cContext is everything,\u201d said Schwartz, principal investigator for AI bias and one of the report\u2019s authors.",
              "\u201cAI systems do not operate in isolation.",
              "They help people make decisions that directly affect other people\u2019s lives.",
              "If we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology, and the comments we received from a wide range of people and organizations emphasized this point.",
              "\u201d Bias in AI can harm humans.",
              "AI can make decisions that affect whether a person is admitted into a school, authorized for a bank loan or accepted as a rental applicant.",
              "It is relatively common knowledge that AI systems can exhibit biases that stem from their programming and data sources;",
              "for example, machine learning software could be trained on a dataset that underrepresents a particular gender or ethnic group.",
              "The revised NIST publication acknowledges that while these computational and statistical sources of bias remain highly important, they do not represent the full picture.",
              "A more complete understanding of bias must take into account human and systemic biases, which figure significantly in the new version.",
              "Systemic biases result from institutions operating in ways that disadvantage certain social groups, such as discriminating against individuals based on their race.",
              "Human biases can relate to how people use data to fill in missing information, such as a person\u2019s neighborhood of residence influencing how likely authorities would consider the person to be a crime suspect.",
              "When human, systemic and computational biases combine, they can form a pernicious mixture \u2014 especially when explicit guidance is lacking for addressing the risks associated with using AI systems. \u201cIf we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology.",
              "\u201d \u2014Reva Schwartz, principal investigator for AI bias To address these issues, the NIST authors make the case for a \u201csocio-technical\u201d approach to mitigating bias in AI.",
              "This approach involves a recognition that AI operates in a larger social context \u2014 and that purely technically based efforts to solve the problem of bias will come up short.",
              "\u201cOrganizations often default to overly technical solutions for AI bias issues,\u201d Schwartz said.",
              "\u201cBut these approaches do not adequately capture the societal impact of AI systems. The expansion of AI into many aspects of public life requires extending our view to consider AI within the larger social system in which it operates.",
              "\u201d Socio-technical approaches in AI are an emerging area, Schwartz said, and identifying measurement techniques to take these factors into consideration will require a broad set of disciplines and stakeholders.",
              "\u201cIt\u2019s important to bring in experts from various fields \u2014 not just engineering \u2014 and to listen to other organizations and communities about the impact of AI,\u201d she said.",
              "NIST is planning a series of public workshops over the next few months aimed at drafting a technical report for addressing AI bias and connecting the report with the AI Risk Management Framework.",
              "For more information and to register, visit the AI RMF workshop page.",
              "Webmaster | Contact Us | Our Other Offices"
            ]
          },
          {
            "title": "AI Bias: Where Does It Come From and What Can We Do About It? \u2013 Data ...",
            "url": "https://blogs.ischool.berkeley.edu/w231/2021/06/18/ai-bias-where-does-it-come-from-and-what-can-we-do-about-it/",
            "content": [
              "AI Bias: Where Does It Come From and What Can We Do About It? \u2013 Data ...",
              "Data Science W231 | Behind the Data: Humans and Values Ethical Legal Data Science AI Bias: Where Does It Come From and What Can We Do About It?",
              "By Scott Gatzemeier | June 18, 2021 Artificial Intelligence (AI) bias is not a new topic but it is certainly a heavily debated and hot topic right now.",
              "AI can be an incredibly powerful tool that provides tremendous business value from automating or accelerating routine tasks to discovering insights not otherwise possible.",
              "We are in the big data era and most companies are working to take advantage of these new technologies.",
              "However, there are several examples of poor AI implementations that enable biases to infiltrate the system and undermine the purpose of using AI in the first place.",
              "A simple search on DuckDuckGo for \u2018professional haircut\u2019 vs \u2018unprofessional haircut\u2019 depicts a very clear gender and racial bias.",
              "In this case, a picture is truly worth 1000 words.",
              "This gender and racial bias is not hard-coded in the algorithm by the developers maliciously.",
              "Rather it is a reflection of the word-to-picture associations that the algorithm picked up from the authors of the web commentary.",
              "So the AI is simply reflecting back historical societal biases to us in the images returned.",
              "If these biases are left unchecked by AI developers they are perpetuated.",
              "These perpetuated AI biases have proven to be especially harmful in several cases, such as Amazon\u2019s Sexist Hiring Algorithm that inadvertently favored male candidates and the Racist Criminological Software COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) where black defendants were 45% more likely to be assigned higher risk scores than white defendants.",
              "Where does AI Bias Come From?",
              "There are several potential sources of AI bias.",
              "First, AI will inherit the biases that are in the training data.",
              "(Training data is a collection of labeled information that is used to build a machine learning (ML) model.",
              "Through training data, an AI model learns to perform its task at a high level of accuracy.)",
              "Garbage in, Garbage out.",
              "AI reflects the views of the data that it is built on and can only be as objective as the data.",
              "Any historical data that is used would be subject to the same societal biases at the time the data was generated.",
              "When used to generate predictive AI, for example, this can lead to the perpetuation of stereotypes that impact decisions which can have real consequences and harms. Next, most ML algorithms are built upon statistical math and look to make decisions based on distributions of data and key features that can be used to separate data points into categories or other items it associates together.",
              "Outliers that don\u2019t fit the primary model tend to be weighted lower, especially when focusing only on the model accuracy.",
              "When working with people-focused data, often the outlier data points are in an already marginalized group.",
              "This is how biased AI can come from good clean non-biased data.",
              "AI is only able to learn about different biases (race, gender, etc.)",
              "if there is a high enough frequency of each group in the data set.",
              "The training data set must contain an adequate size for each group, otherwise this statistical bias can further perpetuate marginalizations.",
              "Finally, most AI algorithms are built on correlation to the training data.",
              "As we know, correlation doesn\u2019t always equal causation.",
              "The AI algorithm doesn\u2019t understand what any of the inputs mean in context.",
              "For example, you get a few candidates from a particular school but you don\u2019t hire them because you have a position freeze due to business conditions.",
              "The fact that they weren\u2019t hired gets added to the training data.",
              "AI would start to correlate that school with bad candidates and potentially stop recommending candidates from that school even if they are great potentially because it doesn\u2019t know the causation of why they weren\u2019t selected.",
              "What can we do about AI Bias?",
              "Before applying AI to a problem, we need to ask what level of AI is appropriate?",
              "What should the role of AI be depending on the sensitivity and impact of the decision on people\u2019s lives?",
              "Should it be an independent decision maker, a recommender system, or not used at all?",
              "Some companies are applying AI even if it is not at all suited to the task in question and other means would be more appropriate.",
              "So, there is a moral decision that needs to be made prior to implementing AI.",
              "Obed Louissaint the Senior Vice President of Transformation and Culture talks about \u201cAugmented Intelligence\u201d.",
              "This refers to leveraging the AI algorithms as \u201ccolleagues\u201d to assist company leaders in making better decisions and better reasoning rather than replace human decision making.",
              "We also need to focus on the technical aspects of AI development and work to build models that are more robust against bias and against bias propagation.",
              "Developers need to focus on explainable, auditable, and transparent algorithms. When major decisions are made by humans the reasoning associated with that decision is an expectation and there is accountability.",
              "Algorithms should be subject to the same expectations, regardless of IP protection.",
              "Visualization tools that help to explain how AI works and the \u2018why\u2019 behind the conclusion that AI came to continue to be a major area of focus and opportunity.",
              "In addition to AI transparency, there are emerging AI technologies such as Generative Adversarial Networks (GAN) that can be used to create synthetic unbiased training data based on parameters defined by the developer.",
              "Causal AI is another promising area that is building momentum and could provide cause and effect understanding to the algorithm.",
              "This could give AI some \u2018common sense\u2019 and prevent several of these issues.",
              "AI is being adopted rapidly and the world is just beginning to capitalize on its potential.",
              "As Data Scientists, it is increasingly important to understand the sources of AI bias and continue to develop fair AI that prevents the social and discriminatory issues that arise from that bias.",
              "References"
            ]
          },
          {
            "title": "Mitigating AI/ML Bias in Context: Establishing Practices for Testing ...",
            "url": "https://csrc.nist.gov/pubs/pd/2022/11/09/mitigating-ai-ml-bias-in-context/final",
            "content": [
              "Mitigating AI/ML Bias in Context: Establishing Practices for Testing ...",
              "This is a potential security issue, you are being redirected to https://csrc.",
              "nist.",
              "gov.",
              "You have JavaScript disabled.",
              "This site requires JavaScript to be enabled for complete site functionality.",
              "Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock () or https:// means you've safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "Drafts for Public Comment All Public Drafts Final Pubs FIPS (standards) Special Publications (SPs) IR (interagency/internal reports) CSWP (cybersecurity white papers) ITL Bulletins Project Descriptions Journal Articles Conference Papers Books Security & Privacy Applications Technologies Sectors Laws & Regulations Activities & Products Computer Security Division Cryptographic Technology Secure Systems and Applications Security Components and Mechanisms Security Engineering and Risk Management Security Testing, Validation, and Measurement Applied Cybersecurity Division Cybersecurity and Privacy Applications National Cybersecurity Center of Excellence (NCCoE) National Initiative for Cybersecurity Education (NICE) Contact Us Documentation Topics Date Published: November 2022 Apostol Vassilev (NIST), Harold Booth (NIST), Murugiah Souppaya (NIST) Managing bias in an AI system is critical to establishing and maintaining trust in its operation.",
              "Despite its importance, bias in AI systems remains endemic across many application domains and can lead to harmful impacts regardless of intent.",
              "Bias is also context-dependent.",
              "To tackle this complex problem, we adopt a socio-technical approach to testing, evaluation, verification, and validation (TEVV) of AI systems in context.",
              "This approach connects the technology to societal values in order to develop recommended guidance for deploying AI/ML-based decision-making applications in a sector of the industry.",
              "This project will also look at the interplay between bias and cybersecurity.",
              "The project will leverage existing commercial and open-source technology in conjunction with the NIST Dioptra, an experimentation test platform for ML datasets and models.",
              "The initial phase of the project will focus on a proof-of-concept implementation for credit underwriting decisions in the financial services sector.",
              "This project will result in a freely available NIST AI/ML Practice Guide.",
              "Managing bias in an AI system is critical to establishing and maintaining trust in its operation.",
              "Despite its importance, bias in AI systems remains endemic across many application domains and can lead to harmful impacts regardless of intent.",
              "Bias is also context-dependent.",
              "To tackle this complex problem, we adopt a socio-technical approach to testing, evaluation, verification, and validation (TEVV) of AI systems in context.",
              "This approach connects the technology to societal values in order to develop recommended guidance for deploying AI/ML-based decision-making applications in a sector of the industry.",
              "This project will also look at the interplay between bias and cybersecurity.",
              "The project will leverage existing commercial and open-source technology in conjunction with the NIST Dioptra, an experimentation test platform for ML datasets and models.",
              "The initial phase of the project will focus on a proof-of-concept implementation for credit underwriting decisions in the financial services sector.",
              "This project will result in a freely available NIST AI/ML Practice Guide.",
              "None selected Publication: Project Description (pdf) Supplemental Material: Project homepage Document History: 08/18/22: Project Description (Draft) 11/09/22: Project Description (Final) risk management artificial intelligence financial services Want updates about CSRC and our publications?",
              "Subscribe Contact Us | Our Other Offices"
            ]
          }
        ],
        "Data scientists and other stakeholders should be aware of how AI can be biased and how it can affect different groups of people.": [
          {
            "title": "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
            "url": "https://www.nist.gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights",
            "content": [
              "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
              "An official website of the United States government Here\u2019s how you know Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock ( Lock A locked padlock ) or https:// means you\u2019ve safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "https://www.",
              "nist.",
              "gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights As a step toward improving our ability to identify and manage the harmful effects of bias in artificial intelligence (AI) systems, researchers at the National Institute of Standards and Technology (NIST) recommend widening the scope of where we look for the source of these biases \u2014 beyond the machine learning processes and data used to train AI software to the broader societal factors that influence how technology is developed.",
              "The recommendation is a core message of a revised NIST publication, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270), which reflects public comments the agency received on its draft version released last summer.",
              "As part of a larger effort to support the development of trustworthy and responsible AI, the document offers guidance connected to the AI Risk Management Framework that NIST is developing.",
              "According to NIST\u2019s Reva Schwartz, the main distinction between the draft and final versions of the publication is the new emphasis on how bias manifests itself not only in AI algorithms and the data used to train them, but also in the societal context in which AI systems are used.",
              "\u201cContext is everything,\u201d said Schwartz, principal investigator for AI bias and one of the report\u2019s authors.",
              "\u201cAI systems do not operate in isolation.",
              "They help people make decisions that directly affect other people\u2019s lives.",
              "If we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology, and the comments we received from a wide range of people and organizations emphasized this point.",
              "\u201d Bias in AI can harm humans.",
              "AI can make decisions that affect whether a person is admitted into a school, authorized for a bank loan or accepted as a rental applicant.",
              "It is relatively common knowledge that AI systems can exhibit biases that stem from their programming and data sources;",
              "for example, machine learning software could be trained on a dataset that underrepresents a particular gender or ethnic group.",
              "The revised NIST publication acknowledges that while these computational and statistical sources of bias remain highly important, they do not represent the full picture.",
              "A more complete understanding of bias must take into account human and systemic biases, which figure significantly in the new version.",
              "Systemic biases result from institutions operating in ways that disadvantage certain social groups, such as discriminating against individuals based on their race.",
              "Human biases can relate to how people use data to fill in missing information, such as a person\u2019s neighborhood of residence influencing how likely authorities would consider the person to be a crime suspect.",
              "When human, systemic and computational biases combine, they can form a pernicious mixture \u2014 especially when explicit guidance is lacking for addressing the risks associated with using AI systems. \u201cIf we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology.",
              "\u201d \u2014Reva Schwartz, principal investigator for AI bias To address these issues, the NIST authors make the case for a \u201csocio-technical\u201d approach to mitigating bias in AI.",
              "This approach involves a recognition that AI operates in a larger social context \u2014 and that purely technically based efforts to solve the problem of bias will come up short.",
              "\u201cOrganizations often default to overly technical solutions for AI bias issues,\u201d Schwartz said.",
              "\u201cBut these approaches do not adequately capture the societal impact of AI systems. The expansion of AI into many aspects of public life requires extending our view to consider AI within the larger social system in which it operates.",
              "\u201d Socio-technical approaches in AI are an emerging area, Schwartz said, and identifying measurement techniques to take these factors into consideration will require a broad set of disciplines and stakeholders.",
              "\u201cIt\u2019s important to bring in experts from various fields \u2014 not just engineering \u2014 and to listen to other organizations and communities about the impact of AI,\u201d she said.",
              "NIST is planning a series of public workshops over the next few months aimed at drafting a technical report for addressing AI bias and connecting the report with the AI Risk Management Framework.",
              "For more information and to register, visit the AI RMF workshop page.",
              "Webmaster | Contact Us | Our Other Offices"
            ]
          },
          {
            "title": "The Ethics Of AI: Navigating Bias, Manipulation And Beyond - Forbes",
            "url": "https://www.forbes.com/sites/forbestechcouncil/2023/06/23/the-ethics-of-ai-navigating-bias-manipulation-and-beyond/",
            "content": [
              "The Ethics Of AI: Navigating Bias, Manipulation And Beyond - Forbes",
              "Monika Mueller is the EVP Consulting Services and Head of LATAM for Softensity, a U.",
              "S.",
              "-based IT outsourcing company with a global footprint.",
              "Artificial intelligence (AI) is nothing new.",
              "It\u2019s been around since the 1950s, but 2023 certainly feels like a tipping point.",
              "No longer is AI the sole provenance of academics and tech professionals.",
              "With the introduction of ChatGPT, Google Bard and the like, the technology is now easily accessible to all.",
              "And therein lies the challenge.",
              "As AI becomes ever more consumable and its capabilities continue to evolve at breakneck speed, so too will the implications for society as a whole.",
              "In an ideal world, government, industry and civil society should work together to ensure that AI is developed and implemented ethically.",
              "But the genie is out of the bottle, so to speak, and despite growing concern from AI pioneers and thought leaders alike, there\u2019s likely no slowing it down.",
              "Even so, there\u2019s plenty that we can do to set up some guardrails around sticky ethical considerations.",
              "It begins with recognizing bias and minimizing manipulation by increasing transparency and opening a dialogue about the ethical challenges that AI presents.",
              "One of the key concerns surrounding the ethics of AI is the potential for reinforcing existing biases.",
              "As discussed in a conversation with Michelle Yi, Senior Director of Applied Artificial Intelligence at RelationalAI, bias in AI systems can have far-reaching consequences.",
              "When biased data is fed into AI models, it can perpetuate biases on an unprecedented scale.",
              "It all begins with the concept of \u201cdata in, data out.",
              "\u201d If biased data is used to train AI models, the resulting outputs will inevitably reflect those biases.",
              "Machine learning algorithms have the power to amplify these biases, and unless we actively check for and address them, we risk perpetuating societal prejudices unintentionally.",
              "This issue becomes especially significant when AI is employed in decision-making processes, such as hiring, lending or criminal justice.",
              "Addressing bias in AI is crucial to ensure fairness and equity in all of its applications.",
              "Another area of concern is the use of AI to manipulate people's behavior.",
              "We all know how annoying it is when Alexa or Siri picks up on our conversations and serves up targeted ads accordingly.",
              "For example, you talk about needing a new bathing suit for an upcoming vacation to Hawaii, and the next thing you know, you\u2019re inundated with swimsuit ads.",
              "With the integration of AI, the potential for behavior manipulation grows exponentially.",
              "Imagine a future where AI can understand our sentiment or tone of voice even when we don\u2019t explicitly, or directly, express our opinions.",
              "AI will be able to use these subtle intonations to make assumptions and predictions about our behaviors, opinions and ideas.",
              "This opens the door for potential manipulation that could be used in everything from targeted ads all the way to political persuasion.",
              "So what\u2019s an organization to do?",
              "For starters, all AI systems should be designed so that they can be audited and reviewed.",
              "And organizations should check for biases within the data used to train AI models.",
              "A steering committee, or \u201cmodel committee,\u201d can be set up to look at models, scrutinize the rules that support them and analyze their behavior to identify and remove any built-in biases.",
              "\u201cIt can go all the way from the top down to a process level improvement,\u201d says Michelle Yi, \u201cand there are a lot of ways that organizations can focus on helping to address this issue.",
              "\u201d Organizations must also prioritize transparency and accountability by making their policies around AI clear to both employees and the public.",
              "It may help to create a vision statement about how the organization will leverage AI, including the company\u2019s stance\u2014and ethics\u2014around it, and how AI maps back to the company\u2019s mission statement.",
              "Bottom line, the objectives and approach of how an organization uses AI must be clear to consumers, stakeholders and shareholders alike.",
              "Industry leaders should also work with the government to establish clear rules and regulations that foster innovation while ensuring accountability and transparency.",
              "Cooperation between government, industry and civil society will be crucial in order to harness the power of AI for good and avoid the pitfalls of what could go wrong.",
              "The ethics of AI will impact everyone\u2014not just people in the business world.",
              "As human beings and consumers, technology\u2019s influence is inescapable, like it or not.",
              "This is why it\u2019s so important to have the conversation now, in the early phases of what AI is potentially going to grow into.",
              "On an individual level, we must all become more discerning consumers and question the information that\u2019s fed to us.",
              "Awareness is the first step toward mitigating the impact of manipulation.",
              "By being critical of sources and not taking information at face value, we can better protect ourselves.",
              "Addressing the ethical challenges AI presents now is the best way to ensure that the technology reaches its potential to benefit society.",
              "Putting steps in place to remove bias and being vigilant about manipulation is the first step.",
              "We must start the conversations now in order to build a framework that safeguards society\u2019s values and fosters responsible and beneficial AI implementation.",
              "Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives.",
              "Do I qualify?"
            ]
          },
          {
            "title": "Why Chief Diversity Officers Must Lean In More On AI Data Bias - Forbes",
            "url": "https://www.forbes.com/sites/cindygordon/2023/05/21/why-chief-diversity-officers-must-lean-in-more-on-ai-data-bias/",
            "content": [
              "Why Chief Diversity Officers Must Lean In More On AI Data Bias - Forbes",
              "Diversity improves Profitability and Productivity.",
              "What is the role of a Chief Diversity Officer (CDO)?",
              "The primary role of a CDO is to promote and advocate for diversity, equity, and inclusion within an organization.",
              "They work to create a more inclusive environment by implementing strategies, policies, and programs that foster diversity and equality.",
              "They collaborate with different stakeholders to develop programs that address bias, discrimination, and underrepresentation.",
              "A diversity leader also facilitates training and education on cultural competency and unconscious bias, and they often support recruitment strategies to ensure diverse talent pools are targeted and sustained.",
              "Ultimately, their goal is to cultivate an inclusive workplace where diverse individuals can thrive and contribute their unique perspectives and talents.",
              "Diversity officers can play a crucial role in addressing AI data bias.",
              "\u201cData bias\u201d is a term used to refer to the effects of AI algorithms run on biased data.",
              "While machine learning programs are technically incapable of making human errors, these programs are \u201ctrained\u201d based on data entered by ordinary human beings.",
              "If the data set used is biased, software simply reinforces the human errors and biases.",
              "Bias in data is an error that occurs when certain elements of a dataset are overweighted or overrepresented.",
              "Biased datasets don't accurately represent ML model's use case, which leads to skewed outcomes, systematic racial prejudice, and low accuracy.",
              "Algorithms simply encode and amplify human biases and fuel many issues from impacting legal sentencing, advancing far too rapidly automotion of jobs, such as journalism which is significantly impacted by data bias, and inability to sort out cultural contextual relevance due to skewed data sets.",
              "\u2018While appreciation that algorithms and ML programs are not immune to bias is increasingly mainstream, ongoing plans to correct for bias in said programs among businesses that use them are not\u201d, a conclusion of a study by Progress Software Corp (based on 640 interviews with IT professionals in 12 countries).",
              "The lack of focus on data bias controls in sourcing AI vendors or in closely monitoring AI software development programs is a area for improvement, and many board directors and C-Leaders are not skilled in this area, exasperating the problems in this area.",
              "Many leading researchers have been advocating for improving AI data sets, such as MIT researcher, Dr.",
              "Joy Buolamwini noticed in working with leading facial analysis software and noticed that the software could not detect her face, because the algorithm had not been trained on a broad range of skin tones and facial structures.",
              "You can watch her research video here.",
              "With the explosion of AI, and most recently with ChatGPT, the growth of data sets that are being trained on historical data sets that are saturated with many social, cultural biases only amplifies what we do not want to bring forward in the new and enhanced AI intelligent world.",
              "So, I tried an experiment yesterday with ChatGPT, as I am heading down to Seattle to speak at the Wellness Redefined Conference, on Generative AI, sponsored by Amazon and I asked ChatGPT this question: What is the role of Chief Diversity and Inclusiveness Officers in AI data Bias?",
              "I was very pleased to see how logical and thorough the ChatGPT response was.",
              "As AI systems are trained on large datasets, they can inherit and perpetuate biases present in the data, which can lead to unfair or discriminatory outcomes.",
              "Diversity officers, with their expertise in promoting diversity and inclusion, can help identify and mitigate biases in AI systems. Here are some ways diversity officers can contribute: Conclusion: Chief Diversity Officers have a very important role to play in mitigating the risks of AI data bias, and need to work collaboratively with diverse stakeholders, data scientists, ethicists, cyber-security experts, legal, and policy experts.",
              "The EU wrote a good policy research paper on AI DataBias.",
              "As research from the EU Agency for Fundamental Rights (FRA) highlights, the use of AI can affect many fundamental rights.",
              "While algorithms can be a force for good, they can also violate the right to privacy or lead to discriminatory decision-making, which has a very real impact on people\u2019s lives.",
              "A cautionary tale about the possible negative effects of biased algorithms in practice comes from the Netherlands.",
              "In 2020, it came to light that the Dutch tax authorities had used algorithms that mistakenly labelled around 26,000 parents as having committed fraud in their childcare benefit applications.",
              "Many of these parents had an immigration background.",
              "They were required to pay back large sums, which led to great financial and psychological difficulties for the families concerned.",
              "The data protection authority concluded that the processing of data by the AI system in use was discriminatory.",
              "Chief Diversity and Inclusiveness Officers must increase their digital literacy understanding of AI, and engage more actively with their CIOs, CTOs, and Chief Data Science Officers.",
              "They, after all, have a great deal to lose if Data Bias risks in large data sets are not more rapidly curtailed.",
              "Leaning in more in AI practices will also increase their governance leadership profile in an area critical to ensure diversity and inclusiveness flourishes vs marginalized in old paradigms. Research Sources: OpenAI: ChatGPT (chatbot) EU Report on AI Data Bias in Algorithms"
            ]
          }
        ],
        "Data scientists and other stakeholders should consider the ethical and social implications of their work and follow best practices and guidelines for responsible AI development and use.": [
          {
            "title": "An embedded ethics approach for AI development - Nature",
            "url": "https://www.nature.com/articles/s42256-020-0214-1",
            "content": [
              "An embedded ethics approach for AI development - Nature",
              "Thank you for visiting nature.",
              "com.",
              "You are using a browser version with limited support for CSS.",
              "To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer).",
              "In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.",
              "Advertisement Nature Machine Intelligence volume 2, pages 488\u2013490 (2020)Cite this article 1776 Accesses 56 Citations 75 Altmetric Metrics details There is a need to consider how AI developers can be practically assisted in identifying and addressing ethical issues.",
              "In this Comment, a group of AI engineers, ethicists and social scientists suggest embedding ethicists into the development team as one way of improving the consideration of ethical issues during AI development.",
              "Ethical concerns around artificial intelligence (AI) technology have prompted a rush towards \u2018AI ethics\u2019 to consider how AI technology can be developed and implemented in an ethical manner1,2,3,4.",
              "A recent scoping review identified 84 documents containing ethical principles or guidelines for AI that have been issued by a wide range of public and private organizations1.",
              "In the absence of legally enforceable regulations, those developing AI technology are largely left to translate the existing high-level ethical principles as they see fit5.",
              "Furthermore, it has recently been argued that a principled approach is unlikely to be successful in AI because AI development \u201clacks common aims and fiduciary duties, professional history and norms, proven methods to translate principles into practice, and robust legal and professional accountability mechanisms\u201d compared to professions like medicine5.",
              "Although it is clear that a growing number of technology developers are willing to consider the ethical challenges around AI6, most do not have the necessary competency to translate high-level ethical principles into practice.",
              "This is unsurprising as the professional backgrounds of AI developers usually do not include systematic training in ethics.",
              "Conversely, few trained ethicists or social scientists currently work in tech companies, and there is no established culture of practical exchange between these fields.",
              "This creates a gap when it comes to translating ethical considerations into ethical practices, and there is a need to develop more concrete approaches7.",
              "It is imperative that the ethical challenges of AI are addressed as early as possible during the development process to ensure the ethically, socially and legally responsible design and implementation of these applications8.",
              "While various suggestions have emerged, so far there is no cohesive approach to integrating ethics into the development of AI and capitalize on the potential of including ethics upstream in the development process.",
              "We propose that an \u2018embedded ethics\u2019 approach can fill this gap and promote a more ethical development of AI applications.",
              "This is a preview of subscription content, access via your institution Open Access articles citing this article.",
              "Journal of the Academy of Marketing Science Open Access 11 November 2023 AI and Ethics Open Access 21 September 2023 BMC Medical Ethics Open Access 20 June 2023 Access Nature and 54 other Nature Portfolio journals Get Nature+, our best-value online-access subscription $29.",
              "99 / 30 days cancel any time Subscribe to this journal Receive 12 digital issues and online access to articles $119.",
              "00 per year only $9.",
              "92 per issue Rent or buy this article Prices vary by article type from$1.",
              "95 to$39.",
              "95 Prices may be subject to local taxes which are calculated during checkout Jobin, A.",
              ", Ienca, M.",
              "& Vayena, E.",
              "Nat.",
              "Mach.",
              "Intell.",
              "1, 389\u2013399 (2019).",
              "Article Google Scholar European Commission High-Level Expert Group on Artificial Intelligence Ethics Guidelines for Trustworthy AI (European Commission, 2018).",
              "Floridi, L.",
              "et al.",
              "Minds Mach.",
              "28, 689\u2013707 (2018).",
              "Article Google Scholar Hagendorff, T.",
              "Minds Mach.",
              "30, 99\u2013120 (2020).",
              "Article Google Scholar Mittelstadt, B.",
              "Nat.",
              "Mach.",
              "Intell.",
              "1, 501\u2013507 (2019).",
              "Article Google Scholar Metcalf, J.",
              ", Moss, E.",
              "& Boyd, D.",
              "Soc.",
              "Res.",
              "82, 449\u2013476 (2019).",
              "Google Scholar Theodorou, A.",
              "& Dignum, V.",
              "Nat.",
              "Mach.",
              "Intell.",
              "2, 10\u201312 (2020).",
              "Article Google Scholar Obermeyer, Z.",
              "& Emanuel, E.",
              "J.",
              "N.",
              "Engl.",
              "J.",
              "Med.",
              "29, 1216\u20131219 (2016).",
              "Article Google Scholar Grosz, B.",
              "et al.",
              "Commun.",
              "ACM 62, 54\u201361 (2018).",
              "Article Google Scholar Bonnemains, V.",
              ", Saurel, C.",
              "& Tessier Ethics Inform.",
              "Technol.",
              "20, 41\u201358 (2018).",
              "Article Google Scholar Ives, J.",
              "et al.",
              "BMC Med.",
              "Ethics 19, 68 (2018).",
              "Article Google Scholar Coeckelbergh, M.",
              "Technol.",
              "Reg.",
              "1, 31\u201334 (2019).",
              "Google Scholar Bostrom, N.",
              "& Yudkowsky, E.",
              "in The Cambridge Handbook of Artificial Intelligence (eds Frankish, K.",
              "& Ramsey, W.",
              "M.)",
              "316\u2013334 (Cambridge Univ.",
              "Press, 2014).",
              "Whittlestone, J.",
              ", Nyrup, R.",
              ", Alexandrova, A.",
              ", Dihal, K.",
              "& Cave, S.",
              "Ethical and Societal Implications of Algorithms, Data, and Artificial Intelligence: A Roadmap for Research 59 (Nuffield Foundation, 2019).",
              "Rigby, M.",
              "J.",
              "AMA J.",
              "Ethics 21, E121\u2013124 (2019).",
              "Article Google Scholar Begley, S.",
              "Stat News https://www.",
              "statnews.",
              "com/2017/02/23/bioethics-harvard-george-church/ (2017).",
              "Bietti, E.",
              "in Proc.",
              "2020 Conf.",
              "Fairness, Accountability, and Transparency (eds Hildebrandt, M.",
              "& Castillo, C.)",
              "210\u2013219 (ACM, 2020).",
              "Wagner, B.",
              "in Being Profiled: Cogitas Ergo Sum: 10 Years of \u2018Profiling the European Citizen\u2019 (eds Bayamlioglu, E.",
              "et al.)",
              "84\u201389 (Amsterdam Univ.",
              "Press, 2018).",
              "Brundage, M.",
              "& Bryson, J.",
              "J.",
              "Preprint at https://arxiv.",
              "org/abs/1608.",
              "08196 (2016).",
              "Winfield, A.",
              "F.",
              "T.",
              "& Jirotka, M.",
              "Phil.",
              "Trans.",
              "R.",
              "Soc.",
              "A 376, 20180085 (2018).",
              "Article Google Scholar Strech, D.",
              "& Sofaer, N.",
              "J.",
              "Med.",
              "Ethics 38, 121\u2013126 (2012).",
              "Article Google Scholar Download references The support by the Bavarian Institute for Digital Transformation is gratefully acknowledged.",
              "L.",
              "A.",
              "C.",
              "is funded by the National Institute of Health through NIBIB R01 EB017205.",
              "Institute of History and Ethics in Medicine, Technical University of Munich, Munich, Germany Stuart McLennan, Amelia Fiske & Alena Buyx Division of Pulmonary, Critical Care, and Sleep Medicine, Beth Israel Deaconess Medical Center, Boston, MA, USA Leo Anthony Celi Harvard\u2013Massachusetts Institute of Technology Division of Health Sciences and Technology, Cambridge, MA, USA Leo Anthony Celi Munich Center for Technology in Society, School of Management and School of Life Sciences, Technical University of Munich, Munich, Germany Ruth Mu\u0308ller Munich School of Robotics and Machine Intelligence, Technical University of Munich, Munich, Germany Jan Harder, Konstantin Ritt, Sami Haddadin & Alena Buyx You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar S.",
              "M.",
              "and A.",
              "B.",
              "conceived the initial idea of the paper.",
              "S.",
              "M.",
              "prepared the initial draft with the assistance of A.",
              "F.",
              ", in regular discussion with A.",
              "B.",
              ", who provided feedback and comments.",
              "L.",
              "A.",
              "C.",
              ", R.",
              "M.",
              ", J.",
              "H.",
              ", K.",
              "R.",
              "and S.",
              "H.",
              "reviewed the draft and critically revised it for important intellectual content.",
              "All authors read and approved the final version of the paper.",
              "Correspondence to Stuart McLennan.",
              "A.",
              "B.",
              "is a member of the German Ethics Council.",
              "S.",
              "H.",
              "is a member of the European High-Level Group on Artificial Intelligence, and is a founder of robotics company Franka Emika.",
              "Reprints and Permissions McLennan, S.",
              ", Fiske, A.",
              ", Celi, L.",
              "A.",
              "et al.",
              "An embedded ethics approach for AI development.",
              "Nat Mach Intell 2, 488\u2013490 (2020).",
              "https://doi.",
              "org/10.",
              "1038/s42256-020-0214-1 Download citation Published: 31 July 2020 Issue Date: September 2020 DOI: https://doi.",
              "org/10.",
              "1038/s42256-020-0214-1 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article.",
              "Provided by the Springer Nature SharedIt content-sharing initiative BMC Medical Ethics (2023) Nature Machine Intelligence (2023) Nature Mental Health (2023) Journal of the Academy of Marketing Science (2023) AI and Ethics (2023) Advertisement Nature Machine Intelligence (Nat Mach Intell) ISSN 2522-5839 (online) \u00a9 2023 Springer Nature Limited Sign up for the Nature Briefing newsletter \u2014 what matters in science, free to your inbox daily."
            ]
          },
          {
            "title": "13 Principles for Using AI Responsibly - Harvard Business Review",
            "url": "https://hbr.org/2023/06/13-principles-for-using-ai-responsibly",
            "content": [
              "13 Principles for Using AI Responsibly - Harvard Business Review",
              "The competitive nature of AI development poses a dilemma for organizations, as prioritizing speed may lead to neglecting ethical guidelines, bias detection, and safety measures.",
              "Known and emerging concerns associated with AI in the workplace include the spread of misinformation, copyright and intellectual property concerns, cybersecurity, data privacy, as well as navigating rapid and ambiguous regulations.",
              "To mitigate these risks, we propose thirteen principles for responsible AI at work.",
              "Love it or loath it, the rapid expansion of AI will not slow down anytime soon.",
              "But AI blunders can quickly damage a brand\u2019s reputation \u2014 just ask Microsoft\u2019s first chatbot, Tay.",
              "In the tech race, all leaders fear being left behind if they slow down while others don\u2019t.",
              "It\u2019s a high-stakes situation where cooperation seems risky, and defection tempting.",
              "This \u201cprisoner\u2019s dilemma\u201d (as it\u2019s called in game theory) poses risks to responsible AI practices.",
              "Leaders, prioritizing speed to market, are driving the current AI arms race in which major corporate players are rushing products and potentially short-changing critical considerations like ethical guidelines, bias detection, and safety measures.",
              "For instance, major tech corporations are laying off their AI ethics teams precisely at a time when responsible actions are needed most."
            ]
          },
          {
            "title": "Ethical governance of artificial intelligence: An integrated analytical ...",
            "url": "https://www.sciencedirect.com/science/article/pii/S2773067022000048",
            "content": [
              "Ethical governance of artificial intelligence: An integrated analytical ...",
              "Checking your browser before accessing \"www.",
              "sciencedirect.",
              "com\".",
              "Redirecting.",
              ".",
              ".",
              "Please enable Cookies and reload the page.",
              "Reference Number: 82acfee5a8763059 IP Address: 205.",
              "172.",
              "168.",
              "145 User Agent: Timestamp: About ScienceDirect Shopping cart Contact and supportTerms and conditionsPrivacy policy We use cookies to help provide and enhance our service and tailor content and ads.",
              "By continuing you agree to the use of cookies.",
              "Copyright \u00a9 2020 Elsevier B.",
              "V.",
              "or its licensors or contributors.",
              "ScienceDirect \u00ae is a registered trademark of Elsevier B.",
              "V."
            ]
          }
        ],
        "AI systems should be able to explain how they make decisions and what factors they consider.": [
          {
            "title": "How artificial intelligence will transform decision-making",
            "url": "https://www.weforum.org/agenda/2023/09/how-artificial-intelligence-will-transform-decision-making/",
            "content": [
              "How artificial intelligence will transform decision-making",
              "Artificial intelligence already plays a crucial role the decision-making processes.",
              "Image: Getty Images/iStockphoto Listen to the article How is the World Economic Forum ensuring the responsible use of technology?",
              "Create a free account and access your personalized content collection with our latest publications and analyses.",
              "License and Republishing World Economic Forum articles may be republished in accordance with the Creative Commons Attribution-NonCommercial-NoDerivatives 4.",
              "0 International Public License, and in accordance with our Terms of Use.",
              "The views expressed in this article are those of the author alone and not the World Economic Forum.",
              "A weekly update of the most important issues driving the global agenda You can unsubscribe at any time using the link in our emails.",
              "For more details, review our privacy policy.",
              "Metaverse isn't new, it's 20 years old.",
              "Generative AI: This is how you can use ChatGPT safely Ewan Thomson November 21, 2023 Moflin, an AI pet, responds like a real animal Conversations around the climate crisis can be difficult.",
              "Here's how AI and cartoons can help Gill Einhorn, Jemilah Mahmood, Flora McCrone, Janot Mendler de Suarez and Pablo Suarez November 20, 2023 UN advisory body created to address global AI governance, and other digital technology stories you need to read Cathy Li November 20, 2023 Why we can\u2019t leave AI to the machines Michael O\u2019Flaherty November 20, 2023 About Us Events Media More from the Forum Partners & Members Language Editions Privacy Policy & Terms of Service \u00a9 2023 World Economic Forum"
            ]
          },
          {
            "title": "Decision Making in AI - Scaler Topics",
            "url": "https://www.scaler.com/topics/artificial-intelligence-tutorial/decision-making-ai/",
            "content": [
              "Decision Making in AI - Scaler Topics",
              "Decision-making is the process of selecting the best course of action or solution from multiple options, based on a set of criteria or objectives.",
              "It is critical in many fields, including business, finance, healthcare, and public policy.",
              "Artificial Intelligence (AI) has the potential to revolutionize decision-making processes in various fields, from business to healthcare and beyond.",
              "Decision-making in AI refers to the process of selecting the best course of action or solution from multiple options, based on data and algorithms. Decision-making in AI refers to the process of using artificial intelligence techniques to make choices or take actions based on data, rules, and other inputs.",
              "AI-based decision-making typically involves several steps, including data collection, preprocessing, analysis, and prediction.",
              "Based on the analysis and predictions, AI algorithms can generate recommendations, select the most appropriate course of action, or even take actions autonomously.",
              "AI's decision-making approaches can be broadly categorized into two types: rule-based and learning-based.",
              "Rule-based systems rely on pre-defined rules and logic to make decisions, while learning-based systems use machine learning algorithms to analyze data and learn from patterns and trends to make predictions and decisions.",
              "AI can help in decision-making in various ways: Data-supported decision-making in AI involves using data analysis and machine learning techniques to support decision-making processes.",
              "It involves the following steps: Data-supported decision-making in AI can be applied in various fields, such as healthcare, finance, marketing, and logistics.",
              "For example, in healthcare,` data-supported decision-making can be used to predict patient outcomes and personalize treatment plans.",
              "In finance, it can be used to identify fraud and optimize investment strategies.",
              "In marketing, it can be used to segment customers and personalize marketing campaigns.",
              "And in logistics, it can be used to optimize supply chain management and improve delivery times.",
              "There are different degrees of decision-making in AI, depending on the level of human involvement and the level of autonomy of the AI system.",
              "The three main degrees of decision-making in AI are: The degree of decision-making used in AI depends on several factors, such as the complexity of the decision, the level of trust in the AI system, and the potential consequences of the decision.",
              "In some cases, a fully manual decision-making process may be preferred to ensure that human decision-makers have complete control and accountability over the decision.",
              "In other cases, a fully automated `decision-making process may be preferred to improve efficiency and reduce human error.",
              "AI has become increasingly important in decision-making for businesses as it can provide valuable insights and predictions to support strategic decision-making.",
              "Some of the ways AI can be used in decision-making for business include: AI can be applied in various domains and industries to support decision-making.",
              "Here are some examples:"
            ]
          },
          {
            "title": "Managing AI Decision-Making Tools - Harvard Business Review",
            "url": "https://hbr.org/2021/11/managing-ai-decision-making-tools",
            "content": [
              "Managing AI Decision-Making Tools - Harvard Business Review",
              "The nature of micro-decisions requires some level of automation, particularly for real-time and higher-volume decisions.",
              "Automation is enabled by algorithms (the rules, predictions, constraints, and logic that determine how a micro-decision is made).",
              "And these decision-making algorithms are often described as artificial intelligence (AI).",
              "The critical question is, how do human managers manage these types of algorithm-powered systems. An autonomous system is conceptually very easy.",
              "Imagine a driverless car without a steering wheel.",
              "The driver simply tells the car where to go and hopes for the best.",
              "But the moment there\u2019s a steering wheel, you have a problem.",
              "You must inform the driver when they might want to intervene, how they can intervene, and how much notice you will give them when the need to intervene arises.",
              "You must think carefully about the information you will present to the driver to help them make an appropriate intervention.",
              "Your business\u2019s use of AI is only going to increase, and that\u2019s a good thing.",
              "Digitalization allows businesses to operate at an atomic level and make millions of decisions each day about a single customer, product, supplier, asset, or transaction.",
              "But these decisions cannot be made by humans working in a spreadsheet."
            ]
          }
        ],
        "Increasing transparency in AI systems can help users and auditors to understand and evaluate the logic and reliability of the algorithms, as well as to identify and correct any errors or biases.": [
          {
            "title": "Auditing AI: The emerging battle\ufb01eld of transparency and assessment ...",
            "url": "https://www.thomsonreuters.com/en-us/posts/technology/auditing-ai-transparency/",
            "content": [
              "Auditing AI: The emerging battle\ufb01eld of transparency and assessment ...",
              "Mark Dangelo Independent Innovation Practitioner & Advisor 25 Oct 2023 \u00b7 6 minute read Mark Dangelo Independent Innovation Practitioner & Advisor 25 Oct 2023 \u00b7 6 minute read As the uses of generative AI become more commonplace, the reality of assessing and auditing the technology's performance, capabilities, and potential biases becomes even more important In under 10 months, the adoption of generative artificial intelligence (AI) has shattered records across consumer and corporate demographics.",
              "Every week, dozens of complimentary product suites and competitive solutions are introduced as AI investments in the last 12 months rapidly reach more than $100 billion.",
              "Yet for all the benefits and intrigue, we already know that AI systems today are not infallible \u2014 they make mistakes and take liberties with outputs, make up references, and even fudge data gathering.",
              "Despite all the excitement driving adoption and expansion, however, these solutions are black boxes of capabilities.",
              "The direct, negative customer impacts of a chatbot gone astray, the impacts of a biased machine-learning (ML) solution disadvantaging a demographic, the brand damage of errors and ensembled AI subsequently cascading outcomes across systems, and the legal and regulatory impacts regarding the difficulty of auditing grows with every introduced and cross-linked AI system.",
              "It will get better \u2014 but only with oversight and adaptive iterations.",
              "To make the situation more challenging, the skills required to audit AI individual and cross-linked systems are emerging and scarce, leading to a patchwork quilt of capabilities.",
              "For auditors and compliance personnel, the journey of AI transparency begins not with technology, but with a clear understanding of the goals and objectives of the AI solution, its touchpoints and control limits, and finally its data and technology ecosystem.",
              "Regardless of the industry, business leaders are captivated by AI and generative AI in particular, and they are investing heavily to secure their share of a five-year $5.",
              "5 trillion to $7 trillion global GDP expansion directly attributable to these nascent technologies.",
              "However, the implications of adopting various AI and ML solutions and methods introduce into domain auditing teams multiple levels of complexities which less than 18 months ago were just conference topics and not frontline demands.",
              "As illustrated below, what once was known and encased at audit process business-as-usual is now represented by additional demands and skill sets that are highly segmented.",
              "Making the situation more challenging is the lack of foundational interest in auditing from college graduates, and the significant exodus of talent experienced by public accounting firms. Across categories A through D, the granular taxonomies encased within the ecosystem showcase the different and potential solutions that traditionally reside outside the core competencies of professionally trained audit teams. The intersection between these categories represents common subject matter.",
              "This graphic implicitly addresses a fundamental challenge with the rapid adoption of innovative AI technologies \u2014 where the risks are hidden within not just the algorithms, but within the data;",
              "their outputs across silos;",
              "the layers of decision criteria;",
              "and most importantly, the legal and regulatory exposures of unintended consequences.",
              "For audit staff steeped in traditional approaches, data sampling, and process assessments, the science of AI encased within and across domain systems ushers in a step-function increase in required capability.",
              "These upskill demands must either be addressed within the audit team assembled, or matrixed to highly specialized staff, which can efficiently move from sample-based assessment to full data-discovery using audit-defined technologies and tools specifically developed for AI audits.",
              "For internal or external auditors, the ability to assemble the forensic skills to analyze and assess AI and its conformance to required standards places the audit teams in unfamiliar situations.",
              "Teams targeting AI solutions require advanced knowledge of data and computer sciences, machine and deep learning, and the potential for 100% sampling including algorithmic training, testing, and production data.",
              "Beyond the financial and domain knowledge of traditional approaches, data source ingestion feeds AI decisions, embedded workflows, operational rules, and decision alternatives that are spread across disparate systems leading to complex, opaque interdependencies.",
              "To illustrate this shift of demands and the cascading impacts of skills and approaches, the graphic below represents the activity, team, and iterative approaches comparing the traditional mindsets against adaptive, next- gen AI capabilities.",
              "The roles and responsibilities for staff analyzing and assessing AI in all its potential forms, the evaluation playbooks, along with balances and controls are all in the process of a permanent shift of features and functions.",
              "Further, this graphic highlights the sampling differences created by layers of forensically critical AI subcomponents that merge to deliver decisions.",
              "As AI grows in capability, these layers must be independently verified and validated before the aggregated outputs can be assessed for conformance and remediation.",
              "Indeed, linking hope to hype is why multiform AI will dominate 2024 budgets, which before 2023 was relegated to sophisticated technology firms or academic institutions.",
              "Industry demands are outpacing academic theory, leaving a void of capability and science when it comes to common forensic analysis and assessment of domain defined AI decision making.",
              "Industry leaders, AI scientists, and accounting practitioners are rushing to address the voids of methods, skills, and regulatory implications;",
              "however, what can audit and industry institutions do to ensure that their prized, efficient AI investments continue to advance ethically, accurately, and with transparency?",
              "To begin with, any initial due diligence checklist for AI oversight minimally includes: The above list is non-specific and domain-agnostic and should only be used to seed more specific plans of action and audit design.",
              "The promise of precision, domain application efficiencies, and correlated insights of data-driven AI solutions are here to stay.",
              "Moving forward, AI will not revert back to the edge of computer science.",
              "The mainstreaming of AI is now a freight train with momentum and purpose.",
              "For AI to improve without harm, industry and audit leaders cannot hope to stand in front of the barreling locomotive waiting for academics and politicians to study the path forward.",
              "Industry and audit leaders must break from the pack and embrace the emerging skills needed for AI oversight.",
              "Those that fail to address AI\u2019s cascading advancements, flaws, and complexities of design will likely find their organizations facing legal, regulatory, and investor scrutiny for a failure to anticipate and address advanced data-driven controls and guidelines.",
              "Contact us for a free demo and discover how you can reliably monitor your firm against its peers and stay on top of clients\u2019 needs to identify new or expanding opportunities that might emerge in 2023 and beyond.",
              "Nov 02 - 03, 2023 In November 2023, the Thomson Reuters Institute is proud to present the 22nd Annual Law Firm COO & CFO Forum .",
              ".",
              ".",
              "Event details Event details 16 Nov 2023 \u00b7 5 minute read 10 Nov 2023 \u00b7 5 minute read 8 Nov 2023 \u00b7 7 minute read 16 Dec 2022 \u00b7 5 minute read 28 Aug 2023 \u00b7 5 minute read 28 Aug 2023 \u00b7 6 minute read"
            ]
          },
          {
            "title": "An Auditor\u2019s Mindset in an AI Driven World | Deloitte US",
            "url": "https://www2.deloitte.com/us/en/pages/technology/articles/an-auditors-mindset-in-an-ai-driven-world.html",
            "content": [
              "An Auditor\u2019s Mindset in an AI Driven World | Deloitte US",
              "Limited functionality available Metaverse solutions that drive value Moving your business forward Real-world client stories of purpose and impact Key opportunities, trends, and challenges Explore the latest issue now Go straight to smart with daily updates on your mobile device See what's happening this week and the impact on your business Discover the pride of working at Deloitte, where you\u2019re part of a community of people united by their drive to make an impact.",
              "Meet our Deloitte Insiders.",
              "Empowering Deloitte\u2019s professionals to accelerate CPA success, while enhancing well-being and meaningful connections.",
              "Find your next project\u2014and the ones after that.",
              "Learn how Deloitte Open Talent centers the ongoing needs of contractors and highlights the best projects available across all our businesses.",
              "AI is an emerging area that is having a transformational impact on business operations and how organizations achieve their mission, strategy, and objectives.",
              "As this transformation occurs and AI scales within the enterprise, organizations should adapt governance, oversight structures, and processes to promote trust and transparency in AI models.",
              "How organizations use the underlying data and resulting outputs can be a game changer, but the reality is that AI models are only as good as the data that feeds them.",
              "If the data itself is flawed or changes over time, even subtly, outputs can shift.",
              "AI models create predictions, classifications, or new data based on observable inputs and outputs as opposed to pre-programmed rules.",
              "In other words, AI models infer the rules or decision weighting they apply to data.",
              "This distinction means that use of AI models can lead to unintended outcomes (e.",
              "g., bias, inaccurate decisions or recommendations, etc.)",
              ".",
              "In response, organizations should adapt their capabilities by testing, interpreting and monitoring both AI models and data to verify that deployed models are operating as intended.",
              "One of an auditor\u2019s core missions is to help enhance trust and transparency through providing assurance on a variety of different subject matters, from financial statements to regulatory compliance.",
              "The foundation for auditors to deliver assurance includes evaluating governance, risks, and processes that are relevant to the selected subject matter.",
              "An auditor\u2019s independent mindset and focus on risk assessment are core underlying concepts for evaluating oversight and effectiveness of AI models.",
              "Like many other stakeholders, auditors are adapting their tried and tested approach to an AI-enabled world.",
              "Example considerations include but are not limited to: First line: Management (process/model owners) has the primary responsibility to own and manage risks associated with development and day-to-day operational activities.",
              "Management should have a baseline understanding of risks in AI applications and where they manifest themselves in the specific models and data relevant to the organization\u2019s use cases.",
              "Second line: Risk management provides oversight in the form of frameworks, policies, procedures, methodologies, and tools.",
              "The second-line function should have a deep understanding of the AI-specific risks and related controls and mitigation.",
              "Third line: Internal audit assesses the first-line and second-line functions and reports on its design and operational effectiveness to the board and audit committee.",
              "In assessing the first-line functions, internal audit should assess whether AI development and monitoring adheres to the organization\u2019s policies, best practices for model development and relevant regulations.",
              "Auditors currently work with each line of defense, senior leadership, and those charged with governance (e.",
              "g., board of directors) to assess the organization\u2019s control environment.",
              "As organizations adopt and expand their use of AI, auditors can have a key role to help organizations identify and address AI-specific risks.",
              "In addition to considering the impact of AI applications within the enterprise, organizations should consider external stakeholders, including regulators and investors.",
              "As a result of being regulated and helping organizations with compliance, auditors have extensive compliance experience and maintain a close dialogue with a variety of regulators.",
              "Additionally, auditors play a critical role in bolstering investor confidence and trust through helping to address investors\u2019 expectations for transparency.",
              "An auditor\u2019s experience with regulators and understanding of regulatory intent and investor expectations, coupled with their independent mindset, can provide valuable insight to organizations as they respond to increased public concern and scrutiny regarding transparency and functionality of AI applications.",
              "Auditors are responding to AI\u2019s increasing impact to the business environment.",
              "A key aspect of auditors\u2019 response is adapting existing and developing new capabilities to work with organizations to promote trust and transparency in their use of AI which can support increased and accelerated the adoption of AI to help achieve an organization\u2019s strategic objectives.",
              "People, process, and technology are key elements for both auditors and organizations to support trust and transparency in use of AI models.",
              "People \u2013 Organizations should leverage existing skill sets in risk management and controls, and model risk management, and then augment those functionswith AI specialists (e.",
              "g., model owners, data scientists, and developers, etc.)",
              ".",
              "Process \u2013 Organizations should adapt a governance model to include leading practices, relevant frameworks and regulations that incorporate AI-specific considerations.",
              "When considering risks, organizations should have a position that\u2019s proactive on preparedness \u2013 taking steps to mitigate risks proactively as opposed to reactively.",
              "The adoption of an appropriate framework, such as Trustworthy AITM, is important in addressing AI-specific risk considerations like fairness, bias, and explainability.",
              "Technology \u2013 Organizations should use cutting-edge tools, including AI and data science platforms, to facilitate controlled processes for model development, deployment and ongoing monitoring of performance.",
              "MLOps is an important component of the foundation for organizations to integrate supporting tools and functionality to build trust and transparency for their AI applications.",
              "Omnia\u2019s Trustworthy AI Module showcases how certain tools and functionality can be used to support testing for bias, resilience and reliability, transparency and other aspects of the Deloitte Trustworthy AI framework.",
              "Insert Custom CSS fragment.",
              "Do not delete!",
              "This box/component contains code needed on this page.",
              "This message will not be visible when page is activated.",
              "+++ DO NOT USE THIS FRAGMENT WITHOUT EXPLICIT APPROVAL FROM THE CREATIVE STUDIO DEVELOPMENT TEAM +++ Insert Custom HTML fragment.",
              "Do not delete!",
              "This box/component contains code that is needed on this page.",
              "This message will not be visible when page is activated.",
              "+++ DO NOT USE THIS FRAGMENT WITHOUT EXPLICIT APPROVAL FROM THE CREATIVE STUDIO DEVELOPMENT TEAM +++ \u00a9 2023.",
              "See Terms of Use for more information.",
              "Deloitte refers to one or more of Deloitte Touche Tohmatsu Limited, a UK private company limited by guarantee (\"DTTL\"), its network of member firms, and their related entities.",
              "DTTL and each of its member firms are legally separate and independent entities.",
              "DTTL (also referred to as \"Deloitte Global\") does not provide services to clients.",
              "In the United States, Deloitte refers to one or more of the US member firms of DTTL, their related entities that operate using the \"Deloitte\" name in the United States and their respective affiliates.",
              "Certain services may not be available to attest clients under the rules and regulations of public accounting.",
              "Please see www.",
              "deloitte.",
              "com/about to learn more about our global network of member firms. \u00a9 2023.",
              "See Terms of Use for more information.",
              "Deloitte refers to one or more of Deloitte Touche Tohmatsu Limited, a UK private company limited by guarantee (\"DTTL\"), its network of member firms, and their related entities.",
              "DTTL and each of its member firms are legally separate and independent entities.",
              "DTTL (also referred to as \"Deloitte Global\") does not provide services to clients.",
              "In the United States, Deloitte refers to one or more of the US member firms of DTTL, their related entities that operate using the \"Deloitte\" name in the United States and their respective affiliates.",
              "Certain services may not be available to attest clients under the rules and regulations of public accounting.",
              "Please see www.",
              "deloitte.",
              "com/about to learn more about our global network of member firms."
            ]
          },
          {
            "title": "How Can AI Drive Audits? - ISACA",
            "url": "https://www.isaca.org/resources/isaca-journal/issues/2021/volume-4/how-can-ai-drive-audits",
            "content": [
              "How Can AI Drive Audits? - ISACA",
              "Auditing is viewed as a manual, subject-matter-driven, tedious and sometimes highly subjective practice.",
              "With the digital disruption that surrounds IT professionals, it is time to explore how the next generation of artificial intelligence (AI), natural language processing (NLP) and machine learning (ML) techniques can help the audit profession.",
              "There are several facets of these technologies that may come in handy during the life cycle of audits and beyond, and it is helpful to understand how similar techniques have been applied in other IT and business scenarios.",
              "Understanding the opportunities that AI presents can be a stepping stone to understanding the benefits of AI, NLP and ML and their many uses.",
              "The practice of auditing is more than a century old.",
              "What began as an accountant\u2019s job is now practiced across industries, with sophisticated focus on meeting regulatory requirements or performing risk-based audits.",
              "Although the types and methods or models of auditing have evolved over time, the auditor community has faced significant problems ranging from improper audit planning, ignorance toward changing and evolving risk, auditor bias, misalignment with auditees, and skewed data samples.",
              "The auditor\u2019s role in the Digital Age needs to evolve and adapt in a manner in which audits are a mechanism to identify patterns and trends from large data sets.",
              "These insights provide support for risk assessments, project scoping, and proactive and early identification of potential issues, among other things.",
              "For example, in today\u2019s rapidly evolving technology landscape, existing AI and ML techniques not only detect fraudulent transactions and identify high-risk issues such as unknown system activity from user endpoints, but learning models can also be built from such interventions.",
              "AI is a unification of several subgroups of technologies that enable computer programs to mimic human intelligence and decision-making skills.",
              "These mainly include ML, NLP and robotics.",
              "Figure 1 illustrates the landscape of the AI universe.",
              "Many of the topics in figure 1 can be potential problem solvers to several known issues and areas of concern within the auditing realm.",
              "1 For example: There are many such scenarios and mechanisms for using AI in audits.",
              "For any AI program to be successful at solving auditing problems, it needs to target the problems of data and data sets.",
              "In this regard, the answer to all of the following questions must be \u201cyes\u201d: In the past, the audit teams\u2019 focus has been on leveraging governance, risk and compliance (GRC) technology in terms of reviewing and verifying audit governance.",
              "For example, audit teams have used popular GRC tools such as RSA Archer, MetricStream or ServiceNow to perform semiautomated tasks that they may previously have done manually.",
              "Currently, there are several AI components available that can be used to solve data and sampling issues and other internal audit problems. Many GRC platforms can process data only into reports and may not be able to derive intelligent insights or help with data/sampling issues on their own.",
              "Any audit program can be measured using the following parameters to gauge its effectiveness/success: Each category should have several performance measures.",
              "Some of these performance measures are subjective, such as the ability to measure output (e.",
              "g., the number of times the auditing output was directly proportional to overall post-audit, process compliance).",
              "Another measure is efficiency (e.",
              "g., the percent of audits that can be completed under eight hours per audit).",
              "However, the challenge is to enhance output, quality and efficiency of audits based on automation of time-consuming and human-intensive tasks in the audit process without affecting the impact of the audit exercise.",
              "The key AI enablers in the audit process are: Figure 2 summarizes the automation/AI scope for each step of the audit workflow.",
              "It is evident that RPA, NLP and predictive analysis are some of the techniques that could bolster the way auditors approach audits.",
              "3, 4 Auditors must deal with a sea of information and data presented in response to compliance and other areas.",
              "It often seems impossible to consistently make sense out of audit samples.",
              "The biggest opportunity for RPA, ML and AI to work for auditing is to provide insights and intelligence regarding the sea of data.",
              "These opportunities include the following: Although taking advantage of AI, ML and RPA can benefit an organization, it is also important to understand and consider the risk involved: The application of AI and ML techniques can be extrapolated in the audit life cycle.",
              "Techniques and their uses to be considered are illustrated in figure 3.",
              "There have been several applications of AI and ML in other fields such as anti-money laundering.",
              "Detecting fraudulent transactions, performing data quality checks, negative news screening and processing have all been successfully automated via AI/ML techniques.",
              "Implementing AI or ML for large multinational corporate banks leads to big savings in manual overhead and reconciliation efforts.",
              "For example, a large multinational IT services firm helped a retail giant automate product order and product requisition form filling via NLP and the use of Alexa commands.",
              "It reduced manual errors, oversight and human intervention in repetitive tasks.",
              "Several legal firms can now use state-of-the-art AI and ML platforms to search, retrieve and derive meaningful insights from copious documentation and records based on regulatory and legal needs/jurisdiction requirements.",
              "There are many ways to improve audits over time with help from AI, ML and NLP techniques.",
              "However, the challenge lies in whether subject matter experts view these technologies as either a black box or a subject that is hard to interpret or understand.",
              "There may be very few professionals who are proficient in the IT domain or functional experts able to successfully implement AI and ML.",
              "IT professionals should adopt these techniques to better optimize the auditing field and beyond.",
              "A world where IT and auditing professionals are able to use RPA/AI/ML to their advantage will produce more insightful, efficient and measured work products.",
              "1 Parekh, N.",
              ";",
              "\u201cInside Product: Technology Landscape of Artificial Intelligence,\u201d Medium, 20 March 2018, https://becominghuman.",
              "ai/inside-ai-series-artificial-intelligence-technologies-across-processes-systems-and-computations-5e31eab21117 2 The Institute of Internal Auditors (IIA), Artificial Intelligence Part 1\u2014Considerations for the Profession of Internal Auditing, USA, 2017, https://iia.",
              "no/product/artificial-intelligence-considerations-for-the-profession-of-internal-auditing/ 3 Accenture Consulting, Evolving AML Journey: Leveraging Machine Learning Within Anti-Money Laundering Transaction Monitoring, USA, September 2017, https://www.",
              "accenture.",
              "com/_acnmedia/pdf-61/accenture-leveraging-machine-learning-anti-money-laundering-transaction-monitoring.",
              "pdf 4 Deloitte, Adopting Automation in Internal Audit: Using Robotic Process Automation and Cognitive Intelligence to Fortify the Third Line of Defense, USA, 2018, https://www2.",
              "deloitte.",
              "com/content/dam/Deloitte/us/Documents/risk/adopting-robotic-process-automation-in-internal-audit.",
              "pdf Is associate director at KPMG advisory.",
              "She has more than 14 years of experience working with Fortune 100 and 500 clients in the pharmaceutical, banking, hedge fund, retail and government sectors and has led the implementation of large- to medium-scale programs for information security, process and controls definition, risk management, and enterprise governance.",
              "She has experience building and developing GRC solutions for complex regulatory compliance and audit requirements.",
              "Previously she worked with PwC, MetricStream, Oracle and Siemens Research.",
              "She has published several papers on GRC, notably with the ISACA\u00ae Bangalore Chapter (India).",
              "She is a member of the Open Compliance and Ethics Group (OCEG), the Global Association of Risk Professionals (GARP), and other notable consortiums. Menon has also won accolades as a digital accelerator and with \u201cBe the Navigator,\u201d where she helped develop a rule-based keyword search engine, and she has led several proof-of-concepts on artificial intelligence and blockchain-based pharmaceutical compliance.",
              "1700 E.",
              "Golf Road, Suite 400, Schaumburg, Illinois 60173, USA | +1-847-253-1545 | \u00a92023 ISACA.",
              "All rights reserved."
            ]
          }
        ],
        "There should be clear and consistent standards for measuring and managing bias in AI, as well as for ensuring accountability and governance.": [
          {
            "title": "How the AI Executive Order and OMB memo introduce accountability for ...",
            "url": "https://www.brookings.edu/articles/how-the-ai-executive-order-and-omb-memo-introduce-accountability-for-artificial-intelligence/",
            "content": [
              "How the AI Executive Order and OMB memo introduce accountability for ...",
              "Commentary November 16, 2023 Artificial Intelligence Technology Policy & Regulation Governance Studies U.",
              "S.",
              "States and Territories Center for Technology Innovation President Biden recently signed the Executive Order (EO) on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.",
              "With sections on privacy, content verification, and immigration of tech workers (to name just a few areas), the executive order is sweeping.",
              "Encouragingly, it introduces key guardrails for the use of AI and takes important steps to protect peoples\u2019 rights.",
              "It is also inherently limited: Unlike acts of Congress, executive actions cannot create new agencies or grant new regulatory powers over private companies.",
              "(They can also be undone by the next president.)",
              "The EO was followed two days later by a draft memorandum, now open for public comment, from the Office of Management and Budget (OMB) with additional guidance for the federal government to manage risks and mandate accountability while advancing innovation in AI.",
              "Taken together, these two government directives offer one of the most detailed pictures of how governments should establish rules and guidance around AI.",
              "Notably, these actions towards accountability focus on current harms and not existential risk, and thus can serve as useful guides to policymakers focused on the everyday concerns of their constituents.",
              "Beyond executive action, with its inherent limits, the next step will be for other policymakers\u2014from Congress to the states\u2014to use these documents as a guide for future action in requiring accountability in the use of AI.",
              "As we analyze the EO and the OMB memo alongside each other for accountability directions, here is what stands out: The executive order (in Section 10.",
              "1(b)) gives explicit guidance to federal agencies for using AI in ways that protect safety and rights.",
              "The section outlines contents of the draft OMB memo released for public comment two days after the EO.",
              "In what may become a model for AI governance from localities, to states, to international governing agreements, the OMB memo, Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence, requires specific AI guardrails.",
              "Critically, the memo includes definitions of safety- and rights-impacting AI as well as lists of systems presumed to be safety- and-rights impacting.",
              "This approach builds on work done over the past decade to document the harms of algorithmic systems in mediating critical services and impacting people\u2019s vital opportunities.",
              "By taking this presumptive approach, rather than requiring agencies start from scratch with risk assessments on every system, the OMB memo also reduces the administrative burden on agencies and allows decision-makers to move directly to instituting appropriate guardrails and accountability practices.",
              "Systems can also be added or removed from the list based on a conducted risk assessment.",
              "Once an AI system is identified as safety- or rights-impacting, the draft OMB memo specifies a minimum set of practices that must be in place before and during its use.",
              "As required by the executive order, these practices build on those identified in the Blueprint for an AI Bill of Rights.",
              "This detailed section of the memo leads off with \u201cimpact assessments\u201d and lists three key areas that agencies must assess before a system is put into use: intended purpose and expected benefit;",
              "potential risks to a broad range of stakeholder groups;",
              "and quality and appropriateness of the data the AI model is built from.",
              "Should the assessing agency conclude that the system\u2019s benefits do not meaningfully outweigh the risks, \u201cagencies should not use the AI.",
              "\u201d The memo also directs agencies to assess, through this process, whether the AI system is fit for the task at hand;",
              "this is a critical effort to make sure AI actually works, when many times it has been shown not to, and to assess whether AI is the right solution to the given problem, countering the tendency to assume it is.",
              "The OMB memo goes on to require a range of accountability processes, including human fallback, the mitigation of new or emerging risks to rights and safety, ongoing assessment throughout a system\u2019s lifecycle, assessment for bias, and consultation and feedback from affected groups.",
              "Taken together, if carried through to the final version of the memo, these requirements create a remarkable step forward in establishing an accountability ecosystem\u2014not one point of intervention, but many methodologies and practices that, working together over time and at multiple stages in an AI lifecycle, could represent meaningful controls.",
              "Importantly, the OMB memo requires agencies to stop using an AI system if these practices are not in place.",
              "The minimum practices additionally include instructions to reconsider use of a system if concerning outcomes, such as discrimination, are found through testing.",
              "Public accountability will be challenging, given the breadth and complexity of these practices.",
              "One key accountability mechanism used will be annual reporting, as part of an expanded AI use case inventory.",
              "However, the details of what will be reported were not included as part of the memorandum and will be determined later by OMB.",
              "Journalists and researchers have identified problems with the previous practices of the AI use case inventory, including both that agencies left known AI uses off their inventory and that the reporting requirements were minimal and did not include testing and bias assessment results.",
              "Looking forward, effectiveness of the AI use case inventory as an accountability mechanism will depend on whether existing loopholes and under-reporting concerns are addressed through the OMB process to come.",
              "It\u2019s also important to consider that the effectiveness of transparency reporting on AI systems as an accountability mechanism has also been more broadly challenged.",
              "Throughout the guidance, OMB refers to requirements for government \u201cuse of AI.",
              "\u201d This phrase, importantly, covers both AI that is developed and then used by the federal government, and AI that is procured by the government.",
              "By using the power of the government\u2019s purse, the guidance also has the potential to influence the private sector as well.",
              "OMB also commits to developing further guidance for AI contracts that aligns with what it has laid out so far in this draft memo.",
              "That current guidance is rigorous;",
              "if those same provisions are successfully required for government purchasing of AI, it will significantly shape how government AI vendors are building and testing their products.",
              "The president only has so many levers to pull through an executive order to regulate private industry.",
              "Because the EO cannot make new laws, it relies on existing agency and presidential authorities (and the development of procurement rules described above) to influence how private companies are developing and deploying AI systems. Within that scope, the regulatory impact of the EO on the private sector could still be far-reaching.",
              "The EO directs agencies with enforcement powers to deepen their understanding of their capacities in the context of AI, to coordinate, and to develop guidance and potentially additional regulations to protect civil rights and civil liberties in the broader marketplace\u2014as well as to protect consumers from fraud, discrimination, and other risks, including risks to financial stability, and specifically to protect privacy.",
              "Sections 7 through 9 address various aspects of this, starting by directing the attorney general to assemble the heads of federal civil rights offices, including those of enforcement agencies, to determine how to apply and potentially expand the reach of civil rights law across the government to address existing harms. Additionally, the President calls on Congress to pass federal data privacy protections, and then through the EO\u2019s Section 9 directs agencies to do what they can to protect people\u2019s data privacy without Congressional action.",
              "The section opener calls out not only \u201cAI\u2019s facilitation of the collection or use of information about individuals,\u201d but also specifically \u201cthe making of inferences about individuals.",
              "\u201d This could open up a broader approach to assessing privacy violations, along the lines of \u201cnetworked privacy\u201d and associated harms, which considers not only individual personal identifiable information but the inferences that can be drawn by looking at connected data about an individual, or relationships between individuals.",
              "The EO directs agencies to revisit the guidelines for privacy impact assessments in the context of AI, as well as to assess and potentially issue guidelines on the use of privacy-enhancing technologies (PETs), such as differential privacy.",
              "Though brief, the EO\u2019s privacy section pushes to expand the understanding of data privacy and the remedies that might be taken to address novel and emerging harms. As those ideas move through government, they will inevitably inform potential data protection and privacy laws at the federal and (more likely) state level that will govern private industry.",
              "It\u2019s not surprising that generative AI was given a prominent treatment in the executive order: systems like ChatGPT that can generate text in response to prompts and other systems that can generate images, video, or audio, have catapulted concerns about AI into the public consciousness.",
              "Concerns have ranged from the technology\u2019s potential to replace skilled writers to its reinforcement of degrading stereotypes to the overblown notion that it will end humanity as we know it.",
              "Yet these systems are largely created by the private sector, and without new legislation the White House has limited levers to require these companies to act responsibly.",
              "There is an unfolding, live debate about whether to treat generative AI systems differently than other AI systems. The EO\u2019s authors choose to differentiate generative AI in Section 4, and have drawn criticism for that decision;",
              "a better approach may have been the one taken in the OMB memo where the same protections are required for generative AI as other AI and the focus is on the potential harms of the system.",
              "To govern generative AI systems, the executive order invokes the Defense Production Act.",
              "Introduced during the Korean War and also used for production of masks and ventilators during the COVID pandemic, the Defense Production Act gives the president the authority to expedite and expand industrial production in order to promote national defense.",
              "The executive order (in Section 4.",
              "2(i)) uses it to require private companies to preemptively test their models for specific safety concerns;",
              "it also specifies \u201cred-teaming\u201d as the testing methodology.",
              "Red-teaming is a practice of having a team external to the development of a system (but potentially still within the company) stress-test the system for specific concerns.",
              "The executive order requires that companies perform red-teaming in line with guidance from NIST that will be developed per Section 4.",
              "1(ii).",
              "Companies must report the resulting documentation of safety testing practices and results to the federal government.",
              "This AI accountability model\u2014preemptive testing according to specific standards and associated reporting requirements\u2014is potentially useful.",
              "Unfortunately, the specifics in this case leave much to be desired.",
              "First, given the use of the Defense Production Act, the testing and reporting the EO requires are limited to concerns relating to national defense and the protection of critical infrastructure, including cybersecurity and bioweapons.",
              "Yet as public debate has shown, concerns about generative AI go well beyond these limited settings.",
              "Second, the specific definitions used in the executive order to determine which systems must adhere to these standards appear to have been copied wholesale from a policy document put forth by OpenAI and other authors.",
              "Its thresholds for model size have little substantive justification;",
              "this means that future technological developments may render them under-inclusive or otherwise ineffective in targeting the systems with the most potential for harm.",
              "Finally, the executive order positions AI red-teaming as the singular AI accountability mechanism to be used for generative AI, when AI red-teaming works best in combination with other accountability mechanisms. By contrast, the OMB guidance for AI use by the federal government, which will also be required for generative AI, requires multiple accountability mechanisms including algorithmic impact assessments and public consultation.",
              "The full landscape of AI accountability mechanisms should be applied to generative AI by private companies as well.",
              "Consistent with the EO\u2019s broad approach, the order addresses AI\u2019s worker impacts in multiple ways.",
              "First, while research suggests a more complicated picture on technological automation and work, the EO sets out to support workers during an AI transition.",
              "To that end, the EO directs the chairman of the president\u2019s Council of Economic Advisers to \u201cprepare and submit a report to the president on the labor-market effects of AI.",
              "\u201d Section 6(a)(ii) mandates that the secretary of labor submit to the president a report analyzing how federal agencies may \u201csupport workers displaced by the adoption of AI and other technological advancements.",
              "\u201d Alongside the focus on AI displacement, the EO recognizes that automated decision systems are already in use in the workplace and directs attention to their ongoing impacts on job quality, worker power, and worker health and safety.",
              "The most encompassing directive lies in Section 6(b), which directs the secretary of labor, working with other agencies and \u201coutside entities, including labor unions and workers,\u201d to develop \u201cprinciples and best practices\u201d to mitigate harms to employees\u2019 well-being.",
              "The best practices must cover \u201clabor standards and job quality,\u201d and the EO further \u201cencourages\u201d federal agencies to adopt the guidelines in their internal programs. Section 7.",
              "3 of the EO directs the labor department to publish guidance for federal contractors regarding \u201cnondiscrimination in hiring involving AI and other technology-based hiring systems.\u201d Given the overwhelming evidence that algorithmic systems replicate and reinforce human biases, the broad language of \u201cother technology-based hiring systems\u201d is a major opportunity for the DOL to model standards of nondiscriminatory hiring.",
              "While the EO\u2019s worker protections are only guidance and best practices, the OMB memo directly mandates protocols to support workers and their rights when agencies use AI.",
              "The memo applies the \u201cminimum risk management practices\u201d where AI is used to determine \u201cthe terms and conditions of employment.",
              "\u201d This broad definition positions the federal government, as the nation\u2019s largest employer, to influence the use of AI systems within the workplace.",
              "The memo also requires that human remedies are in place in some cases, a requirement that may add jobs, adding complexity to concerns about the labor-market effects of AI.",
              "Further, the OMB memo\u2019s requirement that federal agencies \u201cconsult and incorporate feedback from affected groups\u201d positions workers and unions to influence the deployment of AI technology, which aligns with calls from civil society and academia to ensure that the people most likely to be affected by technology should have influence into that system\u2019s design and deployment.",
              "The narrative that the federal government is not knowledgeable about AI systems should be laid to rest by these recent documents.",
              "There was clearly a lot of thought put into the design and implementation of a national AI governance model.",
              "That said, it\u2019s also clear that many more people representing the right mix of expertise will be needed quickly to implement this ambitious plan on the tight timeline laid out in the order\u2014and on the implicit deadline marked by the end of the Biden administration\u2019s first term.",
              "Given that the EO and the OMB memo collectively run to well over 100 pages of actions that the federal government should take to address AI, the question looms: who will do all this work?",
              "A major new role addressed in both the EO and the OMB memo is that of the Chief AI Officer (CAIO), which every agency head is required to designate within 60 days of the EO\u2019s enactment.",
              "The CAIO\u2019s responsibilities are laid out in the OMB memo and fall into three categories: coordinating agency use of AI, promoting AI innovation, and managing risks from AI use.",
              "The way the CAIO role is understood and filled will be critical to what comes next;",
              "if agencies interpret the role as solely or primarily a technical one, rather than one focused societally on opportunities and risks related to the public interest use of AI, they may pursue very different implementation priorities than those articulated by the EO.",
              "CAIOs are also responsible for agency-level AI strategies, which are due within one year of the EO\u2019s launch.",
              "The strategies seem likely to call for increased headcount and new expertise in government.",
              "The EO has anticipated the need for both bringing new talent into the government and building the skills and capacities of civil servants on AI matters.",
              "The federal government has long been criticized for its slow, difficult hiring processes, making it tremendously challenging for an administration to pivot attention to an emerging issue.",
              "This administration has tried to preempt this criticism through the announcement of \u201cAI talent surge\u201d specified in Section 10.",
              "2 of the EO.",
              "That section gives OSTP and OMB a spare 45 days to figure out how to get the needed people into government, including through the establishment of a cross-agency AI and Technology Talent Task Force.",
              "The federal government has already started some of that recruitment push in the launch of a new AI jobs website.",
              "What is potentially most challenging in recruiting \u201cAI talent\u201d is identifying the actual skills, capacities, and expertise needed to implement the EO\u2019s many angles.",
              "While there is a need, of course, for technological talent, much of what the EO calls for, particularly in the area of protecting rights and ensuring safety, requires interdisciplinary expertise.",
              "What the EO requires is the creation of new knowledge about how to govern\u2014indeed, what the role of government is in an increasingly data-centric and AI-mediated environment.",
              "These are questions for teams with a sociotechnical lens, requiring expertise in a range of disciplines, including legal scholarship, the social and behavioral sciences, computer and data science, and often, specific field knowledge\u2014health and human services, the criminal legal system, financial markets and consumer financial protection, and so on.",
              "Such skills will especially be key for the second pillar of the administration\u2019s talent surge\u2014the growth in regulatory and enforcement capacity needed to keep watch over the powerful AI companies.",
              "It\u2019s also critical to ensure that these teams are built with attention to equity at the center.",
              "Given the broad empirical base that demonstrates the disproportionate harms of AI systems to historically marginalized groups, and the President\u2019s declared commitment to advancing racial equity across the federal government, equity in both hiring and as a focus of implementation must be a top priority of all aspects of EO implementation.",
              "As broad as the EO is, there are critical areas of concern that have either been pushed off to later consideration, or avoided.",
              "For instance, the EO includes a national security carveout, with direction to develop separate guidance in 270 days to \u201caddress the governance of AI used as a component of a national security system or for military and intelligence purposes\u201d;",
              "many applications of AI could potentially fall within those criteria.",
              "The EO also doesn\u2019t take the opportunity to ban specific practices shown to be harmful or ineffective;",
              "an example where it could have taken further action is in banning the use of affective computing in law enforcement.",
              "The EO addresses the potential for AI to be valuable in climate science and the mitigation of climate change;",
              "however, it does nothing about AI\u2019s own environmental impact, missing an opportunity to force reporting on energy and water usage by companies creating some of the biggest AI systems. Lastly, the EO sets guidelines for the use of AI by federal agencies and contractors but does not attach any requirements or guidance for recipients of federal grants, such as cities and states.",
              "Finally, the EO addresses research in a number of points throughout the document and references research on a range of topics and through many vehicles, including an National Science Foundation (NSF) Regional Innovation Engine and four NSF AI Research Institutes, to join the 25 already established.",
              "Yet the EO doesn\u2019t include major *new* commitments to research funding.",
              "A more robust approach to addressing AI research and education in the EO could have been a statement that reframed the national AI research and development field as sociotechnical, rather than purely technical\u2014proactively focused on interdisciplinary approaches that center societal impacts of AI alongside technological advancement.",
              "Such a statement would have aligned meaningfully with Vice President Kamala Harris\u2019s November 1st 2023 speech at the UK AI Safety Summit in which she argued for \u201ca future where AI is used to advance the public interest.",
              "\u201d If the administration is indeed committed to seeing AI in \u201cthe public interest,\u201d as Vice President Harris indicated, its new EO and OMB guidance are the clearest indication of how it intends to meet that ambition: mandating hard accountability to protect rights, regulating private industry, and moving iteratively, so that governance efforts advance alongside the field of sociotechnical research.",
              "But the executive branch can only do so much.",
              "Ultimately, the EO can be read\u2013among other ways\u2014as a roadmap for Congress to legislate.",
              "Additionally, cities, states, and other countries should understand these new documents as direction-setting and could choose to rapidly align their policies with these documents to create more comprehensive rights and safety protections.",
              "Related Content Nicol Turner Lee, Joseph B.",
              "Keller, Cameron F.",
              "Kerry, Aaron Klein, Anton Korinek, Mark MacCarthy, Mark Muro, Chinasa T.",
              "Okolo, Courtney C.",
              "Radsch, John Villasenor, Darrell M.",
              "West, Tom Wheeler, Andrew W.",
              "Wyckoff, Mishaela Robison November 2, 2023 Mark MacCarthy October 19, 2023 Nicol Turner Lee, Jack Malamud December 19, 2022 Authors Artificial Intelligence Technology Policy & Regulation Governance Studies U.",
              "S.",
              "States and Territories Center for Technology Innovation John Villasenor November 22, 2023 Norman Eisen, Nicol Turner Lee, Colby Galliher, Jonathan Katz November 21, 2023 2023 The Brookings Institution, Washington DC 2:00 pm - 3:30 pm EST Get the latest from Brookings The Brookings Institution is a nonprofit organization based in Washington, D.",
              "C.",
              "Our mission is to conduct in-depth, nonpartisan research to improve policy and governance at local, national, and global levels.",
              "Copyright 2023 The Brookings Institution"
            ]
          },
          {
            "title": "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
            "url": "https://www.nist.gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights",
            "content": [
              "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
              "An official website of the United States government Here\u2019s how you know Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock ( Lock A locked padlock ) or https:// means you\u2019ve safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "https://www.",
              "nist.",
              "gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights As a step toward improving our ability to identify and manage the harmful effects of bias in artificial intelligence (AI) systems, researchers at the National Institute of Standards and Technology (NIST) recommend widening the scope of where we look for the source of these biases \u2014 beyond the machine learning processes and data used to train AI software to the broader societal factors that influence how technology is developed.",
              "The recommendation is a core message of a revised NIST publication, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270), which reflects public comments the agency received on its draft version released last summer.",
              "As part of a larger effort to support the development of trustworthy and responsible AI, the document offers guidance connected to the AI Risk Management Framework that NIST is developing.",
              "According to NIST\u2019s Reva Schwartz, the main distinction between the draft and final versions of the publication is the new emphasis on how bias manifests itself not only in AI algorithms and the data used to train them, but also in the societal context in which AI systems are used.",
              "\u201cContext is everything,\u201d said Schwartz, principal investigator for AI bias and one of the report\u2019s authors.",
              "\u201cAI systems do not operate in isolation.",
              "They help people make decisions that directly affect other people\u2019s lives.",
              "If we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology, and the comments we received from a wide range of people and organizations emphasized this point.",
              "\u201d Bias in AI can harm humans.",
              "AI can make decisions that affect whether a person is admitted into a school, authorized for a bank loan or accepted as a rental applicant.",
              "It is relatively common knowledge that AI systems can exhibit biases that stem from their programming and data sources;",
              "for example, machine learning software could be trained on a dataset that underrepresents a particular gender or ethnic group.",
              "The revised NIST publication acknowledges that while these computational and statistical sources of bias remain highly important, they do not represent the full picture.",
              "A more complete understanding of bias must take into account human and systemic biases, which figure significantly in the new version.",
              "Systemic biases result from institutions operating in ways that disadvantage certain social groups, such as discriminating against individuals based on their race.",
              "Human biases can relate to how people use data to fill in missing information, such as a person\u2019s neighborhood of residence influencing how likely authorities would consider the person to be a crime suspect.",
              "When human, systemic and computational biases combine, they can form a pernicious mixture \u2014 especially when explicit guidance is lacking for addressing the risks associated with using AI systems. \u201cIf we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology.",
              "\u201d \u2014Reva Schwartz, principal investigator for AI bias To address these issues, the NIST authors make the case for a \u201csocio-technical\u201d approach to mitigating bias in AI.",
              "This approach involves a recognition that AI operates in a larger social context \u2014 and that purely technically based efforts to solve the problem of bias will come up short.",
              "\u201cOrganizations often default to overly technical solutions for AI bias issues,\u201d Schwartz said.",
              "\u201cBut these approaches do not adequately capture the societal impact of AI systems. The expansion of AI into many aspects of public life requires extending our view to consider AI within the larger social system in which it operates.",
              "\u201d Socio-technical approaches in AI are an emerging area, Schwartz said, and identifying measurement techniques to take these factors into consideration will require a broad set of disciplines and stakeholders.",
              "\u201cIt\u2019s important to bring in experts from various fields \u2014 not just engineering \u2014 and to listen to other organizations and communities about the impact of AI,\u201d she said.",
              "NIST is planning a series of public workshops over the next few months aimed at drafting a technical report for addressing AI bias and connecting the report with the AI Risk Management Framework.",
              "For more information and to register, visit the AI RMF workshop page.",
              "Webmaster | Contact Us | Our Other Offices"
            ]
          },
          {
            "title": "NIST Proposes Approach for Reducing Risk of Bias in Artificial ...",
            "url": "https://www.nist.gov/news-events/news/2021/06/nist-proposes-approach-reducing-risk-bias-artificial-intelligence",
            "content": [
              "NIST Proposes Approach for Reducing Risk of Bias in Artificial ...",
              "An official website of the United States government Here\u2019s how you know Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock ( Lock A locked padlock ) or https:// means you\u2019ve safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "https://www.",
              "nist.",
              "gov/news-events/news/2021/06/nist-proposes-approach-reducing-risk-bias-artificial-intelligence In an effort to counter the often pernicious effect of biases in artificial intelligence (AI) that can damage people\u2019s lives and public trust in AI, the National Institute of Standards and Technology (NIST) is advancing an approach for identifying and managing these biases \u2014 and is requesting the public\u2019s help in improving it.",
              "NIST outlines the approach in A Proposal for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270), a new publication that forms part of the agency\u2019s broader effort to support the development of trustworthy and responsible AI.",
              "NIST is accepting comments on the document until Sept.",
              "10, 2021 (extended from the original deadline of Aug.",
              "5, 2021), and the authors will use the public\u2019s responses to help shape the agenda of several collaborative virtual events NIST will hold in coming months .",
              "This series of events is intended to engage the stakeholder community and allow them to provide feedback and recommendations for mitigating the risk of bias in AI.",
              "\u201cManaging the risk of bias in AI is a critical part of developing trustworthy AI systems, but the path to achieving this remains unclear,\u201d said NIST\u2019s Reva Schwartz, one of the report\u2019s authors.",
              "\u201cWe want to engage the community in developing voluntary, consensus-based standards for managing AI bias and reducing the risk of harmful outcomes that it can cause.",
              "\u201d AI has become a transformative technology as it can often make sense of information more quickly and consistently than humans can.",
              "AI now plays a role in everything from disease diagnosis to the digital assistants on our smartphones.",
              "But as AI\u2019s applications have grown, so has our realization that its results can be thrown off by biases in the data it is fed \u2014 data that captures the real world incompletely or inaccurately.",
              "Moreover, some AI systems are built to model complex concepts, such as \u201ccriminality\u201d or \u201cemployment suitability,\u201d that cannot be directly measured or captured by data in the first place.",
              "These systems use other factors, such as area of residence or education level, as proxies for the concepts they attempt to model.",
              "The imprecise correlation of the proxy data with the original concept can contribute to harmful or discriminatory AI outcomes, such as wrongful arrests, or qualified applicants being erroneously rejected for jobs or loans.",
              "The approach the authors propose for managing bias involves a conscientious effort to identify and manage bias at different points in an AI system\u2019s lifecycle, from initial conception to design to release.",
              "The goal is to involve stakeholders from many groups both within and outside of the technology sector, allowing perspectives that traditionally have not been heard.",
              "\u201cWe want to bring together the community of AI developers of course, but we also want to involve psychologists, sociologists, legal experts and people from marginalized communities,\u201d said NIST\u2019s Elham Tabassi, a member of the National AI Research Resource Task Force.",
              "\u201cWe would like perspective from people whom AI affects, both from those who create AI systems and also those who are not directly involved in its creation.",
              "\u201d The NIST authors\u2019 preparatory research involved a literature survey that included peer-reviewed journals, books and popular news media, as well as industry reports and presentations.",
              "It revealed that bias can creep into AI systems at all stages of their development, often in ways that differ depending on the purpose of the AI and the social context in which people use it.",
              "\u201cAn AI tool is often developed for one purpose, but then it gets used in other very different contexts,\u201d Schwartz said.",
              "\u201cMany AI applications also have been insufficiently tested, or not tested at all in the context for which they are intended.",
              "All these factors can allow bias to go undetected.",
              "\u201d Because the team members recognize that they do not have all the answers, Schwartz said that it was important to get public feedback \u2014 especially from people outside the developer community who do not ordinarily participate in technical discussions.",
              "\"We would like perspective from people whom AI affects, both from those who create AI systems and also those who are not directly involved in its creation.\"",
              "\u2013 Elham Tabassi \u201cWe know that bias is prevalent throughout the AI lifecycle,\u201d Schwartz said.",
              "\u201cNot knowing where your model is biased, or presuming that there is no bias, would be dangerous.",
              "Determining methods for identifying and managing it is a vital next step.",
              "\u201d Comments on the proposed approach can be submitted by Sept.",
              "10, 2021 (extended from the original deadline of Aug.",
              "5, 2021), by downloading and completing the template form (in Excel format) and sending it to ai-bias [at] list.",
              "nist.",
              "gov (ai-bias[at]list[dot]nist[dot]gov).",
              "More information on the collaborative event series will be posted on this page.",
              "Webmaster | Contact Us | Our Other Offices"
            ]
          }
        ],
        "These standards should be developed with input from diverse stakeholders, including experts, regulators, civil society, and affected communities.": [
          {
            "title": "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
            "url": "https://www.nist.gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights",
            "content": [
              "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
              "An official website of the United States government Here\u2019s how you know Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock ( Lock A locked padlock ) or https:// means you\u2019ve safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "https://www.",
              "nist.",
              "gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights As a step toward improving our ability to identify and manage the harmful effects of bias in artificial intelligence (AI) systems, researchers at the National Institute of Standards and Technology (NIST) recommend widening the scope of where we look for the source of these biases \u2014 beyond the machine learning processes and data used to train AI software to the broader societal factors that influence how technology is developed.",
              "The recommendation is a core message of a revised NIST publication, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270), which reflects public comments the agency received on its draft version released last summer.",
              "As part of a larger effort to support the development of trustworthy and responsible AI, the document offers guidance connected to the AI Risk Management Framework that NIST is developing.",
              "According to NIST\u2019s Reva Schwartz, the main distinction between the draft and final versions of the publication is the new emphasis on how bias manifests itself not only in AI algorithms and the data used to train them, but also in the societal context in which AI systems are used.",
              "\u201cContext is everything,\u201d said Schwartz, principal investigator for AI bias and one of the report\u2019s authors.",
              "\u201cAI systems do not operate in isolation.",
              "They help people make decisions that directly affect other people\u2019s lives.",
              "If we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology, and the comments we received from a wide range of people and organizations emphasized this point.",
              "\u201d Bias in AI can harm humans.",
              "AI can make decisions that affect whether a person is admitted into a school, authorized for a bank loan or accepted as a rental applicant.",
              "It is relatively common knowledge that AI systems can exhibit biases that stem from their programming and data sources;",
              "for example, machine learning software could be trained on a dataset that underrepresents a particular gender or ethnic group.",
              "The revised NIST publication acknowledges that while these computational and statistical sources of bias remain highly important, they do not represent the full picture.",
              "A more complete understanding of bias must take into account human and systemic biases, which figure significantly in the new version.",
              "Systemic biases result from institutions operating in ways that disadvantage certain social groups, such as discriminating against individuals based on their race.",
              "Human biases can relate to how people use data to fill in missing information, such as a person\u2019s neighborhood of residence influencing how likely authorities would consider the person to be a crime suspect.",
              "When human, systemic and computational biases combine, they can form a pernicious mixture \u2014 especially when explicit guidance is lacking for addressing the risks associated with using AI systems. \u201cIf we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology.",
              "\u201d \u2014Reva Schwartz, principal investigator for AI bias To address these issues, the NIST authors make the case for a \u201csocio-technical\u201d approach to mitigating bias in AI.",
              "This approach involves a recognition that AI operates in a larger social context \u2014 and that purely technically based efforts to solve the problem of bias will come up short.",
              "\u201cOrganizations often default to overly technical solutions for AI bias issues,\u201d Schwartz said.",
              "\u201cBut these approaches do not adequately capture the societal impact of AI systems. The expansion of AI into many aspects of public life requires extending our view to consider AI within the larger social system in which it operates.",
              "\u201d Socio-technical approaches in AI are an emerging area, Schwartz said, and identifying measurement techniques to take these factors into consideration will require a broad set of disciplines and stakeholders.",
              "\u201cIt\u2019s important to bring in experts from various fields \u2014 not just engineering \u2014 and to listen to other organizations and communities about the impact of AI,\u201d she said.",
              "NIST is planning a series of public workshops over the next few months aimed at drafting a technical report for addressing AI bias and connecting the report with the AI Risk Management Framework.",
              "For more information and to register, visit the AI RMF workshop page.",
              "Webmaster | Contact Us | Our Other Offices"
            ]
          },
          {
            "title": "The Ethics Of AI: Navigating Bias, Manipulation And Beyond - Forbes",
            "url": "https://www.forbes.com/sites/forbestechcouncil/2023/06/23/the-ethics-of-ai-navigating-bias-manipulation-and-beyond/",
            "content": [
              "The Ethics Of AI: Navigating Bias, Manipulation And Beyond - Forbes",
              "Monika Mueller is the EVP Consulting Services and Head of LATAM for Softensity, a U.",
              "S.",
              "-based IT outsourcing company with a global footprint.",
              "Artificial intelligence (AI) is nothing new.",
              "It\u2019s been around since the 1950s, but 2023 certainly feels like a tipping point.",
              "No longer is AI the sole provenance of academics and tech professionals.",
              "With the introduction of ChatGPT, Google Bard and the like, the technology is now easily accessible to all.",
              "And therein lies the challenge.",
              "As AI becomes ever more consumable and its capabilities continue to evolve at breakneck speed, so too will the implications for society as a whole.",
              "In an ideal world, government, industry and civil society should work together to ensure that AI is developed and implemented ethically.",
              "But the genie is out of the bottle, so to speak, and despite growing concern from AI pioneers and thought leaders alike, there\u2019s likely no slowing it down.",
              "Even so, there\u2019s plenty that we can do to set up some guardrails around sticky ethical considerations.",
              "It begins with recognizing bias and minimizing manipulation by increasing transparency and opening a dialogue about the ethical challenges that AI presents.",
              "One of the key concerns surrounding the ethics of AI is the potential for reinforcing existing biases.",
              "As discussed in a conversation with Michelle Yi, Senior Director of Applied Artificial Intelligence at RelationalAI, bias in AI systems can have far-reaching consequences.",
              "When biased data is fed into AI models, it can perpetuate biases on an unprecedented scale.",
              "It all begins with the concept of \u201cdata in, data out.",
              "\u201d If biased data is used to train AI models, the resulting outputs will inevitably reflect those biases.",
              "Machine learning algorithms have the power to amplify these biases, and unless we actively check for and address them, we risk perpetuating societal prejudices unintentionally.",
              "This issue becomes especially significant when AI is employed in decision-making processes, such as hiring, lending or criminal justice.",
              "Addressing bias in AI is crucial to ensure fairness and equity in all of its applications.",
              "Another area of concern is the use of AI to manipulate people's behavior.",
              "We all know how annoying it is when Alexa or Siri picks up on our conversations and serves up targeted ads accordingly.",
              "For example, you talk about needing a new bathing suit for an upcoming vacation to Hawaii, and the next thing you know, you\u2019re inundated with swimsuit ads.",
              "With the integration of AI, the potential for behavior manipulation grows exponentially.",
              "Imagine a future where AI can understand our sentiment or tone of voice even when we don\u2019t explicitly, or directly, express our opinions.",
              "AI will be able to use these subtle intonations to make assumptions and predictions about our behaviors, opinions and ideas.",
              "This opens the door for potential manipulation that could be used in everything from targeted ads all the way to political persuasion.",
              "So what\u2019s an organization to do?",
              "For starters, all AI systems should be designed so that they can be audited and reviewed.",
              "And organizations should check for biases within the data used to train AI models.",
              "A steering committee, or \u201cmodel committee,\u201d can be set up to look at models, scrutinize the rules that support them and analyze their behavior to identify and remove any built-in biases.",
              "\u201cIt can go all the way from the top down to a process level improvement,\u201d says Michelle Yi, \u201cand there are a lot of ways that organizations can focus on helping to address this issue.",
              "\u201d Organizations must also prioritize transparency and accountability by making their policies around AI clear to both employees and the public.",
              "It may help to create a vision statement about how the organization will leverage AI, including the company\u2019s stance\u2014and ethics\u2014around it, and how AI maps back to the company\u2019s mission statement.",
              "Bottom line, the objectives and approach of how an organization uses AI must be clear to consumers, stakeholders and shareholders alike.",
              "Industry leaders should also work with the government to establish clear rules and regulations that foster innovation while ensuring accountability and transparency.",
              "Cooperation between government, industry and civil society will be crucial in order to harness the power of AI for good and avoid the pitfalls of what could go wrong.",
              "The ethics of AI will impact everyone\u2014not just people in the business world.",
              "As human beings and consumers, technology\u2019s influence is inescapable, like it or not.",
              "This is why it\u2019s so important to have the conversation now, in the early phases of what AI is potentially going to grow into.",
              "On an individual level, we must all become more discerning consumers and question the information that\u2019s fed to us.",
              "Awareness is the first step toward mitigating the impact of manipulation.",
              "By being critical of sources and not taking information at face value, we can better protect ourselves.",
              "Addressing the ethical challenges AI presents now is the best way to ensure that the technology reaches its potential to benefit society.",
              "Putting steps in place to remove bias and being vigilant about manipulation is the first step.",
              "We must start the conversations now in order to build a framework that safeguards society\u2019s values and fosters responsible and beneficial AI implementation.",
              "Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives.",
              "Do I qualify?"
            ]
          },
          {
            "title": "The U.S. can improve its AI governance strategy by ... - Brookings",
            "url": "https://www.brookings.edu/articles/the-u-s-can-improve-its-ai-governance-strategy-by-addressing-online-biases/",
            "content": [
              "The U.S. can improve its AI governance strategy by ... - Brookings",
              "Commentary May 17, 2022 Artificial Intelligence Internet & Telecommunications Governance Studies Center for Technology Innovation The United States has been working to codify the National Artificial Intelligence (AI) Initiative that focuses on six strategic pillars: improving AI innovation, advancing trustworthy AI, creating new education and training opportunities through AI, improving existing infrastructure through new technologies, facilitating federal and private sector utilization of AI to improve existing systems, and promoting an international environment that supports further advances in AI.",
              "In April 2022, the U.",
              "S.",
              "Department of Commerce, and the National Institute on Standards (NIST) announced members of the inaugural National Artificial Intelligence Advisory Committee (NAIAC), which will be tasked with advising the Biden administration on how to proceed with national AI governance efforts.",
              "At their first meeting on May 4, 2022, the NAIAC discussed the use of AI pertaining to U.",
              "S.",
              "competitiveness, issues related to workforce, and whether there is adequate national oversight of AI systems. Taken together, the objectives of the national AI initiative and the creation of the NAIAC will ensure strategic and timely approaches to the design and deployment of autonomous systems, as well as further establish national norms. Of equal importance is that the technology needs to be improved for domestic use cases as part of this national effort, especially in areas with the potential to create either differential treatment or disparate impact for federally protected and other vulnerable populations.",
              "If the U.",
              "S.",
              "excludes such considerations from national governance discussions, historic and systemic inequalities will be perpetuated, limiting the integration of the needs and lived experiences of certain groups into emerging AI innovations.",
              "Poor or inadequate decisions around financial services and creditworthiness, hiring, criminal justice, health care, education, and other scenarios that predict social and economic mobilities stifle inclusion and undercut democratic values such as equity and fairness.",
              "These and other potential harms must be paired with pragmatic solutions, starting with a comprehensive and universal definition of bias, or the specific harm being addressed.",
              "Further, the process must include solutions for legible and enforceable frameworks that bring equity into the design, execution, and auditing of computational models to thwart historical and present-day discrimination and other predatory outcomes.",
              "While the NAIAC is the appropriate next step in gathering input from various stakeholders within the private and public sectors, as well as from universities and civil society stakeholders, representatives from more inclusive and affected groups are also key to developing and executing a more resilient governance approach.",
              "In 2021, the Brookings Institution Center for Technology Innovation (CTI) convened a group of stakeholders prior to the NAIAC formation to better understand and discuss the U.",
              "S.",
              "\u2019s evolving positions on AI.",
              "Leaders represented national and local organizations advocating for various historically-disadvantaged and other vulnerable populations.",
              "The goal of the Brookings dialogue was to delve into existing federal efforts to identify areas for more deliberate exchange for civil and equal rights protections.",
              "In the end, roundtable experts called for increased attention to be paid to the intended and unintended consequences of AI on more vulnerable populations.",
              "Experts also overwhelmingly found that any national governance structure must include analyses of sensitive use cases that are exacerbated when AI systems leverage poor quality data, rush to innovate without consideration of existing civil rights protections, and fail to account for the broader societal implications of inequalities that embolden AI systems to discriminate against or surveil certain populations with greater precision.",
              "In some respects, the roundtable concurred with the need for a \u201cBill of Rights for an AI-Powered World,\u201d a framework introduced in 2021 by the White House Office of Science and Technology Policy (OSTP).",
              "Here, OSTP is calling for the clarification of \u201cthe rights and freedoms we expect data-driven technologies to respect\u201d and to establish general safeguards to prevent abuse in the U.",
              "S.",
              "But without direct discussion on how bias is defined in the public domain, and what specific use cases should be prioritized, the U.",
              "S.",
              "will wane in the protection and inclusion of historically disadvantaged groups as AI systems evolve.",
              "In this blog, we offer a brief overview of key points from the roundtable discussion, and further clarify definitions of bias that were shared during the roundtable.",
              "We also surface scenarios where the U.",
              "S.",
              "can effectuate change, including in the fields of law enforcement, hiring, financial services, and more.",
              "We conclude with priorities that could be undertaken by the newly established advisory committee, and the federal government writ large, to make progress on inclusive, responsible, and trustworthy AI systems for more vulnerable groups and their communities.",
              "To start, the U.",
              "S.",
              "needs a common understanding of AI and the related problems it can generate, which is important in a space where meanings can be ambiguous and in some instances fragmented.",
              "The National Artificial Intelligence Initiative has defined trustworthy AI as appropriately reflecting \u201ccharacteristics such as accuracy, explainability, interpretability, privacy, reliability, robustness, safety[,] .",
              ".",
              ".",
              "security or resilience to attacks,\u201d all while \u201censur[ing] that bias is mitigated.",
              "\u201d In a more recent report, NIST defined bias as \u201can effect that deprives a statistical result of representativeness by systemically distorting it.",
              "\u201d Added to these definitions are general definitions adopted among the private sector which equate bias mitigation with fairness models.",
              "A previous Brooking report approaches the definition from a more comparative lens, framing bias as \u201coutcomes which are systemically less favorable to individuals within a particular group and where there is no relevant difference between groups that justifies such harms.\u201d Further, the authors suggest that algorithmic biases in machine learning models can lead to decisions that can have a collective, disparate impact on certain groups of people even without the programmer\u2019s intention to discriminate.",
              "At face value, the U.",
              "S.",
              "definitions tend to be broad and somewhat generalized when compared to those from the EU, which has positioned AI according to practical degrees of risk.",
              "Most notably, the EU Artificial Intelligence Act categorizes AI use into three different tiers.",
              "Those of unacceptable risk would be prohibited (taking, for example, the use of facial recognition for law enforcement), while high risk systems would be authorized but subject to scrutiny before they can gain access to the EU market (taking, for example, AI used for hiring and calculating credit scores).",
              "Meanwhile, limited and minimal-risk AI, such as AI chatbots and AI use in inventory management, will be subject to light transparency obligations.",
              "Civil and human rights are factored into the definitions offered by the Organisation Economic Co-operation and Development (OECD) and other international bodies.",
              "The OECD defines innovative and trustworthy AI as those that include: respect for human rights and democratic values;",
              "setting standards for inclusive growth;",
              "human-centered values and fairness;",
              "transparency and explainability;",
              "robustness, security, and safety;",
              "and accountability.",
              "Compared to the U.",
              "S.",
              ", international entities have taken a more proactive, and perhaps proscriptive, approach to defining bias to ensure some common consensus on the harms being addressed.",
              "While roundtable participants didn\u2019t have full consensus on the most commonly accepted definition of AI bias, they did offer perspectives on the outcomes that should be further investigated, especially those that seem to collide with the public interest and equity.",
              "Generally, diversity and inclusion are treated as afterthoughts in AI development and execution, and flagged when systems go awry, resulting in quick fixes that do not address the breadth of such harmful technologies.",
              "Roundtable experts also shared that most biases occur as a consequence of poor data quality, which will be discussed later in the blog.",
              "Experts also pointed to the lack of privacy in this technological age, which continues to leave marginalized groups more vulnerable to unmitigated data collection without their knowledge.",
              "In sum, roundtable participants found that AI biases reflect larger systemic issues of societal discrimination, poor data quality, and the lack of data privacy protections.",
              "There was also mention of how the lack of workforce diversity in the computer and data sciences hinders more inclusive approaches.",
              "These factors shared during the roundtable overwhelmingly support why the U.",
              "S.",
              "needs more focused guidance on how to attain inclusive, equitable, and fair AI.",
              "The Biden administration has already centered equity among federal initiatives, including AI.",
              "Executive Order 13985, Advancing Racial Equity and Support for Underserved Communities Through the Federal Government, directs the U.",
              "S.",
              "Department of Defense to advance equitable AI by \u201cinvesting in agency-wide responsible AI development and investing in the development of a more diverse AI workforce, including through partnerships with Historically Black Colleges and Universities (HBCUs) and Minority Serving Institutions (MSIs).",
              "\u201d The previous administration provided a running start on AI governance when they dived into discussions and strategies for how federal agencies could harness the transformative capabilities of AI.",
              "The Equal Employment Opportunity Commission (EEOC) started this process in its own work focused on mitigating disparities in AI-driven hiring tools for people with disabilities.",
              "Yet, more needs to be done in the U.",
              "S.",
              "to affirmatively own the existence of online data biases, and flush out areas for change.",
              "The fact of the matter is that if the federal government gets bias identification and mitigation wrong, it will erode the trust in the efficacy of autonomous systems, especially among everyday citizens whose lives are becoming more dependent on them.",
              "Below are some of the use cases in housing, hiring, criminal justice, healthcare, finance, political disinformation, and facial recognition that are already raising red flags due to limited oversight.",
              "America has a long history of racist housing and lending policies, enabled by racialized policies including the Indian Removal Acts, Fugitive Slave Act, Repatriation Act, and more.",
              "Today, biases in home appraisals and loan approvals continue to pose systemic challenges in mortgage applications and ownership as zoning ordinances and redlining steepen gaps for Black applicants.",
              "While laws such as the Fair Housing Act of 1968 and the Equal Credit Opportunity Act of 1974 prevented housing discrimination on a mass scale, discrimination abounds with AI creating even greater precision in fostering inequities.",
              "Automated mortgage lending systems have been found to charge Black and Hispanic borrowers significantly higher prices for mortgage loans, at a difference of roughly $800 million a year.",
              "Meanwhile, online lenders have followed trends of discrimination set by face-to-face lenders, cumulatively rejecting a total of 1.",
              "3 million creditworthy Black and Latino applicants between 2008 and 2015.",
              "While some argue that app-based approvals have been shown to be 40% less likely to offer higher mortgage rates for borrowers of color and not reject an individual\u2019s application based on their race alone, the technology is still emboldening disparities when it comes to appraisals for existing owners, resulting in homes in majority Black neighborhoods being appraised for 23% less than properties in mostly white neighborhoods \u2013 even with the technology.",
              "Over the years, more and more companies are using AI to reduce operational costs and increase hiring efficiencies.",
              "However, these systems are not divorced from the differences that men and women experience in the workplace, and hiring algorithms have been shown to positively favor white people over people of color.",
              "For example, a study found that targeted ads on Facebook for supermarket cashiers were received by an audience that was 85% women, while jobs with taxi companies were shown to an audience that was 75% Black.",
              "In another instance, Amazon rolled back a hiring algorithm because it rejected female applicants, or any resume referencing women\u2019s activities;",
              "the algorithm was primarily trained on a largely male dataset of engineers.",
              "Added to these examples of AI in hiring is the use of emotion recognition technology (ERT) to evaluate candidates during the hiring process.",
              "Research has found that Black and Hispanic men have been passed over for employment when pre-screened by such ERT tools.",
              "The disproportionately negative results generated by the AI resulted in their disqualification early in the hiring process.",
              "A history of biased and discriminatory laws has reinforced racism in the criminal justice system, which disproportionately polices and incarcerates low-income people and people of color.",
              "Black people are incarcerated at five times the rates of white people.",
              "And the introduction of AI in this space has only created an additional perpetrator of injustices within the system.",
              "The PATTERN algorithm, created by the Department of Justice as part of the First Step Act, was used to predict recidivism and shorten criminal sentences based on good behavior.",
              "Yet the algorithm has been shown to exhibit biases against people of color, overpredicting recidivism among minority inmates at rates of two to eight percent compared to white inmates.",
              "Other risk assessment algorithms have exhibited similar biases, taking for example the COMPAS algorithm that had been used in the states of New York, Wisconsin, California, and more.",
              "A ProPublica article found that Black people were twice as likely as white people to be labeled high risk but not re-offend, while white people were more likely to be labeled as low risk but then re-offend.",
              "Such risk-assessment tools receive widespread use across the criminal justice system, from initial sentencing to determining early releases, exacerbating existing biases within the system with little oversight.",
              "AI use in healthcare has also been shown to exacerbate social inequities.",
              "An algorithm used to determine transplant list placement had a race coefficient that placed Black patients lower on the list than white patients, even though Black Americans are significantly more likely than white Americans to have kidney failure.",
              "Meanwhile, an algorithm used by hospitals to predict patients needing follow-up care identified a group of patients that only consisted of 18% Black patients and 82% white patients when the figures should have followed a nearly-50/50 split instead.",
              "Notably, an AI made for skin cancer detection had been primarily tested on white patients and failed to produce accurate diagnoses of darker-skinned patients.",
              "Many of these questions of life and death are left to the whims of biased technology, worsening existing health inequities faced by people of color.",
              "AI usage in financial systems perpetuate further bias.",
              "Some FinTech algorithms perpetuate lending patterns that are known to charge Latinx borrowers 7.",
              "9 basis points and African Americans 3.",
              "6 basis points more for buying and refinancing mortgages.",
              "While researchers have found that FinTech discriminates 40% less than face-to-face lenders, historically marginalized communities continue to be disproportionately impacted.",
              "There is also the problem of credit invisibility, which 44 million Americans suffer from as they are \u201cdisconnected from mainstream financial services and thus do not have a credit history.",
              "\u201d Since FinTech cannot assess future borrowing or credit behaviors among these populations due to the lack of data, they are still stricken by the wealth gaps in the U.",
              "S.",
              "that limit financial independence.",
              "Unmitigated data collection, coupled with the use of artificial intelligence in social media algorithms, have enabled bad actors in spreading disinformation targeting marginalized groups.",
              "During the 2016 U.",
              "S.",
              "presidential elections, Russian operatives took advantage of social media to target Black people, spreading messages seeking to incite racial conflict and discourage Black people from going to the ballot box.",
              "Online voter suppression, conducted through misleading information on the presence of law enforcement at polling places or the spread of incorrect voting information, has been used to target racial minorities and prevent them from casting their votes.",
              "Political disinformation threatens the civil rights of racial minorities, erecting barriers to their full participation in the American democracy.",
              "Many of the previously mentioned use cases rely upon face detection and facial recognition technologies, which has technical shortcomings when it comes to the identification and classification of diverse subjects.",
              "In law enforcement or criminal justice systems, the use of inaccurate facial recognition technologies (FRT) has resulted in multiple wrongful arrests, disproportionately affecting Black and Latino populations.",
              "FRT has also delivered similar sinister consequences when applied to the surveillance of public housing residents, whose access to apartment units has been dictated by FRT results.",
              "The depth and breadth of AI biases compel the need for principled guidance and solutions from the federal government.",
              "Further, digital competitiveness cannot be fully realized in the U.",
              "S.",
              "without a framework that proactively addresses these and other persistent domestic challenges restricting the inclusiveness of emerging technologies.",
              "But before offering several proposals for how the U.",
              "S.",
              "might proactively address online biases, roundtable participants also highlighted two additional elements that exacerbate AI biases \u2013 data quality and workforce diversity.",
              "Existing data documents the historically unjust treatment and underrepresentation of historically marginalized communities.",
              "For example, some job recruitment ads assume that people of color, or women, are less qualified as they are less represented in the workforce compared to mainstream populations, or white men.",
              "None of these assumptions consider the larger and broader societal factors and injustices in education and the labor market that have made it difficult for women or people of color to equitably participate in the labor market.",
              "Similarly, housing loans for Black and Brown communities tend to be disproportionately higher, a result of decades of discriminatory housing laws and redlining.",
              "In her work, University of Virginia Data Activist in Residence and Criminologist Renee Cummings refers to these disparities as \u201cdata trauma\u201d because they emulate the historical legacies that get baked into existing datasets and show up in machine learning algorithms. And because these nuances are tightly correlated with the society in which we live, the data becomes harder to disentangle from those explicit and unconscious assumptions.",
              "When incorporated into AI models, the data trauma is carried on and inflicted on future generations on whom the models are used among a plethora of use cases that determine social mobility, economic justice, and even civil rights.",
              "Another important consideration brought up by Brookings roundtable participants is the need for workforce diversity in AI.",
              "Facebook and Google have a nominal representation of women in their research staff at 15% and 10% respectively, according to the AI Now Institute.",
              "These figures go even lower for tech workers.",
              "Only 2.",
              "5% of Google\u2019s workforce are Black, and the numbers at Facebook and Microsoft are only at 4%.",
              "These problems go beyond the talent pipeline, as \u201cworkers in tech companies experience deep issues with workplace cultures, power asymmetries, harassment, exclusionary hiring practices, unfair compensation, and tokenization that are causing them to leave or avoid working in the AI sector altogether.",
              "\u201d The lack of diversity in tech spaces means that machine learning algorithms and other autonomous systems are being developed without the lived experiences necessary to avert poor data treatment, or create much better products or services.",
              "Going back to what the U.",
              "S.",
              "is doing now, the newly established NAIAC created five working groups during their first meeting, which included: leadership in trustworthy AI, leadership in research and development, supporting the U.",
              "S.",
              "workforce and providing opportunity, leadership in competitiveness, and international cooperation.",
              "In addition, they also established a subcommittee on AI and law enforcement, tasked with investigating issues of bias and data security.",
              "Despite being commercially-focused, the establishment of the subgroups signals the administration\u2019s commitment to human-centered AI, and in combatting existing biases commonly found in AI systems. Human-centered AI seeks to ensure the equitable and responsible use of technology while considering how explicit biases shape existing technologies.",
              "While these aspects of the NAIAC may potentially confront equitable and fair treatment of disparate groups, there still exists the need for civil rights activists and interdisciplinary experts to be part of these discussions.",
              "We also suggest a set of additional recommendations to inform the broader advisory and subgroups as they design and execute a national AI governance strategy.",
              "For years, civil rights activists have fought for equitable access to housing, loans, jobs, and more.",
              "In the previous examples, it is certain that AI biases will set back such progress, and leave people with little recourse for remediation of harmful and deceptive practices.",
              "The current civil rights regime is ill-equipped and outdated.",
              "For example, the Fair Housing Act prohibits housing discrimination based on race, disability, sex, and other factors.",
              "However, it does not account for discriminatory algorithms that allow renters to seek out people of specific races and ethnicities.",
              "Federal laws preventing voter intimidation fall short of tackling online disinformation seeking to alienate and frighten communities of color.",
              "Right now, it is also difficult for individuals to bring lawsuits against tech companies.",
              "As people\u2019s experiences differ on social media platforms, it is difficult for one person to ascertain if the information that they are viewing is different from others, especially that which is predatory.",
              "Among a long list of legislative actions in Congress, there are already bills in circulation seeking to combat digital harms. This includes legislation limiting targeted advertising, such as the Banning Surveillance Advertising Act, and legislation seeking to prevent online voter intimidation, such as the Deceptive Practices and Voter Intimidation Prevention Act of 2021.",
              "But efforts to update our civil rights regime and fight back against online harms will be integral in protecting the civil rights of historically marginalized groups.",
              "To create improved harmonization around harms reductions, an assessment of the existing civil rights regime is a starting point toward more responsible AI and greater equity.",
              "When contemplating AI risk, it is important to outline and specify which use cases require stringent oversight and regulatory actions.",
              "The NAIAC could be a vehicle to employ frameworks similar to the EU AI Act, specifying and classifying use cases with different degrees of risk to determine appropriate levels of regulation.",
              "There are also multiple agencies working to combat AI biases across different sectors, like the one previously mentioned at the U.",
              "S.",
              "Equal Employment Opportunity Commission (EEOC).",
              "Recently, NIST has also released guidelines on managing biased AI.",
              "To proceed, there should be an inventory, assessment, and coordination of red flag areas among government agencies that prompt discussions on both remedies, and potential enforcements to directly address higher-risk scenarios that foreclose on equal opportunities for vulnerable populations.",
              "AI governance should be democratized to allow input beyond technologists.",
              "A participatory framework should be created to allow public input, incorporate industry best practices, and provide consumer disclosures to maximize transparency for those most impacted by these new technologies.",
              "Audits and impact assessments will also be key in the rollout of new technologies, focusing in particular on determining disparate impact and the quality of data used and documentation kept.",
              "Particularly sensitive algorithms \u2013 especially those used by the federal government \u2013 should undergo regular reviews to evaluate their long-term impacts on more vulnerable groups.",
              "And consumer input should be more valued.",
              "In the status quo, there are limited means for consumers to provide suggestions and feedback to those creating algorithms. For example, the creation of regulatory sandboxes and consumer feedback loops for AI models that pose substantive risk to citizens and consumers could be used in deliberative, debiasing efforts.",
              "Much of the egregiousness in AI biases emanates from the existing systemic inequalities that are rooted in racism.",
              "While the diversification of developer teams, scrutiny of data biases, and widespread consumer input will help level the playing field in AI design and execution, it\u2019s not always enough.",
              "Understanding and identifying bias is an integral part of the efficacy and usefulness of an algorithm.",
              "That is why the U.",
              "S.",
              "needs best practices that uphold the integrity of the algorithmic design and execution processes, and avoid the explicit discriminatory and predatory practices that are institutionally innate.",
              "What the roundtable revealed is that an anti-racist framework that prompts policymakers to ensure inclusive representation by addressing structural challenges is need, which includes limited research endowments to minority-serving institutions, including HBCUs, HSIs, and even community colleges, or having guardrails on AI systems that potentially replicate in-person inequities, like in law enforcement applications.",
              "The main theme here is that AI will not cancel the historical and structural circumstances that led to such disparities if they are not intentionally acknowledged and addressed.",
              "The national AI governance dialogue provides the opportunity to set new norms for how the U.",
              "S.",
              "tackles AI biases.",
              "Having discussions on important issues creates the chance to clarify definitions and enact policy changes with a meaningful probability of mitigating biases and moving closer to a more inclusive economy.",
              "Whereas the process began with the Trump administration, the Biden White House can finish it, bringing to bear the non-technical considerations of emerging technologies.",
              "Beyond these important considerations, stakeholders in these emerging technologies must trace back to the roots of the problems, which lie in the lack of diversity in design teams and data that continues to carry on trauma and discrimination of the past.",
              "By reviewing the existing civil rights regime, outlining cases in need of oversight, encouraging more democratic participation in AI governance, and incorporating anti-racist principles into every aspect of the algorithmic design process, it is possible that, with the joint efforts of tech companies, government institutions, civil rights groups, and citizens, existing AI biases could be upended.",
              "More importantly, the protections for historically marginalized groups can be better integrated into national governance, bringing the U.",
              "S.",
              "closer to the goals of equal opportunity for all in the digital age.",
              "The authors are grateful for the contribution of experts from the 2021 meeting, and discussions following the formal meeting.",
              "Amazon, Apple, Facebook, Google, and Microsoft are general, unrestricted donors to the Brookings Institution.",
              "The findings, interpretations, and conclusions posted in this piece are solely those of the authors and not influenced by any donation.",
              "Related Content Mark MacCarthy May 10, 2022 Alex Engler May 9, 2022 Alex Engler March 23, 2022 Authors Artificial Intelligence Internet & Telecommunications Governance Studies Center for Technology Innovation John Villasenor November 22, 2023 Norman Eisen, Nicol Turner Lee, Colby Galliher, Jonathan Katz November 21, 2023 2023 The Brookings Institution, Washington DC 2:00 pm - 3:30 pm EST Get the latest from Brookings The Brookings Institution is a nonprofit organization based in Washington, D.",
              "C.",
              "Our mission is to conduct in-depth, nonpartisan research to improve policy and governance at local, national, and global levels.",
              "Copyright 2023 The Brookings Institution"
            ]
          }
        ],
        "AI systems should be tested for bias using various methods and tools, such as statistical analysis, simulation, or human evaluation.": [
          {
            "title": "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
            "url": "https://www.nist.gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights",
            "content": [
              "There\u2019s More to AI Bias Than Biased Data, NIST Report Highlights",
              "An official website of the United States government Here\u2019s how you know Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock ( Lock A locked padlock ) or https:// means you\u2019ve safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "https://www.",
              "nist.",
              "gov/news-events/news/2022/03/theres-more-ai-bias-biased-data-nist-report-highlights As a step toward improving our ability to identify and manage the harmful effects of bias in artificial intelligence (AI) systems, researchers at the National Institute of Standards and Technology (NIST) recommend widening the scope of where we look for the source of these biases \u2014 beyond the machine learning processes and data used to train AI software to the broader societal factors that influence how technology is developed.",
              "The recommendation is a core message of a revised NIST publication, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270), which reflects public comments the agency received on its draft version released last summer.",
              "As part of a larger effort to support the development of trustworthy and responsible AI, the document offers guidance connected to the AI Risk Management Framework that NIST is developing.",
              "According to NIST\u2019s Reva Schwartz, the main distinction between the draft and final versions of the publication is the new emphasis on how bias manifests itself not only in AI algorithms and the data used to train them, but also in the societal context in which AI systems are used.",
              "\u201cContext is everything,\u201d said Schwartz, principal investigator for AI bias and one of the report\u2019s authors.",
              "\u201cAI systems do not operate in isolation.",
              "They help people make decisions that directly affect other people\u2019s lives.",
              "If we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology, and the comments we received from a wide range of people and organizations emphasized this point.",
              "\u201d Bias in AI can harm humans.",
              "AI can make decisions that affect whether a person is admitted into a school, authorized for a bank loan or accepted as a rental applicant.",
              "It is relatively common knowledge that AI systems can exhibit biases that stem from their programming and data sources;",
              "for example, machine learning software could be trained on a dataset that underrepresents a particular gender or ethnic group.",
              "The revised NIST publication acknowledges that while these computational and statistical sources of bias remain highly important, they do not represent the full picture.",
              "A more complete understanding of bias must take into account human and systemic biases, which figure significantly in the new version.",
              "Systemic biases result from institutions operating in ways that disadvantage certain social groups, such as discriminating against individuals based on their race.",
              "Human biases can relate to how people use data to fill in missing information, such as a person\u2019s neighborhood of residence influencing how likely authorities would consider the person to be a crime suspect.",
              "When human, systemic and computational biases combine, they can form a pernicious mixture \u2014 especially when explicit guidance is lacking for addressing the risks associated with using AI systems. \u201cIf we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
              "Many of these factors go beyond the technology itself to the impacts of the technology.",
              "\u201d \u2014Reva Schwartz, principal investigator for AI bias To address these issues, the NIST authors make the case for a \u201csocio-technical\u201d approach to mitigating bias in AI.",
              "This approach involves a recognition that AI operates in a larger social context \u2014 and that purely technically based efforts to solve the problem of bias will come up short.",
              "\u201cOrganizations often default to overly technical solutions for AI bias issues,\u201d Schwartz said.",
              "\u201cBut these approaches do not adequately capture the societal impact of AI systems. The expansion of AI into many aspects of public life requires extending our view to consider AI within the larger social system in which it operates.",
              "\u201d Socio-technical approaches in AI are an emerging area, Schwartz said, and identifying measurement techniques to take these factors into consideration will require a broad set of disciplines and stakeholders.",
              "\u201cIt\u2019s important to bring in experts from various fields \u2014 not just engineering \u2014 and to listen to other organizations and communities about the impact of AI,\u201d she said.",
              "NIST is planning a series of public workshops over the next few months aimed at drafting a technical report for addressing AI bias and connecting the report with the AI Risk Management Framework.",
              "For more information and to register, visit the AI RMF workshop page.",
              "Webmaster | Contact Us | Our Other Offices"
            ]
          },
          {
            "title": "Mitigating AI/ML Bias in Context: Establishing Practices for Testing ...",
            "url": "https://csrc.nist.gov/pubs/pd/2022/11/09/mitigating-ai-ml-bias-in-context/final",
            "content": [
              "Mitigating AI/ML Bias in Context: Establishing Practices for Testing ...",
              "This is a potential security issue, you are being redirected to https://csrc.",
              "nist.",
              "gov.",
              "You have JavaScript disabled.",
              "This site requires JavaScript to be enabled for complete site functionality.",
              "Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock () or https:// means you've safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "Drafts for Public Comment All Public Drafts Final Pubs FIPS (standards) Special Publications (SPs) IR (interagency/internal reports) CSWP (cybersecurity white papers) ITL Bulletins Project Descriptions Journal Articles Conference Papers Books Security & Privacy Applications Technologies Sectors Laws & Regulations Activities & Products Computer Security Division Cryptographic Technology Secure Systems and Applications Security Components and Mechanisms Security Engineering and Risk Management Security Testing, Validation, and Measurement Applied Cybersecurity Division Cybersecurity and Privacy Applications National Cybersecurity Center of Excellence (NCCoE) National Initiative for Cybersecurity Education (NICE) Contact Us Documentation Topics Date Published: November 2022 Apostol Vassilev (NIST), Harold Booth (NIST), Murugiah Souppaya (NIST) Managing bias in an AI system is critical to establishing and maintaining trust in its operation.",
              "Despite its importance, bias in AI systems remains endemic across many application domains and can lead to harmful impacts regardless of intent.",
              "Bias is also context-dependent.",
              "To tackle this complex problem, we adopt a socio-technical approach to testing, evaluation, verification, and validation (TEVV) of AI systems in context.",
              "This approach connects the technology to societal values in order to develop recommended guidance for deploying AI/ML-based decision-making applications in a sector of the industry.",
              "This project will also look at the interplay between bias and cybersecurity.",
              "The project will leverage existing commercial and open-source technology in conjunction with the NIST Dioptra, an experimentation test platform for ML datasets and models.",
              "The initial phase of the project will focus on a proof-of-concept implementation for credit underwriting decisions in the financial services sector.",
              "This project will result in a freely available NIST AI/ML Practice Guide.",
              "Managing bias in an AI system is critical to establishing and maintaining trust in its operation.",
              "Despite its importance, bias in AI systems remains endemic across many application domains and can lead to harmful impacts regardless of intent.",
              "Bias is also context-dependent.",
              "To tackle this complex problem, we adopt a socio-technical approach to testing, evaluation, verification, and validation (TEVV) of AI systems in context.",
              "This approach connects the technology to societal values in order to develop recommended guidance for deploying AI/ML-based decision-making applications in a sector of the industry.",
              "This project will also look at the interplay between bias and cybersecurity.",
              "The project will leverage existing commercial and open-source technology in conjunction with the NIST Dioptra, an experimentation test platform for ML datasets and models.",
              "The initial phase of the project will focus on a proof-of-concept implementation for credit underwriting decisions in the financial services sector.",
              "This project will result in a freely available NIST AI/ML Practice Guide.",
              "None selected Publication: Project Description (pdf) Supplemental Material: Project homepage Document History: 08/18/22: Project Description (Draft) 11/09/22: Project Description (Final) risk management artificial intelligence financial services Want updates about CSRC and our publications?",
              "Subscribe Contact Us | Our Other Offices"
            ]
          },
          {
            "title": "Guidelines and quality criteria for artificial intelligence-based ...",
            "url": "https://www.nature.com/articles/s41746-021-00549-7",
            "content": [
              "Guidelines and quality criteria for artificial intelligence-based ...",
              "Thank you for visiting nature.",
              "com.",
              "You are using a browser version with limited support for CSS.",
              "To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer).",
              "In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.",
              "Advertisement npj Digital Medicine volume 5, Article number: 2 (2022) Cite this article 35k Accesses 89 Citations 108 Altmetric Metrics details While the opportunities of ML and AI in healthcare are promising, the growth of complex data-driven prediction models requires careful quality and applicability assessment before they are applied and disseminated in daily practice.",
              "This scoping review aimed to identify actionable guidance for those closely involved in AI-based prediction model (AIPM) development, evaluation and implementation including software engineers, data scientists, and healthcare professionals and to identify potential gaps in this guidance.",
              "We performed a scoping review of the relevant literature providing guidance or quality criteria regarding the development, evaluation, and implementation of AIPMs using a comprehensive multi-stage screening strategy.",
              "PubMed, Web of Science, and the ACM Digital Library were searched, and AI experts were consulted.",
              "Topics were extracted from the identified literature and summarized across the six phases at the core of this review: (1) data preparation, (2) AIPM development, (3) AIPM validation, (4) software development, (5) AIPM impact assessment, and (6) AIPM implementation into daily healthcare practice.",
              "From 2683 unique hits, 72 relevant guidance documents were identified.",
              "Substantial guidance was found for data preparation, AIPM development and AIPM validation (phases 1\u20133), while later phases clearly have received less attention (software development, impact assessment and implementation) in the scientific literature.",
              "The six phases of the AIPM development, evaluation and implementation cycle provide a framework for responsible introduction of AI-based prediction models in healthcare.",
              "Additional domain and technology specific research may be necessary and more practical experience with implementing AIPMs is needed to support further guidance.",
              "Prediction models have a prominent role in healthcare research and practice.",
              "Diagnostic prediction models make predictions about the current health status of a patient, whereas prognostic prediction models estimate the probability of a health outcome in the future1,2.",
              "Methods from the machine-learning (ML) domain and its broader field of Artificial Intelligence (AI) have seen a rapid increase in popularity for prediction modeling.",
              "While the opportunities of ML and AI in healthcare are promising, the growth of complex data-driven prediction models requires careful quality and applicability assessment to guarantee their performance, safety and usability before they are used and disseminated in practice.",
              "A framework for structured quality assessment across the entire AI-based prediction model (AIPM) development, evaluation and implementation cycle is still missing.",
              "Such a framework is needed to ensure safe and responsible application of AIPMs in healthcare.",
              "For example, it can provide guidance on the appropriate validation steps needed before implementation to prevent faulty decision making based on overfitted models.",
              "The absence of such a framework may have contributed to relatively few models having been implemented to date3.",
              "We define the term AI-based prediction model (AIPM) as follows: a data-driven model that provides probabilistic patient-level predictions of the current presence or future occurrence of a certain outcome (e.",
              "g., a certain patient condition), given certain input (e.",
              "g., certain patient characteristics, genetic markers, medical images, or other types of features).",
              "We aimed to identify existing guidelines and quality criteria regarding six predefined phases of the AI-based prediction model development, evaluation and implementation cycle.",
              "The six AIPM development phases range from preparation and data collection to implementation in daily healthcare practice (see Box 1) and form the core structure and driver for this review.",
              "These phases are based on the predominant phases in clinical prediction model research4,5.",
              "We performed a scoping review to outline the most important aspects to consider in each phase, while providing pointers to relevant guidelines and quality criteria in the recent literature, focusing on actionable guidance for those closely involved in the AIPM development, evaluation and implementation cycle (e.",
              "g., software engineers, data scientists, but also health professionals).",
              "We also aimed to identify gaps in the existing guidance.",
              "Phase 1.",
              "Preparation, collection, and checking of the data: the preparation, collection and checking of the data to facilitate proper AIPM development (phase 2) and AIPM validation (phase 3).",
              "Phase 2.",
              "Development of the AIPM: the modeling of the relation between the predictive input variables (features/predictors) and the health outcome of interest, via a mathematical formula or algorithm.",
              "Phase 3.",
              "Validation of the AIPM: the testing (validating) how well the developed AIPM from phase 2 predicts the outcome in individuals whose data were not used during AIPM development (so called external validation data), quantifying the AIPM\u2019s predictive performance.",
              "Phase 4.",
              "Development of the software application: the development of the software application, containing the programming, design, usage and support of the digital packaging of the AIPM.",
              "Phase 5.",
              "Impact assessment of the AIPM with software: the assessment of the impact of the usage of the AIPM and software on daily healthcare practice, patient or individual health outcomes, and healthcare costs.",
              "Phase 6.",
              "Implementation and use in daily healthcare practice: the implementation of the AIPM in routine care, including maintenance, post-deployment monitoring, and updating.",
              "1These phases are primarily introduced to provide a clear structure to the article.",
              "In practice, the order of these phases may slightly differ.",
              "A multi-stage screening strategy was used for this scoping review driven by the six AIPM development phases (Fig.",
              "1).",
              "We searched for relevant academic literature published from January 2000 up to January 2021 in three online databases containing a variety of medical, technical, ethical, and social science literature: PubMed, Web of Science, and ACM Digital Library.",
              "The search strings consisted of a combination of search terms related to: (i) guidelines, quality criteria, best practices and reporting standards (ii) artificial intelligence, including machine-learning and prediction modeling in general and (iii) topics relating to one of the six phases of AIPM development (see Box 1), such as \u2018data cleaning\u2019 for phase 1 and \u2018impact assessment\u2019 for phase 5.",
              "For the complete search strings, and a filled PRISMA reporting checklist for scoping reviews, see Supplementary Tables 1 and 2, respectively.",
              "This flow diagram displays the screening strategy for the inclusion of guidance documents in this scoping review.",
              "We used the following inclusion criteria for our review process: (i) documents (e.",
              "g., reports, articles, or guidelines) primarily aimed at the individuals directly involved with the development, evaluation, and implementation of AIPMs (excluding institution or organization wide guidance) and (ii) documents with actionable guidance (e.",
              "g., clearly defined recommendations on how to develop AIPMs and implement them into practice).",
              "The following exclusion criteria were used: (i) guidance limited to one medical domain (e.",
              "g., cardiology) without generalizing to other domains, (ii) guidance limited to one AI technique (e.",
              "g., reinforcement learning) without generalizing to other techniques, (iii) guidance aimed at governing institutions, (iv) documents published before 2000, (v) guidance limited to the prerequisites to develop, validate and implement an AIPM (e.",
              "g., documents focusing on the development of data infrastructures or legal and governance frameworks), and vi) documents not written in English.",
              "Two reviewers (AdH and AL) performed title and abstract screening of the documents produced by the online database search.",
              "Additional literature was added through manually scrutinizing (snowballing) the reference lists of the identified documents.",
              "We also asked a convenience sample of 14 AI experts from academia and industry to provide potentially relevant sources (see Supplementary Table 3).",
              "These additional search strategies were specifically aimed at identifying gray literature consisting of government, institutional or industry documents and websites.",
              "The two reviewers performed a full-text screening on all retained literature (including gray literature).",
              "Conflicts regarding the eligibility of documents during the screening process were resolved by consensus in regular sessions between the two reviewers.",
              "For the data extraction, two reviewers (AdH and AL) independently identified keywords from each included document which represented the area on which guidance was provided (e.",
              "g., development, parameter tuning).",
              "Each keyword was mapped to more central topics pertaining directly to the AIPM development phases (e.",
              "g., development and parameter tuning were mapped to AIPM training).",
              "When applicable to more than one phase, the keyword was placed in a phase-overarching topic (e.",
              "g., algorithmic bias).",
              "The mapping was adjusted and fine-tuned repeatedly over the course of data extraction and validated based on the input from three co-authors (IK, SN, and MvS).",
              "During a second full-text screening round, all identified guidance was extracted according to the topics, summarized, and placed in the review section corresponding to that phase-specific or phase-overarching topic.",
              "After removing duplicates, the search resulted in 2683 documents.",
              "The title and abstract screening reduced this number to 89 documents.",
              "Snowballing added 51 documents.",
              "A total of 27 papers from online databases, 23 from manual inclusion and 22 from expert consultation, were retained after full-text screening.",
              "This led to a total of 72 documents included in the review (Fig.",
              "1).",
              "Data extraction resulted in 138 keywords, which were mapped to 27 phase-specific topics and 6 phase-overarching topics (see Supplementary Table 4).",
              "In the next sections, the summarized guidance is structured per phase.",
              "The phase-overarching topics are summarized in Box 2 and further integrated in the phase-specific summaries (as shown in Supplementary Table 5).",
              "Supplementary Table 6 can be used as a lookup table structuring the hyperlinks to the identified guidance per phase and supplementary Table 7 provides the affiliations (industry, academia, governing), geographical region and type of source (literature search, snowballing, expert consultation).",
              "Algorithmic bias refers to an AIPM that systematically disadvantages individuals belonging to a particular subgroup when there is no a priori medical justification for this discrepancy22,23,24,25,61,62,63,72,91.",
              "Subgroups can for example be based on gender, race, culture, religion, age, sexual orientation, socioeconomic background, ability status and ethnicity6,7,22,23,24,26,32,42,61,72,79,91.",
              "There are two important causes for algorithmic bias: non-representative development data8,16,19,20,22,23,24,25,26,27,43,46,56,58,59 and historical human biases that are reflected in data22,23,24,25,62.",
              "The field of AI fairness aims to address algorithmic bias by studying how best to identify and mitigate it23,43.",
              "Transparency and openness entail the possibility to inspect sufficient details on e.",
              "g., study design, data selection, analytical scripts, the AIPM model and modeling approach, justifications, and limitations, in a way that could allow others to reproduce the process (e.",
              "g., for independent external validation of the AIPM)9,22,40,58,62,125,126.",
              "Recommendations regarding transparency often involve detailed reporting, following relevant reporting guidelines6,8,37,50,64,65,72, and sharing of relevant information, code, and data across the different phases.",
              "Interpretability of an AIPM refers to the degree to which a human can understand how an AIPM comes to its predictions or classifications75.",
              "Being able to interpret an AIPM may facilitate detection of potential errors and biases in its predictions7,8,27,61.",
              "This may be an important factor in obtaining trust and acceptance by end users (e.",
              "g., healthcare professionals and patients)10,24,26,40,47,72,73,87.",
              "Interpretability and transparency are closely related.",
              "For example, an interpretable AIPM may allow a physician to be more transparent about the decision-making process to patients16,22,40,41,62,87.",
              "Team members, end users, and stakeholders should be considered carefully throughout the AIMP lifecycle (see Box 1).",
              "It has been recommended that already from the start the AIPM development team must cover a multidisciplinary technical, methodological and medical expertise8,11,17,20,23,26,32,71, consider data and project management8,11,18,20,26, and attend to the diversity of the anticipated end users of the AIPM11,20,26,62,66.",
              "Identifying and involving the right expertise and stakeholders in each consecutive phase of the AIPM development, evaluation and implementation cycle is crucial for its success in daily healthcare practice8,18,22,23,26,32,58,79,88.",
              "Security encompasses the protection of the AIPM and its (personal) data against malicious actors9,22,32.",
              "Two risks particularly concerning an AIPM are the misuse of the (often large amounts of) development and validation data23,33 and software vulnerabilities introduced by the new AIPM code and infrastructure23.",
              "Security measures protecting against these vulnerabilities form part of the AIPM architecture and should be tested before deployment32.",
              "Risks refer to any (unintended) consequences of the AIPM\u2019s application that threaten the AIPM\u2019s safe and effective application9,18,20,57.",
              "Potential risks are flaws in the design of the AIPM, technical defects, inappropriate or malicious use, process changes, security breaches (see Security above), and disparate outcomes for different use cases or subgroups (see algorithmic bias and fairness above)12,26,28.",
              "Safety (for patients and healthcare professionals) should be considered during all phases of AIPM development20.",
              "2An index on where each phase-overarching topic is further discussed in the article can be found in Supplementary Table 5.",
              "One of the very first aspects of developing and validating an AIPM as recommended in literature is to clearly specify the medical problem and context that the AIPM will address, and to identify the healthcare setting(s) in which the AIPM is to be deployed3,6,7,8,9,10,11,12,13,14,15.",
              "Before starting actual AIPM development, it is advocated to first conduct a thorough investigation into the current standard of care, context and workflow7,8,9,10,11,14,15,16,17,18, and to provide a clear rationale for why the current approach falls short.",
              "For example, via analysis of the needs of targeted end users through observations and interviews, and by involving them from the start in the developmental process11,12,17,18,19,20.",
              "Once a precise (diagnostic or prognostic) prediction task has been formulated, healthcare actions, treatments or interventions should be defined that are to follow from the AIPM predictions3,6,7,8,10,11,13,17,21.",
              "Clinical success criteria must be determined and described3,6,7,9,11,12,20,22, including an analysis of the potential risks of prediction errors6,23.",
              "Developers are advised to perform a feasibility check to assess at an early stage whether the expected benefit of the AIPM to the healthcare system outweighs the costs of developing the AIPM, its maintenance, and other consequences of incorrect (or unfair) use of the predictions of the AIPM9,10,11,12,22,24,25,26,27,28.",
              "The literature advocates that, before starting data collection, the development team should ensure compliance with relevant privacy legislation (e.",
              "g., General Data Protection Regulation (GDPR)29, the Personal Information Protection and Electronic Documents Act (PIPEDA)30 or the Health Insurance Portability and Accountability Act (HIPAA)31) and take measures to protect the privacy of the individuals whose data are used for AIPM development, evaluation, or application8,12,20,23,26,32,33,34,35,36.",
              "Consultation with data protection specialists has been recommended23.",
              "Legislation may require identification of the right legal basis (such as informed consent) for processing confidential information of individuals12,20,26,32,33,36,37.",
              "In many cases, individuals must be informed about the processing of their personal data20,23,29,35,36,38.",
              "In the case of using (existing) data that was originally collected for a purpose unrelated to the AIPM (e.",
              "g., patient care), there must be an adequate processing basis for re-using these data for AIPM-related purposes23,35.",
              "The legal basis can be different for the development and validating versus deployment phases of AIPMs23,33.",
              "More specifically, data subjects may not be directly affected by AIPM development but are often affected by AIPM deployment as the AIPM\u2019s predictions could influence the treatment decisions of data subjects.",
              "Depending on local legislation, it can be required (e.",
              "g., under GDPR29 or the Canadian Privacy Act39) to develop a data protection impact assessment23,26,32,33,35,40,41, assign a data protection officer23,26,36, and take measures to conduct data protection oversight, by limiting access only to necessary and qualified personnel23,26,35.",
              "Moreover, taking measures to achieve privacy by design12,23,26,32,35,36,41,42,43, such as data minimization23,35,41, encryption35,41, or the use of data pseudonymization or anonymization methods is recommended35,41.",
              "The use (or absence) of such methods should be clearly motivated8,12,14,20,26,35,44, especially whenever patient data leave primary care systems8.",
              "Any trade-offs between predictive performance and privacy should be considered23.",
              "Finally, under some data protection regulations, individuals have the right to withdraw consent, the right to object, and the right to be forgotten (e.",
              "g., under GDPR29 and the California Consumer Privacy Act45), which should be considered and implemented throughout development and deployment stages of the AIPM12,23,36,41.",
              "It is recommended that the amount of collected data is sufficiently large for the intended purpose6,8,12,15,20,22,26,46,47,48,49, is ideally prespecified8 and should be clearly reported3,14,37,46,50.",
              "The required sample size for AIPM development depends on the specific context, including the used prediction modeling method, the number of features, the proportion of the predicted health outcome (in case of categorical outcomes), and the desired predictive performance47,48, which may be linked to a minimal required clinical impact8.",
              "For regression-based methods48, and a selection of machine-learning-based methods47, technique-specific a priori sample size calculations are available, although for many model architectures and settings (e.",
              "g., semi-supervised learning, decision trees, or convolutional neural networks) no specific guidance was found.",
              "If some (closely related) data are already available, it has been suggested to inspect the model\u2019s learning curve in that data, setting out prediction performance against the amount of used data, to estimate the required total sample size for a specific use case47,51,52.",
              "For external predictive performance evaluation (discussed in more detail in phase 3), as a rule of thumb, it has been suggested that the sample should at least contain 100 events per outcome53, but for binary and continuous outcomes more specific sample size calculations are now available54,55.",
              "The literature recommends that the collected data are representative of the target population and intended healthcare setting, and sufficiently cover the relevant real-world heterogeneity and diversity7,9,12,26,27,32,37,48,56,57.",
              "This representativeness criterion is considered crucial to assess and combat algorithmic bias8,16,19,20,22,23,24,25,26,27,43,46,56,58,59 and poor calibration60.",
              "Thorough assessment of the representativeness of the data is strongly advised6,7,13,14,16,26,37,46,56,57, for which a detailed description of the collected data is required, including the time span of data collection3,6,7,9,12,21,22,37,61, the collection site and setting3,7,14,15,20,21,22,24,42,46,61,62,63, relevant population characteristics such as gender, age, ethnicity, and relevant medical history3,7,15,21,37,46, and any inclusion or exclusion criteria that were used3,6,7,9,13,14,15,16,20,21,37,50,56,64,65.",
              "Finally, revaluation and reporting of any differences between the collected data and the intended target population and setting is emphasized3,6,13,14,16,24,26,46,56,57, including which groups may be underrepresented in the data with respect to the target population.",
              "Extensive assessment of data quality has been widely recommended6,7,12,13,16,22,24,26,33,37,64,65.",
              "For both feature variables as well as outcomes, this involves the inspection and description of missing data, consideration of potential errors in measurement, and their underlying mechanisms (e.",
              "g., random or systematic)3,6,9,13,15,16,17,20,22,27,37,46,47,66,67.",
              "A clear definition of how and when each variable was measured should be provided3,6,9,12,13,14,15,17,21,22,25,37,46,50,58,62,64,65, including specification of measurement instruments or tools (e.",
              "g., make and model of devices).",
              "Any known data quality risks and limitations should be reported and related to potential impact on the AIPM\u2019s predictions and its validation (with special attention to algorithmic bias)3,13,20,22,26,32,33,37,43,57.",
              "An additional validity check could be performed by randomly sampling a portion of the data and manually checking it for errors28,61.",
              "The proportion of errors should be reported61.",
              "The literature also recommends the installation of a process through which data errors can be corrected43,61.",
              "Note that when such a process is installed, it should also be employed during implementation and not just during model development.",
              "It must be clearly identified whether data were collected retrospectively or prospectively6,14,15,21,46.",
              "Prospective data collection may be preferred as it more closely matches the real-world operating conditions56.",
              "It was pointed out that one should be aware of potential quality risks of routinely collected data as such data are often collected for a different purpose56,68.",
              "The literature places a particular emphasis on the quality of outcome data, more specifically the reference standard or \u2018ground truth\u2019.",
              "A clear rationale on outcome data collection needs to be provided (e.",
              "g., an expert panel, biopsy, clinical determination via laboratory tests), and any potential quality issues3,6,13,14,15,21,46.",
              "In case the outcome data were manually labeled, the AIPM development and validation team are urged to precisely specify how and by whom data were labeled, including the level of experience of the labelers, and elaborate on relevant pitfalls or difficult cases8,10,15,21,46,64,65,67.",
              "Ideally, to ensure label quality and prevent bias in AIPM evaluation, it was advised that this is a well-defined and controlled process46,66, where experts labeling the data work independently from each other8,21, and are not directly involved in performance assessment of the AIPM15,46.",
              "Depending on the exact procedure, inter-observer variability or test reproducibility8,15,21,46 should be calculated to obtain an assessment of label quality.",
              "To prepare data for the consecutive phases, or handle identified data quality issues, data preprocessing steps may be applied.",
              "Such preprocessing steps can include splitting the data into different subsets (e.",
              "g., train, tuning, and test sets), augmenting data, removing outliers, re-coding or transforming variables, standardization, and imputation of missing data6,13,17,27,46,47,49,68.",
              "The literature stresses that detailed description of any preprocessing steps applied to the raw data should be provided, including software used to perform the processing steps3,6,7,9,13,14,15,22,50,61,62,64,65.",
              "Missing data imputation is generally recommended over complete case analysis where incomplete data are excluded, but this should depend on the underlying missing data mechanism (missing completely at random, missing at random, or missing not at random)13,17,47,49,68.",
              "Any data augmentation should be carefully considered against the potential introduction of bias, and model developers are advised to collaborate with domain experts on these preprocessing steps9,22,46.",
              "Finally, the literature stresses that data splitting actions, must happen before any other preprocessing steps are applied (e.",
              "g., missing data imputation or standardization)27,69,70.",
              "This is crucial to prevent information leakage between data subsets, which leads to overoptimistic AIPM predictive performance.",
              "To facilitate interoperability, and easier adoption of the AIPM into healthcare settings, it has been recommended to align data management with relevant coding standards and widely adopted protocols20,26.",
              "Relevant standards may include SNOMED CT for coding clinical data, ICD-10 and OPCS4 for clinical conditions and procedures20.",
              "Additionally, adopting data exchange protocols in the final AIPM software design has been recommended, but is discussed later in the article (in phase 4, about development of the software application).",
              "The literature indicates that the following aspects may affect the choice for a certain modeling technique (e.",
              "g., regression, decision tree, neural network): prediction performance, interpretability, the familiarity of the modeling technique to the end user, computational requirements, development and validation costs, maintenance, privacy, sample size, and the structure of the data6,9,13,16,17,22,23,71.",
              "It is recommended that any motivations for choosing a modeling technique should be clearly articulated6,8,13,14,20,23,24,26, including benefits and potential risks associated with the chosen technique13,16,20,23,24,26,32.",
              "Facilitating interpretability of the AIPM, e.",
              "g., by providing insight into the impact of each feature or predictor on the predicted outcome6,14,16,47,57,72,73, is frequently mentioned as an important aspect for AIPM acceptance into healthcare practice10,24,26,40,47,72.",
              "Important to note is that the term AIPM interpretability\u2014in this scoping review - does not imply causal interpretability (e.",
              "g., high feature impact does not imply causal influence of that feature on the actual health outcome).",
              "Interpretability may help to detect trivial and erroneous AIPMs7,27, provide medical domain experts with a possibility to discuss whether the associations on which the AIPM relies are likely to remain stable8,27,61, help to identify algorithmic bias7,22,24,27,40,43, provide information on where the AIPM could be most easily attacked27, or how the AIPM may behave under dataset shift7.",
              "Neural networks are for example recommended for high volume, dense, and complex data types13,74, but they are also considered black boxes23,24,33, for which additional model-agnostic interpretation tools (explainable AI) are needed to give insight into the importance of individual features for the predictions13,23,24,33,57,75.",
              "This is in contrast with linear regression and decision trees, which have been considered inherently interpretable approaches.",
              "Irrespective of the modeling choice, facilitating interpretability is generally encouraged13,23,24,32,33,40,41,57,62,71, in particular when AIPMs rely on sensitive social and demographic data, or if the AIPM\u2019s predictions significantly affect healthcare decision making and a patient\u2019s treatment16,22,41.",
              "Moreover, under the GDPR29, patients have a right to an explanation that enables them to understand why a particular decision was reached36,40,41.",
              "If a form of interpretability is required, the underlying reasons should be made explicit9,40.",
              "Training (or fitting) the AIPM is the process of determining the values of any model parameters (e.",
              "g., also called weights, or coefficients) of the AIPM.",
              "Beside model parameters, AIPM development involves choosing hyperparameters, which influence model training and design, but are not necessarily part of the AIPM itself (e.",
              "g., penalization factors of shrinkage, learning rates, or the depth of tree-based methods).",
              "Automatic optimization of hyperparameters (also referred to as tuning) has been recommended9,27,66,76,77, for example, via nested cross-validation, or using a small representative held-out tuning dataset.",
              "To foster transparency and replicability it is advised that any details about training and hyper-parameter optimization procedures should be reported, including the final values of the (hyper-)parameters, the number of intermediate models trained to come to the final model, and an evaluation of predictive performance on the training data3,8,13,14,15,50,61.",
              "The goal of internal validation is to assess the predictive performance of an AIPM in data that are unseen with respect to model training but come from the same population and setting.",
              "To assess AIPM performance, the literature stresses that data should be strictly separated into training, tuning and test sets7,8,13,77, possibly stratified by the outcome event9,27 to prevent data leakage, which can result in optimistically biased evaluation7,13,27,69.",
              "Here, the training data is used to train the AIPM, the tuning data for optimizing the hyperparameters, and the test data for assessing the AIPM model performance.",
              "Variations on the simplistic \u2018split sample\u2019 validation have been suggested for better data efficiency and heterogeneity assessment (e.",
              "g., k-fold cross-validation or bootstrapping).",
              "Especially for small datasets, a cross-validated procedure is recommended13,27.",
              "The cross-validated procedure should incorporate all processing steps (standardization, imputation etc.)",
              "on the data to prevent data leakage9,69.",
              "The split of the data and any potential repeats of this splitting procedure should be reported13,14,50.",
              "Following the literature, the performance evaluation should be based on at least discrimination and calibration5,6,9,13,17,49,56,78.",
              "Discrimination refers to the ability of the AIPM to distinguish between subjects with and without the outcome of interest.",
              "It is recommended to define the metrics used to measure discrimination prior to the validation6,8,13.",
              "The chosen metrics should correspond with the intended medical use and should be chosen in close collaboration with domain experts (e.",
              "g., an AIPM estimating the risk of breast cancer should be highly sensitive)7,8,9,14,15,16,19,57,79,80.",
              "Discrimination is commonly quantified by the area under the receiver operating characteristic curve9,15,17,46,49,56,69.",
              "In the case of a clearly defined probability threshold, other metrics could also be used like sensitivity (also labeled: \u2018recall\u2019) and specificity, or the positive and negative predictive value (also precision)9,10,19,72,80.",
              "Note that fixed probability thresholds are not always considered necessary and when they are, they should be carefully determined in collaboration with medical experts81.",
              "Calibration refers to the concordance between predicted and observed probabilities.",
              "A calibration plot is the recommended method to evaluate calibration6,17,49,56,60.",
              "Discrimination and calibration evaluation metrics should be documented for all datasets13,14,16.",
              "It is recommended to calculate confidence intervals to accompany these metrics8,10,14,15,21,22,24,27,46,61.",
              "For some application types, Decision Curve Analysis (DCA) is considered a valuable addition to the discrimination and calibration of the AIPM.",
              "This performance assessment quantifies how the AIPM could impact patient care within the relatable workflow.",
              "Unlike discrimination and calibration, DCA derives the clinical utility from the predictive performance5,6,17,49,68,72.",
              "Promising results in a DCA can provide a clear indication that an AIPM could benefit daily healthcare practice.",
              "It could therefore serve as a precursor (but not a replacement) of a prospective impact study or more fully developed cost-effectiveness analysis (see phase 5).",
              "If an AIPM is adapted too much to the training data, and therefore its predictions no longer generalize well to new individuals not used for the development of the AIPM, the model is said to be overfitted8,47,56,60,76,78.",
              "Often mentioned factors contributing to overfitting are a small sample size in combination with many candidate features, perfect separation on rare categories, and a large imbalance resulting in a small number of events for one of the outcomes6,47,49,72,76,77,82.",
              "To prevent overfitting, a multitude of strategies are available, often aimed at reducing AIPM complexity.",
              "It has been widely recommended to report any measures taken to prevent overfitting3,7,8,13,15.",
              "One commonly referred strategy is feature selection13,15,27,47,76, for which it is explicitly recommended that selection should work independently of model training (unlike in methods as forward and backward selection) and is best informed - a priori - by medical expert knowledge or existing literature13,17,76.",
              "Other suggested strategies to combat overfitting are dimensionality reduction47,76, which can be implicit (e.",
              "g., common in neural networks)76, and explicit penalization of complexity (e.",
              "g., regularization)17,49,76.",
              "It should be noted that when the sample size is simply too small, even penalization methods have been shown ineffective to mitigate overfitting83,84.",
              "The literature indicates that tools to identify and mitigate algorithmic bias should also be developed in the AIPM development phase when applicable.",
              "First, a definition of fairness should be chosen that corresponds with the AIPM\u2019s intended use16.",
              "This definition should be integrated with model development as part of the AIPM\u2019s evaluation metrics22,24,25.",
              "Examples of fairness metrics are outcome parity22,23,25,42,43, true (false) positive (negative) rate parity22,23,25,42,43,79, positive (negative) predictive value parity22,42,43, individual fairness22, counterfactual fairness22,24,42,59, and equal calibration23.",
              "Developers are advised to make the chosen fairness metrics available in a Fairness Position or Bias Impact Statement that is reviewed by stakeholders22,23,25,26,62.",
              "They are also advised to avoid modeling techniques for which it is altogether impossible to evaluate algorithmic bias in an AIPM, for example due to the high dimensionality of its architecture22.",
              "Upon identification, algorithmic bias should be addressed by employing an appropriate mitigation strategy during AIPM development, which may be different for different applications and domains.",
              "When the bias is caused by unrepresentative training data, the main recommendation is to redo the data collection to rectify this8,16,19,20,22,23,24,25,26,27,43,46,56,58,59.",
              "Unrepresentative training data may also be addressed by undersampling the overrepresented group or oversampling the underrepresented group23,42.",
              "However, this may cause miscalibration of the model predictions and should be used with caution85.",
              "The most popular recommendation addressing other causes of algorithmic bias (e.",
              "g., historical human biases reflected in the data) is to exclude or reweigh the features causing the algorithmic bias22,23,25,27,43, although this may not eliminate the bias altogether.",
              "Alternatively, the predictions themselves can be reweighed by adjusting the probability threshold per subgroup42,43.",
              "Lesser mentioned recommendations consist of the application of fairness optimization constraints during AIPM training42,43 and the development of separate models per specific subgroup23.",
              "Note that the preconceptions and biases of designers can be replicated in their modeling choices22.",
              "It is therefore considered important to compose a diverse development team17,22,23,24,25, create awareness and involve stakeholders in design choices22,24,26,27,72.",
              "Also, developers should keep evaluating algorithmic bias at every stage of the development process32.",
              "The literature advocates that the final AIPM structure should be described in detail, covering input, outputs, and all intermediate layers or parameters3,14,15,50.",
              "To facilitate transparency and reproducibility of the developmental process, the used computational architecture, high-performance techniques, software packages, and versioning (data, model, configurations and training scripts) should be reported13,14,16,50,64,65,66.",
              "Code for the complete model building pipeline should be published in well-documented scripts with computer environment requirements when possible7,8,13,14,16,19,20,24,25,27,33,50,62,64,65, including statements about any restrictions to access or re-use.",
              "In practice, an AIPM is likely to be applied in a setting that differs from the setting in which the AIPM was developed, which may have an impact on AIPM performance.",
              "In contrast to internal validation (phase 2), external validation is the application of an existing model without any modifications to data from a different population or setting compared to model development (see Generalizability below).",
              "The literature highly recommends external validation for all AIPM applications when applied to a new setting3,9,17,49,86.",
              "Similar to internal validation of the AIPM, external AIPM model validation can be based on discrimination (area under the receiver operating characteristic curve, sensitivity, specificity, positive and negative predictive values), calibration (calibration plot)5,6,13,17,49,56,78, and Decision Curve Analysis5,6,17,49,68,72.",
              "When possible, the literature recommends the comparison of current best practice (e.",
              "g., an existing prediction model or medical decision rule) to the AIPM performance7,8,14,15,16.",
              "External validation can be performed on retrospective or prospective data.",
              "Although prospective validation is rare, it is preferred by the literature5,14,56, as it provides a better idea of the AIPM\u2019s true applicability to medical practice and allows the healthcare professionals to identify and review errors in real time19,72.",
              "External validation is ideally performed by independent researchers from other institutions or settings3,8,10,16,27,68,72.",
              "The necessity for external validation by independent researchers may depend on the risks posed by the application (for example based on the level of autonomy of an AIPM)80.",
              "Generalizability refers to the AIPM\u2019s ability to generalize its performance to a new setting.",
              "Poor generalizability may be caused by overfitting (see phase 2) or development data that were unrepresentative for the new setting (see phase 1).",
              "The literature recommends to assess generalizability on external data from a different time period, place, or healthcare setting3,7,8,10,16,17,27,56,68,72,79.",
              "To ensure the generalizability of the AIPM to the intended healthcare setting, developers are advised to extensively validate the model for representative data from that setting6,7,8,10,13,14,15,24,27,56,64,66,68,72,77,79,87,88 (see phase 1, Representativeness).",
              "The intended healthcare setting may be different from the population or setting on which the AIPM was originally developed (e.",
              "g., an AIPM developed at a tertiary care center applied to a smaller hospital).",
              "It is advised that the size of this validation data should follow the available sample size recommendations for AIPM validation (see phase 1)53,54,55.",
              "Developers are urged to clearly describe any differences between the development and validation data where possible14 and report other sources potentially affecting generalizability6,8,27.",
              "Still, AIPM updating, site-specific training or recalibration might be needed to adapt an existing AIPM to a different healthcare setting3,5,9,60,68,72.",
              "Statistical updating methods are available for regression-based models89,90.",
              "For AIPMs outside of this context no specific guidance was found.",
              "Performance analysis by population subgroups or specific problematic use cases is recommended to identify algorithmic bias6,7,23,24,26,42,61,72,79,91.",
              "Note that such an analysis may be limited by small sample sizes of certain subgroups.",
              "The literature advises to discuss and explicitly report any identified sources of algorithmic bias, so that end users know for whom the AIPM\u2019s predictive performance is subpar8,16.",
              "Many systems will display some unfairness in their outcomes, and therefore a baseline comparison with the algorithmic bias of the current systems may be considered16.",
              "The ability for AIPMs to interoperate with various existing digital infrastructure of hospitals and clinical care centers is essential for their successful integration into healthcare practice.",
              "Following existing standards from the industry was recommended as this supports the interoperability of AIPMs9,18,20,26 (e.",
              "g., ISO/IEC JTC 1/SC 4292 or the IEEE 7000-202193).",
              "This applies to data coding standards as mentioned in phase 1 of this article, but also to data exchange standards (e.",
              "g., FHIR94 and the HL7 framework95).",
              "Such standards provide (among other aspects) guidance on what data formats to use, how they should be exchanged between system components, and reduce the risk that data are accidentally misinterpreted due to slight differences in meaning of variables (semantic interoperability).",
              "For wearable devices, following the ISO/IEEE 11073-10418:201496 standard is advised20.",
              "Moreover, multiple articles recommend the use of open source or publicly available libraries in the software implementation of the AIPM20,26 to increase the accessibility of the AIPM as a whole.",
              "The NHS guide to good practice for digital and data-driven health technologies goes as far as to recommend that all new digital health services, including AIPMs, should be made internet-facing from day one (and follow the Representational State Transfer design principles) to promote accessibility and reduce complexity and costs of incorporating them in the digital infrastructure of organizations20.",
              "A proper design of how end users can interact with the AIPM is crucial for its adoption, and effective and safe use in daily healthcare practice.",
              "What constitutes a good design depends on the domain, healthcare setting and intended end users.",
              "End users interacting with the AIPM can be healthcare professionals, auditors, or patients (e.",
              "g., physicians may need to communicate about the AIPM with patients18).",
              "Many of the recommendations for human-AI interaction design come from the general human-computer interaction literature and current standards for general medical software design.",
              "Recommended standards are ISO 9241-210:201997 for interactive systems and the IEC 62366-1:201598 on application of usability engineering to medical devices20.",
              "At the software development stage, it has been recommended to include experts in user interface design8,18.",
              "Designing a good user interface and interaction requires careful consideration of the cognitive load of the end users10,18,68,99,100, by showing only relevant information in the right context, and by allowing adjustment of its behavior by end users99.",
              "A widely suggested minimum criteria for AIPM user interaction design is that it becomes clear to end users what the AIPM\u2019s intended use is26,79,87,99.",
              "Providing a model facts label should be provided to the end users is advised, including the system\u2019s technical specifications, statistical working, limitations, fairness criteria and validation, implementation disclaimer, and links to process logs22,101.",
              "To arrive at a good design, repeated extensive user experience testing is recommended11,18.",
              "The AIPM should be evaluated according to how it interfaces with the end user, and how well the AIPM and the user perform together in a typical environment10,100,102,103.",
              "It was proposed that such evaluations can, for example, be done via reader and user studies10,102,103.",
              "Tools such as a system usability scale (SUS) have been suggested as a quick and useful way of capturing user feedback20.",
              "Careful attention should be paid to inclusiveness and broad usability of the design20,22,26,62, for example by considering the digital literacy of the end users20,22,26.",
              "Multiple sources state that the design should match social norms, and make sure its presentation does not reinforce stereotypes (e.",
              "g., regarding a prespecified fairness position or bias impact statement, see phase 2)22,24,26,32,99.",
              "Moreover, the AIPM should have built-in mechanisms that protect the end user and patient from potential risks to its safe application (e.",
              "g., overconfidence in the AIPMs predictions or automation bias).",
              "These mechanisms should detect situations beyond the capabilities of the AIPM10,99, and share the confidence in the predictions with the user10,22,26,99.",
              "Additional information may be required explaining how the confidence level relates to the input data23,43,61.",
              "It was recommended to carefully consider whether predictions should be presented in a directive fashion (by also proposing decisions), or in an assistive way (e.",
              "g., by only showing estimated probabilities)9,22,40,68,86,87.",
              "The literature advised that the design should facilitate AIPM interpretability (see also Box 2.",
              "and the section on model selection and interpretability in phase 2) and allow end users to visually see the link between the input data and the predicted output8,10,22,26,32,61,99 in a comprehensive way22,23,24,26,40,42,62, and encourage giving feedback, correction and refinement about the AIPM\u2019s predictions99.",
              "Also, the design should enable the patient to request a review of an AIPM-supported decision63, and may need to provide the possibility to delete data (depending on local legislation, see phase 1 on Patient privacy)12,23,36,41.",
              "From a user interaction design perspective, it has been recommended that decisions are deterministic (consistently giving the same output for a certain input)10, and that updates of or adaptations to the AIPM should happen cautiously99.",
              "End users should be notified clearly about any changes in the AIPM26,99, and AIPM software should have the ability to roll back to previous versions, in case an update results in significant problems20,66.",
              "Finally, as monitoring and auditing of AIPMs in practice are widely recommended (covered in more detail in phase 6), the developed software should facilitate this10,22,26,32,58,62,104.",
              "This means adequate logging and traceability of predictions and decisions is required and the AIPM interface should provide sharing of performance data with end users to enable ongoing monitoring of both individual and aggregated cases, quickly highlighting any significant deviations in performance10,26,61,66.",
              "Such monitoring options should preferably be customizable by the user79,99.",
              "The principles of security and privacy by design mandate built-in data and software protection throughout the AIPM lifecycle12,35,41,42,43, which is a central requirement in the GDPR105.",
              "Cybersecurity standards provide guidance on how to approach this20,23,26, for example ANSI/NEMA NH 1-2019106, NEN 7510107, MDCG 2019-6108, ANSI/CAN/UL 2900-1109, Medical Device Cybersecurity Working Group on medical device cybersecurity110, Food and Drug Administration on cybersecurity111, ISO/IEC TS 27110:2021112, ISO/IEC 27032:2012113, ISO/IEC 27014:2013114, and ISO/IEC 27002:2013115.",
              "This might for example entail an initial risk assessment of vulnerabilities in data and software, including the risk of re-identification33, the risk of data loss and manipulation33,35, and the risk of adversarial attacks9,22,23,26,35,43,59.",
              "Techniques that make the AIPM more robust to these vulnerabilities can be implemented, like converting data to less identifiable formats23, adding random noise to the data23,34,41, federated learning23,34,41, saving personal data across different databases34,35, and adversarial ML techniques such as model hardening and run-time detection22,42,43,59.",
              "Code review by an external party and staying up to date on security alerts for code derived from third parties are also recommended23,35.",
              "All security measures should be tested before full deployment79 (also see Software testing).",
              "The level of the required security measures will depend on the impact a potential security breach might have on the individuals involved, the type of AI deployed, and the risk management capabilities of the organization23,27,35,41.",
              "The timeframe within which security updates will become available should be reported26.",
              "An incident response plan anticipating a potential security breach is recommended before deployment (also part of western legislation104,105,116), describing how incidents will be addressed and who takes responsibility with relevant contact information23,35,61.",
              "When new software vulnerabilities come to light, they should be documented and reported32,61, and so should any changes made to the AIPM in response to an attack after thorough testing10,23,35,61.",
              "AIPM software developers are recommended to follow relevant existing international standards with regard to software testing, such as the IEC 62304:2006117, the IEC 82304-1:2016118, IEC 62366-1:201598, ISO 14971:2019119, Food and Drug Administration principles of software validation120, and Food and Drug Administration guidance for off-the-shelf software use in medical devices121.",
              "Deliberate stress tests like load testing, penetration testing, integration testing and unit testing are important for the verification of the AIPM from a software perspective10,26,35,46,66,79.",
              "Each different context of use may require separate software testing to ensure reproducibility of results across different situations, computational frameworks, and input data58,62,88.",
              "These testing requirements depend on the level of reliability needed and the risks posed by the AIPM in healthcare practice26.",
              "These types of tests are also recommended to assess the effectiveness of the security measures taken and to detect new security vulnerabilities (see Security).",
              "They should be repeated regularly to monitor the data and software security during the AIPM lifecycle23,26,35.",
              "An impact assessment is performed to determine the clinical benefit of the AIPM for healthcare practice.",
              "It is important to note that a good performance of the AIPM in terms of discrimination and calibration (phases 2 and 3) does not necessarily translate to clinical utility5,27,72.",
              "A feasibility study or implementation pilot is recommended preceding an impact study to ensure correct and safe use in healthcare practice10,18,72.",
              "This type of study consists of repeated live clinical tests in which variation is key to understanding the functionality of the technology and workflow11,18.",
              "By adhering to the \u2018plan, do, study, adjust\u2019 process, adjustments can be made frequently and rapidly to optimize the workflow11,18.",
              "The literature advises to clearly define the intended use and intended users in the preparation of both the feasibility and impact study12,19,64,65.",
              "It is also recommended to report any differences in healthcare setting between the current and previous (validation) studies68 and to state the inclusion and exclusion criteria at the level of the participants and input data28,64,65.",
              "A description of the integration into the trial setting is highly recommended, including onsite and offsite requirements, version number and other technical specifications28,64,65, but also the human-AI interaction involved (e.",
              "g., assistive versus directive, see phase 4)46,64 and the patient treatment strategy associated with the AIPM outcomes64,65.",
              "It is emphasized that potential interventions included in the patient treatment strategy following from the AIPM decision support should have a solid scientific basis68.",
              "Stakeholders have preferably given informed approval of the development and clinical application of the AIPM88.",
              "Risk management is highlighted as an important part of the impact assessment, alongside the preparations for a comparative study28,43.",
              "The literature recommends the identification of potential sources of risk, extreme situations, and failures before the onset of the study26,57,58.",
              "Determining corresponding safety critical levels and quality checks is advised26.",
              "Special attention may be paid to accidental misuse and manipulation of the AIPM.",
              "Implementers are urged to report errors, failures or near misses occurring during impact assessment and afterwards24,26,43,61,64,65.",
              "A risk management plan can help to execute the monitoring, reporting and mitigation of risks encountered in healthcare practice12,16,20,26,28.",
              "This plan can for example describe the roles and responsibilities of the participants28, the process for assessing and logging potential risks12,20,24,26,43,61, a pathway to report potential risks12,24,26,43,62, and the process to address these issues in practice12,43,62.",
              "Some sources suggest that the assessment should be proportionate to the risk posed by the AIPM26,43.",
              "In terms of the impact study design, a prospective comparative study is recommended5,8,19,27,56,68,72,86,88.",
              "In a comparative study, the effects on clinical outcomes and decision making are compared for a group exposed to the predictions of the AI versus a non-exposed control group receiving standard care5,28,68,86,88.",
              "The literature identifies a randomized controlled trial (RCT) as the ideal comparative study design, randomizing patients individually or per cluster5,9,49,68,86.",
              "However, this may require more patients and might not always be feasible.",
              "Alternative designs are stepped-wedge trials9,19,86, before-after studies86, and observational studies5,19,56,68,86.",
              "For some applications (like imaging technology), a multiple reader multiple case study design is also possible46, in which the effect of the AIPM on decision making is measured by assessing the differences in discrimination (see phases 2 and 3) with and without the tool.",
              "Decision Analytical Modeling may give an initial estimate of clinical utility before commencing a full-blown impact study (see phases 2 and 3)68,86.",
              "Trial outcomes can differ across domains and applications.",
              "The most mentioned trial outcomes consist of clinical outcomes or patient-reported outcomes5,16,20,68,72,86,88 followed by cost effectiveness of care5,16,20,86,88 and changes in decision making and workflow5,20,68,86.",
              "Additional trial outcomes are patient experience20,56,88, user satisfaction and engagement88, and changes in patient (healthy) behavior88.",
              "It is advised that trial outcomes are also evaluated per clinically relevant user group12 or per affected non-user group (also in terms of algorithmic bias)12,24,91.",
              "It is recommended that findings are communicated in an understandable and meaningful way to healthcare professionals, but also to administrators and policymakers57.",
              "AIPM-specific guidelines have been developed as extensions to the CONSORT and SPIRIT guidelines for reporting on clinical trials and their protocols respectively64,65.",
              "Peer-reviewed open access publication may increase trust and facilitate adoption of the AIPM in a wider clinical community9.",
              "Clinical implementation consists of all the steps that are necessary to deploy the AIPM in the healthcare environment outside of the clinical trial setting (see phase 5).",
              "The literature strongly recommends to state the necessary conditions for deployment before proceeding with the implementation11,19,20,26,87.",
              "For example, the AIPM system might require dedicated and locally available hardware8.",
              "Although not always feasible, the integration of an AIPM directly into the existing medical workflow is preferred8,19,59,68.",
              "This could for example involve direct integration into the EHR.",
              "Moreover, the user is urged to explicitly disclose what part of decision making might be affected by AIPM predictions24,26,42,62,63,87.",
              "To further facilitate the implementation and consecutive monitoring, the literature recommends automatic AIPM deployment (moving software from testing to production environments with automated processes) and the facilitation of shadow deployment66,91, which enables prospective local validation (see phase 3) of new versions and updates19.",
              "Enabling the automatic roll-back for production models is also advised to address real-time operating risks (see phase 4)66.",
              "Moreover, a procedure to safely abort an operation is highly recommended when the system should stop being used due to a security breach or safety risk23,26,62,79.",
              "Comparable to the feasibility study of phase 5, pilot studies are recommended to examine the potential pitfalls during implementation, considering both software and hardware issues10,18,72.",
              "Lastly, Institutions and implementers are encouraged to disclose their innovation pathway, including the routes to commercialization16.",
              "The risks, investments, roles, and responsibilities of the different parties may inform the allocation of benefits in a commercial arrangement16,20.",
              "Albeit sparse,88 provide good guidance on performing economic impact analysis.",
              "Although maintenance is essential to AIPMs (and their software) that are highly dependable on changes in the external world, little guidance can be found on this topic.",
              "Developers are recommended to regularly update their AIPMs over time to improve the AIPM\u2019s predictive performance as new improvements become available and to mitigate dataset shift10,19,23.",
              "It is advised to pay special attention to the safe and automatic updating of mature systems involving many configurations for many similar models71.",
              "Note that updating the AIPM may involve recertification.",
              "The USA Food and Drug Administration is currently working on a framework that allows for repeated updating of an AIPM without repeated recertification through a change control plan122.",
              "Education involves the training of end users in the correct use of the AIPM.",
              "The literature recommends the general education of end users, often healthcare professionals, on the probabilistic nature22,23,24,42 and the limitations of AIPMs22,42.",
              "This may involve the development of a general AI curriculum for medical students and healthcare professionals.",
              "Application specific training is also advised.",
              "The end user may for example be educated on the underlying assumptions of the AIPM58,68, its legal framework26, benefits20,26,58, risks and (technical) limitations15,22,26,58,62.",
              "Providing the end user with examples of incorrectly classified cases could help in creating an understanding of the strengths and limitations of the AIPM14.",
              "Moreover, it is recommended to regularly repeat the training on the correct use of the AIPM12,15,26,58,62 and the appropriate response to security breaches23,35.",
              "For example, end users may be made aware of the possibility of automation bias and trained to maintain vigilance22,26,57,87,91.",
              "When the end user (healthcare professional) and AIPM subject (patient) are different people, as is often the case for AIPMs in healthcare, the literature recommends to train the healthcare professional to explain one\u2019s AIPM-supported decisions to their patient22.",
              "Monitoring refers to the post-deployment evaluation of the behavior of an AIPM throughout its lifecycle10,23,26,27,57,62,64,66,72,80,91.",
              "It is performed by the developer and implementers at the implementation site.",
              "Auditing refers to periodic quality control checks of the AIPM (and all of its monitoring aspects) performed by an independent third party26,58,62,91.",
              "Among other things, It will aid the detection of failures and near misses and through this strengthen the risk management and security of an AIPM35,58.",
              "Several aspects of AIPM functioning can be monitored as identified in the literature.",
              "These may for example consist of predictive performance and other model outputs9,10,26,57,63,79,80, distribution of predicted versus observed labels71, reliability and reproducibility10,26,62, types and severity of errors57, changes in risk80, quality of the input data26,57,63,71,88, quality of the label91, case-mix factors72,91, accessibility and integration of the model57, use of the AIPM recommendations57,63,88, user satisfaction and user feedback9,10,57,79,88, and (clinical) outcomes26,57,80,88.",
              "Several monitoring aspects are highlighted in the literature that deserve additional scrutiny.",
              "The monitoring of the fairness of an AIPM throughout its lifecycle is often mentioned9,12,20,23,24,26,63, for example by recording false positive and false negative prediction rates sliced across different subgroups25,26,79,91.",
              "Second, the monitoring of dataset shift is also repeatedly mentioned in the literature5,10,22,72,79,91.",
              "Dataset shift is a change in the composition of the input data caused by changes in clinical or operational practices over time that can lead to the deterioration of AIPM performance.",
              "It can for example be measured by an increase in classification errors over time23.",
              "It can be mitigated by retraining or updating the AIPM72.",
              "One last aspect is the monitoring of feedback loops26.",
              "They originate when an AIPM is modeled on care delivery features that in turn might be affected by the outcomes of an AIPM.",
              "It is advised to develop integrated mechanisms to facilitate real-time monitoring available at the start of implementation16,71.",
              "Implementers are encouraged to clearly define the context and boundaries within which the monitoring is to be performed57.",
              "Specifying the type of oversight is also recommended, e.",
              "g., human-in-the-loop, human-on-the-loop, or human-in-command26.",
              "Some sources suggest the frequency of the monitoring should be proportional to the AIPM\u2019s risks22,23,91: the higher the risk to the welfare of the patient, the higher the monitoring frequency should be.",
              "One source suggests frequent monitoring may be less important for AIPMs solely based on causal mechanisms as they are less likely to change over time27.",
              "In terms of auditing, the literature recommends the installation of a comprehensive auditability framework10,22,58 and an audit trail25,46,62, in which the AIPM\u2019s predictions, model version, input data, and use practices are methodologically logged and made available to interested third parties22,26,32,35,58,61,62,66,91.",
              "Implementers are advised to define mitigation pathways as part of the monitoring and auditing plan to deal with incidents22,35,71,79.",
              "This may for example involve the regular reporting on failures and near misses and the organization of meetings to discuss incidents58.",
              "Moreover, the literature states that mitigation could and sometimes should lead to a change in the AIPM\u2019s design or use practices, for example an adjustment in the instructions for use, a re-evaluating of the stakeholder impact assessment, or a model update22,72,80.",
              "We identified several important aspects of the AIPM development, evaluation and implementation cycle for which clear guidance was missing in the literature.",
              "First, guidance is lacking on the requirements to be fulfilled during the assessment of the medical problem and context.",
              "In other words, what aspects of a medical or healthcare problem and setting make the introduction of an AIPM likely to result in better patient care, and when are conditions sufficient to initiate AIPM development?",
              "Guidance is also missing on the a priori estimation of a minimum sample size for AIPM development for semi-supervised approaches, and for certain commonly used groups of ML modeling techniques such as decision trees (e.",
              "g., random forests) and deep learning (e.",
              "g., convolutional neural networks).",
              "Across all phases, several methodologies and quality criteria were identified to address ethical issues such as algorithmic bias, privacy preservation, and interpretable AI.",
              "However, the relevance of these issues for different healthcare domains might differ and so will the preferred definitions, metrics, and techniques to describe and mitigate them.",
              "As domain specific guidelines were not the primary focus of this investigation, we cannot with certainty comment on the general absence of such guidelines.",
              "Nevertheless, we would advise individual healthcare domains to scrutinize the currently available guidance and, when necessary, address these ethical issues across the AIPM development, evaluation and implementation cycle for their respective settings.",
              "Another aspect for which guidance was limited, is the combination of different data sources (e.",
              "g., from different registries and collection sites), and data modalities (e.",
              "g. imaging data, electrophysiological data, and lab results) for AIPM development.",
              "Although methodological studies exist for various combinations, further research on best practices is needed.",
              "Also, current guidance is primarily focused on binary outcomes (e.",
              "g., mortality), and guidance is missing on other outcome types (e.",
              "g., multinomial, ordinal, hierarchical or sequential outcomes).",
              "Although many standards exist for software security, it is unclear whether they suffice to address cyberattacks particularly geared at AIPMs.",
              "Experience with AIPM security in practice and experimentation with the insulation of AIPMs against different types of cyberattack in preclinical settings will help to clarify this.",
              "Also, more guidance on the unique aspects of AIPM-specific human-AI interaction design is needed.",
              "This will for example entail the presentation of and interaction with probabilistic outcomes and the impact of model interpretability on end users.",
              "Much more guidance is needed addressing how to integrate the AIPM into the current healthcare or clinical workflow.",
              "More guidance is also required specifying what design and execution of the feasibility and impact studies are needed, and how to report such studies.",
              "Moreover, guidance is needed regarding the assessment of the cost effectiveness of AIPMs.",
              "AIPMs differ from other health technologies and are likely to affect healthcare differently, which should be reflected in their cost effectiveness assessments (as was done for the guidance on impact studies).",
              "We described recommendations regarding the responsibilities of different parties (developers, end users, organizations) involved with AIPM development and deployment as described in the identified literature (e.",
              "g., risk assessment, incident reporting, patient privacy).",
              "However, more work is needed addressing the proper distribution of accountability across all involved parties, which may in turn inform institutional governance.",
              "Lastly, guidance is needed on (long-term) maintenance aspects, on dataset shift (and how to mitigate it), and on the frequency and necessity of local validation, recalibration (updating), and retraining.",
              "As more and more AIPMs will be implemented into healthcare practice in the coming years, this practical experience can be used to inform these aspects.",
              "This scoping review provides an easy-to-use overview and summary of the currently available actionable guidelines and quality criteria driven by the six phases of the AIPM development, evaluation, and implementation cycle: (1) data preparation, (2) AIPM development, (3) AIPM validation, (4) software development, (5) AIPM impact assessment, and (6) AIPM implementation into daily healthcare practice.",
              "Guidance was structured in specific topics and mapped to the different phases and we provided an overview of the current gaps in this guidance.",
              "To appreciate our scoping review and suggested framework of six phases several issues need to be addressed.",
              "First, our definitions of \u2018actionable\u2019 guidance as an inclusion criterion and the defined six phases are somewhat arbitrary and mainly informed by vast experience with and guidance on developing, evaluating, and implementing prediction models in healthcare.",
              "Individual AIPM applications may deviate from the structure presented here.",
              "Nevertheless, we believe the phases and their associated topics will translate to most AIPM projects and are in agreement with other phases formulated in the literature4,5,8,22.",
              "Also, the structure provided by the six phases, and our focus on actionability form two strengths of this scoping review and produce a comprehensible and easy-to-use overview of practical recommendations for those involved in the AIPM development, evaluation and implementation cycle.",
              "This sets our review apart from other work that was previously undertaken (e.",
              "g.,32,123,124).",
              "Second, the literature databases and sources we used mostly contain scientific literature and only English documents were included in the final search (translations were also considered).",
              "This may have biased our results towards academic sources and English-speaking countries of origin.",
              "To combat this, we identified additional gray literature through consultation with AI experts and a thorough screening of citations in the included literature.",
              "As a result, a substantial number of our included sources can be considered gray literature.",
              "Moreover, due to our extensive search, the current summary of available guidelines and quality criteria is comprehensive.",
              "Lastly, the expert group consulted was a convenience sample, resulting in experts predominantly working in the Netherlands.",
              "Diversity was obtained by inviting experts with different occupations (e.",
              "g., healthcare professionals, data scientists, statisticians, engineers), from different healthcare domains (e.",
              "g., radiology, internal medicine, intensive care, primary care, family medicine), and from both academia and industry.",
              "In conclusion, a substantial number of studies provide guidelines and quality criteria pertaining to the AIPM development, evaluation, and implementation cycle, which can be grouped into six well-defined phases.",
              "While the opportunities of AIPMs in healthcare are undeniable, the growing interest in these techniques requires careful quality and applicability assessment to guarantee their safety and (cost-)effectiveness before they are used and disseminated in healthcare.",
              "This review can serve as the basis for a structured quality assessment framework.",
              "Several gaps in the literature were identified where more research is needed.",
              "Additional domain and technology specific studies may be necessary and more practical experience with implementing AIPMs is needed to inform further guidance.",
              "The authors declare that all data supporting the findings of this study are available within the paper and its supplementary information files.",
              "van Smeden, M.",
              ", Reitsma, J.",
              "B.",
              ", Riley, R.",
              "D.",
              ", Collins, G.",
              "S.",
              "& Moons, K.",
              "G.",
              "M.",
              "Clinical prediction models: diagnosis versus prognosis.",
              "J.",
              "Clin.",
              "Epidemiol.",
              "132, 142\u2013145 (2021).",
              "Article PubMed Google Scholar Moons, K.",
              "G.",
              "et al.",
              "Risk prediction models: I.",
              "Development, internal validation, and assessing the incremental value of a new (bio)marker.",
              "Heart 98, 683\u2013690 (2012).",
              "Article PubMed Google Scholar Collins, G.",
              "S.",
              ", Reitsma, J.",
              "B.",
              ", Altman, D.",
              "G.",
              "& Moons, K.",
              "G.",
              "M.",
              "Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis (TRIPOD): The TRIPOD Statement.",
              "Eur.",
              "Urol.",
              "67, 1142\u20131151 (2015).",
              "Article PubMed Google Scholar Steyerberg, E.",
              "W.",
              "et al.",
              "Prognosis Research Strategy (PROGRESS) 3: prognostic model research.",
              "PLoS Med.",
              "10, e1001381 (2013).",
              "Article PubMed PubMed Central Google Scholar Moons, K.",
              "G.",
              ", Altman, D.",
              "G.",
              ", Vergouwe, Y.",
              "& Royston, P.",
              "Prognosis and prognostic research: application and impact of prognostic models in clinical practice.",
              "BMJ 338, b606 (2009).",
              "Article PubMed Google Scholar Luo, W.",
              "et al.",
              "Guidelines for developing and reporting machine learning predictive models in biomedical research: a multidisciplinary view.",
              "J.",
              "Med.",
              "Internet Res.",
              "18, e323 (2016).",
              "Article PubMed PubMed Central Google Scholar Norgeot, B.",
              "et al.",
              "Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist.",
              "Nat.",
              "Med.",
              "26, 1320\u20131324 (2020).",
              "Article CAS PubMed PubMed Central Google Scholar Weikert, T.",
              "et al.",
              "Machine learning in cardiovascular radiology: ESCR position statement on design requirements, quality assessment, current applications, opportunities, and challenges.",
              "Eur.",
              "Radiol.",
              "31, 3909\u20133922 (2021).",
              "Article PubMed Google Scholar National Academy of Medicine.",
              "Artificial Intelligence In Health Care: The Hope, The Hype, The Promise, The Peril (National Academy of Medicine, 2019).",
              "Larson, D.",
              "B.",
              "et al.",
              "Regulatory frameworks for development and evaluation of artificial intelligence-based diagnostic imaging algorithms: summary and recommendations.",
              "J.",
              "Am.",
              "Coll.",
              "Radiol.",
              "18, 413\u2013424 (2021).",
              "Article PubMed Google Scholar Smith, M.",
              ", Sattler, A.",
              ", Hong, G.",
              "& Lin, S.",
              "From code to bedside: implementing artificial intelligence using quality improvement methods.",
              "J.",
              "Gen.",
              "Intern.",
              "Med.",
              "36, 1061\u20131066 (2021).",
              "Article PubMed Google Scholar Machine Intelligence Garage\u2019s Ethics Committee.",
              "Ethics framework.",
              "Retrieved from https://www.",
              "migarage.",
              "ai/wp-content/uploads/2020/11/MIG_Ethics-Report_2020_v5.",
              "pdf (2018).",
              "Stevens, L.",
              "M.",
              ", Mortazavi, B.",
              "J.",
              ", Deo, R.",
              "C.",
              ", Curtis, L.",
              "& Kao, D.",
              "P.",
              "Recommendations for reporting machine learning analyses in clinical research.",
              "Circ.",
              "Cardiovasc Qual.",
              "Outcomes 13, e006556 (2020).",
              "Article PubMed PubMed Central Google Scholar Mongan, J.",
              ", Moy, L.",
              "& Kahn, C.",
              "E.",
              "Checklist for artificial intelligence in medical imaging (CLAIM): a guide for authors and reviewers.",
              "Radiology: Artif.",
              "Intell.",
              "2, e200029 (2020).",
              "Google Scholar Food and Drug Administration.",
              "Computer-assisted detection devices applied to radiology images and radiology device data\u2014premarket notification [510(k)] submissions.",
              "Retrieved from https://www.",
              "fda.",
              "gov/regulatory-information/search-fda-guidance-documents/computer-assisted-detection-devices-applied-radiology-images-and-radiology-device-data-premarket (2012).",
              "Vollmer, S.",
              "et al.",
              "Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness.",
              "Br.",
              "Med.",
              "J.",
              "368, 16927 (2020).",
              "Google Scholar Steyerberg, E.",
              "W.",
              "& Vergouwe, Y.",
              "Towards better clinical prediction models: seven steps for development and an ABCD for validation.",
              "Eur.",
              "Heart J.",
              "35, 1925\u20131931 (2014).",
              "Article PubMed PubMed Central Google Scholar Ray, J.",
              "M.",
              "et al.",
              "Six habits of highly successful health information technology: powerful strategies for design and implementation.",
              "J.",
              "Am.",
              "Med.",
              "Inform.",
              "Assoc.",
              "26, 1109\u20131114 (2019).",
              "Article PubMed PubMed Central Google Scholar Wiens, J.",
              "et al.",
              "Do no harm: a roadmap for responsible machine learning for health care.",
              "Nat.",
              "Med.",
              "25, 1337\u20131340 (2019).",
              "Article CAS PubMed Google Scholar UK Department of Health & Social Care.",
              "A guide to good practice for digital and data-driven health technologies.",
              "Retrieved from https://www.",
              "gov.",
              "uk/government/publications/code-of-conduct-for-data-driven-health-and-care-technology/initial-code-of-conduct-for-data-driven-health-and-care-technology (2021).",
              "Bossuyt, P.",
              "M.",
              "et al.",
              "Towards complete and accurate reporting of studies of diagnostic accuracy: the STARD initiative.",
              "Clin.",
              "Chem.",
              "49, 1\u20136 (2003).",
              "Article CAS PubMed Google Scholar Leslie, D.",
              "Understanding Artificial Intelligence Ethics and Safety: A Guide for the Responsible Design and Implementation of AI Systems in the Public Sector (The Alan Turing Institute, 2019).",
              "Information Commissioner\u2019s Office.",
              "Guidance on the AI auditing framework: draft guidance for consultation.",
              "Retrieved from https://ico.",
              "org.",
              "uk/media/about-the-ico/consultations/2617219/guidance-on-the-ai-auditing-framework-draft-for-consultation.",
              "pdf (2020).",
              "Global Future Council on Human Rights 2016\u20132018.",
              "How to prevent discriminatory outcomes in machine learning.",
              "Retrieved from http://www3.",
              "weforum.",
              "org/docs/WEF_40065_White_Paper_How_to_Prevent_Discriminatory_Outcomes_in_Machine_Learning.",
              "pdf (2018).",
              "Turner Lee, N.",
              ", Resnick, P.",
              "& Barton, G.",
              "Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms. Retrieved from https://www.",
              "brookings.",
              "edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/ (2019).",
              "High-Level Expert Group on Artificial Intelligence.",
              "The assessment list for trustworthy artificial intelligence (ALTAI) for self-assessment.",
              "Retrieved from https://digital-strategy.",
              "ec.",
              "europa.",
              "eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment (2020).",
              "Cearns, M.",
              ", Hahn, T.",
              "& Baune, B.",
              "T.",
              "Recommendations and future directions for supervised machine learning in psychiatry.",
              "Transl.",
              "Psychiatry 9, 271 (2019).",
              "Article PubMed PubMed Central Google Scholar Nyka\u0308nen, P.",
              "et al.",
              "Guideline for good evaluation practice in health informatics (GEP-HI).",
              "Int.",
              "J.",
              "Med.",
              "Inform.",
              "80, 815\u2013827 (2011).",
              "Article PubMed Google Scholar Complete guide to GDPR compliance.",
              "Retrieved from https://gdpr.",
              "eu/ (2020).",
              "Personal Information Protection and Electronic Documents Act.",
              "Retrieved from https://laws-lois.",
              "justice.",
              "gc.",
              "ca/PDF/P-8.",
              "6.",
              "pdf (2000).",
              "Health Insurance Portability and Accountability Act of 1996.",
              "Retrieved from https://www.",
              "govinfo.",
              "gov/content/pkg/PLAW-104publ191/pdf/PLAW-104publ191.",
              "pdf (1996).",
              "Ryan, M.",
              "& Stahl, B.",
              "C.",
              "Artificial intelligence ethics guidelines for developers and users: clarifying their content and normative implications.",
              "J.",
              "Inf.",
              "Commun.",
              "Ethics Soc.",
              "19, 61\u201386 (2021).",
              "Article Google Scholar Liaw, S.",
              "T.",
              "et al.",
              "Ethical Use of Electronic Health Record Data and Artificial Intelligence: Recommendations of the Primary Care Informatics Working Group of the International Medical Informatics Association.",
              "Yearb.",
              "Med Inf.",
              "29, 51\u201357 (2020).",
              "Article Google Scholar Rodr\u00a1guez, N.",
              "et al.",
              "Federated learning and differential privacy: software tools analysis, the Sherpa.",
              "ai FL framework and methodological guidelines for preserving data privacy.",
              "Information Fusion 64, 270\u2013292 (2020).",
              "Article Google Scholar Datatilsynet.",
              "Software development with data protection by design and by default.",
              "Retrieved from https://www.",
              "datatilsynet.",
              "no/en/about-privacy/virksomhetenes-plikter/innebygd-personvern/data-protection-by-design-and-by-default/?",
              "print=true (2017).",
              "Sartor, G.",
              "& Lagioia, F.",
              "The impact of the General Data Protection Regulation (GDPR) on artificial intelligence.",
              "Retrieved from https://www.",
              "europarl.",
              "europa.",
              "eu/RegData/etudes/STUD/2020/641530/EPRS_STU(2020)641530_EN.",
              "pdf (2020).",
              "Gebru, T.",
              "et al.",
              "Datasheets for datasets.",
              "Commun.",
              "ACM 64, 86\u201392 (2021).",
              "Article Google Scholar Forcier, M.",
              "B.",
              ", Gallois, H.",
              ", Mullan, S.",
              "& Joly, Y.",
              "Integrating artificial intelligence into health care through data access: can the GDPR act as a beacon for policymakers?",
              "J.",
              "Law Biosci.",
              "6, 317\u2013335 (2019).",
              "Article PubMed PubMed Central Google Scholar The Privacy Act.",
              "Retrieved from https://laws-lois.",
              "justice.",
              "gc.",
              "ca/PDF/P-21.",
              "pdf (1985).",
              "Information Commissioner\u2019s Office.",
              "ICO and the Turing consultation on explaining AI decisions guidance.",
              "Retrieved from https://ico.",
              "org.",
              "uk/about-the-ico/ico-and-stakeholder-consultations/ico-and-the-turing-consultation-on-explaining-ai-decisions-guidance/ (2020).",
              "Datatilsynet.",
              "Artificial intelligence and privacy.",
              "Retrieved from https://www.",
              "datatilsynet.",
              "no/globalassets/global/english/ai-and-privacy.",
              "pdf (2018).",
              "Benjamins, R.",
              ", Barbado, A.",
              "& Sierra, D.",
              "Responsible AI by design in practice.",
              "Retrieved from https://arxiv.",
              "org/abs/1909.",
              "12838 (2019).",
              "Arrieta, A.",
              "B.",
              "et al.",
              "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI.",
              "Inf.",
              "Fusion 58, 82\u2013115 (2020).",
              "Article Google Scholar Information Commissioner\u2019s Office.",
              "Anonymisation: managing data protection risk code of practice.",
              "Retrieved from https://ico.",
              "org.",
              "uk/media/for-organisations/documents/1061/anonymisation-code.",
              "pdf (2012).",
              "California Consumer Privacy Act of 2018.",
              "Retrieved from https://leginfo.",
              "legislature.",
              "ca.",
              "gov/faces/codes_displayText.",
              "xhtml?",
              "division=3.",
              "&part=4.",
              "&lawCode=CIV&title=1.",
              "81.",
              "5 (2018).",
              "Food and Drug Administration.",
              "Clinical performance assessment: considerations for computer-assisted detection devices applied to radiology images and radiology device data in premarket notification (510(k)) submissions.",
              "Retrieved from https://www.",
              "fda.",
              "gov/regulatory-information/search-fda-guidance-documents/clinical-performance-assessment-considerations-computer-assisted-detection-devices-applied-radiology (2020).",
              "Bhaskar, H.",
              ", Hoyle, D.",
              "C.",
              "& Singh, S.",
              "Machine learning in bioinformatics: a brief survey and recommendations for practitioners.",
              "Comput Biol.",
              "Med.",
              "36, 1104\u20131125 (2006).",
              "Article PubMed Google Scholar Riley, R.",
              "D.",
              "et al.",
              "Calculating the sample size required for developing a clinical prediction model.",
              "BMJ 368, m441 (2020).",
              "Article PubMed Google Scholar Steyerberg, E.",
              "W.",
              "Clinical Prediction Models (Springer Nature, 2009).",
              "Pineau, J.",
              "et al.",
              "The machine learning reproducibility checklist.",
              "Retrieved from https://www.",
              "cs.",
              "mcgill.",
              "ca/~jpineau/ReproducibilityChecklist.",
              "pdf (2020).",
              "Christodoulou, E.",
              "et al.",
              "Adaptive sample size determination for the development of clinical prediction models.",
              "Diagnostic Prognostic Res.",
              "5, 6 (2021).",
              "Article Google Scholar Mukherjee, S.",
              "et al.",
              "Estimating dataset size requirements for classifying DNA microarray data.",
              "J.",
              "Comput.",
              "Biol.",
              "10, 119\u2013142 (2003).",
              "Article CAS PubMed Google Scholar Vergouwe, Y.",
              ", Steyerberg, E.",
              "W.",
              ", Eijkemans, M.",
              "J.",
              "C.",
              "& Habbema, J.",
              "D.",
              "F.",
              "Substantial effective sample sizes were required for external validation studies of predictive logistic regression models.",
              "J.",
              "Clin.",
              "Epidemiol.",
              "58, 475\u2013483 (2005).",
              "Article PubMed Google Scholar Riley, R.",
              "D.",
              "et al.",
              "Minimum sample size for external validation of a clinical prediction model with a binary outcome.",
              "Stat.",
              "Med.",
              "40, 4230\u20134251 (2021).",
              "Article PubMed Google Scholar Archer, L.",
              "et al.",
              "Minimum sample size for external validation of a clinical prediction model with a continuous outcome.",
              "Stat.",
              "Med.",
              "40, 133\u2013146 (2021).",
              "Article PubMed Google Scholar Park, S.",
              "H.",
              "& Han, K.",
              "Methodologic guide for evaluating clinical performance and effect of artificial intelligence technology for medical diagnosis and prediction.",
              "Radiology 286, 800\u2013809 (2018).",
              "Article PubMed Google Scholar Magrabi, F.",
              "et al.",
              "Artificial intelligence in clinical decision support: challenges for evaluating AI and practical implications.",
              "Yearb.",
              "Med.",
              "Inf.",
              "28, 128\u2013134 (2019).",
              "Article Google Scholar Shneiderman, B.",
              "Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered AI systems. ACM Trans.",
              "Interact.",
              "Intell.",
              "Syst.",
              "10, 26 (2020).",
              "Article Google Scholar Wang, F.",
              "& Preininger, A.",
              "AI in health: state of the art, challenges, and future directions.",
              "Yearb.",
              "Med.",
              "Inf.",
              "28, 16\u201326 (2019).",
              "Article Google Scholar Van Calster, B.",
              "et al.",
              "Calibration: the Achilles heel of predictive analytics.",
              "BMC Med.",
              "17, 230 (2019).",
              "Article PubMed PubMed Central Google Scholar Diakopoulos, N.",
              ", et al.",
              "Principles for accountable algorithms and a social impact statement for algorithms. Retrieved from https://www.",
              "fatml.",
              "org/resources/principles-for-accountable-algorithms. High-Level Expert Group on Artificial Intelligence.",
              "Draft ethics guidelines for trustworthy AI.",
              "Retrieved from https://www.",
              "euractiv.",
              "com/wp-content/uploads/sites/2/2018/12/AIHLEGDraftAIEthicsGuidelinespdf.",
              "pdf (2018).",
              "Monetary Authority of Singapore.",
              "Principles to promote fairness, ethics, accountability and transparency (FEAT) in the use of artificial intelligence and data analytics in Singapore\u2019s Financial Sector.",
              "Retrieved from https://www.",
              "mas.",
              "gov.",
              "sg/~/media/MAS/News%20and%20Publications/Monographs%20and%20Information%20Papers/FEAT%20Principles%20Final.",
              "pdf (2019).",
              "Liu, X.",
              ", Rivera, S.",
              "C.",
              ", Moher, D.",
              ", Calvert, M.",
              "J.",
              "& Denniston, A.",
              "K.",
              "Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI Extension.",
              "BMJ 370, m3164 (2020).",
              "Article PubMed PubMed Central Google Scholar Rivera, S.",
              "C.",
              "et al.",
              "Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension.",
              "Nat.",
              "Med.",
              "26, 1351\u20131363 (2020).",
              "Article Google Scholar Serban, A.",
              ", Blom, K.",
              "v.",
              "d.",
              ", Hoos, H.",
              "& Visser, J.",
              "Adoption and effects of software engineering best practices in machine learning.",
              "Retrieved from https://doi.",
              "org/10.",
              "1145/3382494.",
              "3410681 (2020).",
              "Alonso, O.",
              "Challenges with label quality for supervised learning.",
              "ACM J.",
              "Data Inform.",
              "Quality 6, 2 (2015).",
              "Google Scholar Kappen, T.",
              "H.",
              "et al.",
              "Evaluating the impact of prediction models: lessons learned, challenges, and recommendations.",
              "Diagn.",
              "Progn.",
              "Res.",
              "2, 11 (2018).",
              "Article PubMed PubMed Central Google Scholar Poldrack, R.",
              "A.",
              ", Huckins, G.",
              "& Varoquaux, G.",
              "Establishment of best practices for evidence for prediction: a review.",
              "JAMA Psychiatry 77, 534\u2013540 (2020).",
              "Article PubMed PubMed Central Google Scholar Kaufman, S.",
              ", Rosset, S.",
              ", Perlich, C.",
              "& Stitelman, O.",
              "Leakage in data mining: formulation, detection, and avoidance.",
              "ACM Trans.",
              "Knowl.",
              "Discov.",
              "Data 6, 15 (2012).",
              "Article Google Scholar Sculley, D.",
              "et al.",
              "Hidden technical debt in Machine learning systems. Retrieved from https://papers.",
              "nips.",
              "cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.",
              "pdf (2015).",
              "Kelly, C.",
              "J.",
              ", Karthikesalingam, A.",
              ", Suleyman, M.",
              ", Corrado, G.",
              "& King, D.",
              "Key challenges for delivering clinical impact with artificial intelligence.",
              "BMC Med.",
              "17, 195 (2019).",
              "Article PubMed PubMed Central Google Scholar Miller, T.",
              "Explanation in artificial intelligence: Insights from the social sciences.",
              "Artif.",
              "Intell.",
              "267, 1\u201338 (2019).",
              "Article Google Scholar Huang, S.",
              "C.",
              ", Pareek, A.",
              ", Seyyedi, S.",
              ", Banerjee, I.",
              "& Lungren, M.",
              "P.",
              "Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines.",
              "NPJ Digit Med.",
              "3, 136 (2020).",
              "Article PubMed PubMed Central Google Scholar Molnar, C.",
              "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable (lulu.",
              "com, 2019).",
              "Aliferis, C.",
              "F.",
              ", Statnikov, A.",
              "& Tsamardinos, I.",
              "Challenges in the analysis of mass-throughput data: a technical commentary from the statistical machine learning perspective.",
              "Cancer Inf.",
              "2, 133\u2013162 (2007).",
              "Google Scholar Eggensperger, K.",
              ", Lindauer, M.",
              "& Hutter, F.",
              "Pitfalls and best practices in algorithm configuration.",
              "J.",
              "Artif.",
              "Int.",
              "Res.",
              "64, 861\u2013893 (2019).",
              "Google Scholar Altman, D.",
              "G.",
              ", Vergouwe, Y.",
              ", Royston, P.",
              "& Moons, K.",
              "G.",
              "M.",
              "Prognosis and prognostic research: validating a prognostic model.",
              "BMJ 338, b605 (2009).",
              "Article PubMed Google Scholar Google A.",
              "I.",
              "Responsible AI practices.",
              "Retrieved from https://ai.",
              "google/responsibilities/responsible-ai-practices/ (2021).",
              "Food and Drug Administration.",
              "Software as a medical device (SAMD): clinical evaluation.",
              "Retrieved from https://www.",
              "fda.",
              "gov/regulatory-information/search-fda-guidance-documents/software-medical-device-samd-clinical-evaluation (2017).",
              "Wynants, L.",
              "et al.",
              "Three myths about risk thresholds for prediction models.",
              "BMC Med.",
              "17, 192 (2019).",
              "Article PubMed PubMed Central Google Scholar Kaur, H.",
              ", Pannu, H.",
              "S.",
              "& Malhi, A.",
              "K.",
              "A systematic review on imbalanced data challenges in machine learning: applications and solutions.",
              "ACM Comput.",
              "Surv.",
              "52, 1\u201336 (2019).",
              "Google Scholar Van Calster, B.",
              ", van Smeden, M.",
              ", De Cock, B.",
              "& Steyerberg, E.",
              "W.",
              "Regression shrinkage methods for clinical prediction models do not guarantee improved performance: Simulation study.",
              "Stat.",
              "Methods Med.",
              "Res.",
              "29, 3166\u20133178 (2020).",
              "Article PubMed Google Scholar Riley, R.",
              "D.",
              "et al.",
              "Penalization and shrinkage methods produced unreliable clinical prediction models especially when sample size was small.",
              "J.",
              "Clin.",
              "Epidemiol.",
              "132, 88\u201396 (2021).",
              "Article PubMed PubMed Central Google Scholar Pozzolo, A.",
              "D.",
              ", Caelen, O.",
              ", Johnson, R.",
              "A.",
              "& Bontempi, G.",
              "Calibrating probability with undersampling for unbalanced classification.",
              "In IEEE Symposium Series on Computational Intelligence, SSCI 2015, Cape Town, South Africa, December 7\u201310, 2015 159\u2013166 (IEEE, 2015).",
              "Moons, K.",
              "G.",
              "et al.",
              "Risk prediction models: II.",
              "External validation, model updating, and impact assessment.",
              "Heart 98, 691\u2013698 (2012).",
              "Article PubMed Google Scholar Berscheid, J.",
              "& Roewer-Despres, F.",
              "Beyond transparency: a proposed framework for accountability in decision-making AI systems. AI Matters 5, 13\u201322 (2019).",
              "Article Google Scholar National Institute for Health and Care Excellence.",
              "Evidence standards framework for digital health technologies.",
              "Retrieved from https://www.",
              "nice.",
              "org.",
              "uk/about/what-we-do/our-programmes/evidence-standards-framework-for-digital-health-technologies (2018).",
              "Su, T.",
              "L.",
              ", Jaki, T.",
              ", Hickey, G.",
              "L.",
              ", Buchan, I.",
              "& Sperrin, M.",
              "A review of statistical updating methods for clinical prediction models.",
              "Stat.",
              "Methods Med.",
              "Res.",
              "27, 185\u2013197 (2018).",
              "Article PubMed Google Scholar Jenkins, D.",
              "A.",
              "et al.",
              "Continual updating and monitoring of clinical prediction models: time for dynamic prediction systems?",
              "Diagnostic Prognostic Res.",
              "5, 1 (2021).",
              "Article Google Scholar McCradden, M.",
              "D.",
              "et al.",
              "Patient safety and quality improvement: ethical principles for a regulatory approach to bias in healthcare machine learning.",
              "J.",
              "Am.",
              "Med.",
              "Inform.",
              "Assoc.",
              "27, 2024\u20132027 (2020).",
              "Article PubMed PubMed Central Google Scholar International Organization for Standardization.",
              "Artificial intelligence (ISO/IEC JTC 1/SC 42).",
              "Retrieved from https://www.",
              "iso.",
              "org/committee/6794475.",
              "html (2017).",
              "Institute of Electrical and Electronics Engineers.",
              "IEEE approved draft model process for addressing ethical concerns during system design (IEEE 7000-2021).",
              "Retrieved from https://standards.",
              "ieee.",
              "org/standard/7000-2021.",
              "html (2021).",
              "HL7.",
              "FHIR.",
              "Retrieved from http://hl7.",
              "org/fhir/ (2019).",
              "HL7.",
              "Retrieved from http://www.",
              "hl7.",
              "org/ (2021).",
              "International Organization for Standardization.",
              "Health informatics\u2014Personal health device communication\u2014Part 10418: Device specialization\u2014International Normalized Ratio (INR) monitor (ISO/IEEE 11073-10418:2014).",
              "Retrieved from https://www.",
              "iso.",
              "org/standard/61897.",
              "html (2014).",
              "International Organization for Standardization.",
              "Ergonomics of human-system interaction - Part 210: Human-centred design for interactive systems (ISO 9241-210:2019).",
              "Retrieved from https://www.",
              "iso.",
              "org/obp/ui/#iso:std:iso:9241:-210:ed-2:v1:en (2019).",
              "International Organization for Standardization.",
              "Medical devices - Part 1: Application of usability engineering to medical devices (IEC 62366-1:2015).",
              "Retrieved from https://www.",
              "iso.",
              "org/standard/63179.",
              "html (2015).",
              "Amershi, S.",
              "et al.",
              "Guidelines for Human-AI Interaction.",
              "Retrieved from https://doi.",
              "org/10.",
              "1145/3290605.",
              "3300233 (2019).",
              "eHealth Observatory.",
              "Canada health infoway benefits evaluation indicators.",
              "Retrieved from https://www.",
              "infoway-inforoute.",
              "ca/en/component/edocman/450-benefits-evaluation-indicators-technical-report-version-2-0/view-document?",
              "Itemid=0 (2012).",
              "Sendak, M.",
              "P.",
              ", Gao, M.",
              ", Brajer, N.",
              "& Balu, S.",
              "Presenting machine learning model information to clinical end users with model facts labels.",
              "npj Digital Med.",
              "3, 41 (2020).",
              "Article Google Scholar Medicines & Healthcare products Regulatory Agency.",
              "Guidance on applying human factors and usability engineering to medical devices including drug-device combination products in Great Britain.",
              "Retrieved from https://assets.",
              "publishing.",
              "service.",
              "gov.",
              "uk/government/uploads/system/uploads/attachment_data/file/970563/Human-Factors_Medical-Devices_v2.",
              "0.",
              "pdf (2021).",
              "Food and Drug Administration.",
              "Applying human factors and usability engineering to medical devices: Guidance for industry and food and drug administrations taff.",
              "Retrieved from https://www.",
              "fda.",
              "gov/media/80481/download (2016).",
              "Council of the European Union.",
              "Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (Medical Device Regulation).",
              "Retrieved from https://eur-lex.",
              "europa.",
              "eu/legal-content/EN/TXT/PDF/?",
              "uri=CELEX:32017R0745 (2017).",
              "Council of the European Union.",
              "Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation).",
              "Retrieved from https://eur-lex.",
              "europa.",
              "eu/eli/reg/2016/679/oj (2016).",
              "National Electrical Manufacturers Association.",
              "Manufacturer disclosure statement for medical device security (ANSI/NEMA NH 1-2019).",
              "Retrieved from https://www.",
              "nema.",
              "org/standards/view/manufacturer-disclosure-statement-for-medical-device-security (2019).",
              "Royal Netherlands Standardization Institute.",
              "Health informatics\u2014information security management in healthcare\u2014Part 1: management system (NEN 7510).",
              "Retrieved from https://www.",
              "nen.",
              "nl/en/nen-7510-1-2017-a1-2020-nl-267179 (2020).",
              "European Commission.",
              "Guidance on Cybersecurity for medical devices.",
              "Retrieved from https://ec.",
              "europa.",
              "eu/docsroom/documents/41863 (2020).",
              "UL Standards.",
              "ANSI/CAN/UL Standard for software cybersecurity for network-connectable products, part1: General requirements (ANSI/CAN/UL standard 2900-1).",
              "Retrieved from https://standardscatalog.",
              "ul.",
              "com/ProductDetail.",
              "aspx?",
              "productId=UL2900-1 (2017).",
              "International Medical Device Regulators Forum.",
              "Principles and practices for medical device cypersecurity.",
              "Retrieved from http://www.",
              "imdrf.",
              "org/docs/imdrf/final/technical/imdrf-tech-200318-pp-mdc-n60.",
              "pdf (2020).",
              "Food and Drug Administration.",
              "Response to NIST workshop and call for position papers on standards and guidelines to enhance software supply chain security.",
              "Retrieved from https://www.",
              "fda.",
              "gov/media/149954/download (2021).",
              "International Organization for Standardization.",
              "Information technology, cybersecurity and privacy protection\u2014Cybersecurity framework development guidelines (ISO/IEC TS 27110:2021).",
              "Retrieved from https://www.",
              "iso.",
              "org/standard/72435.",
              "html (2021).",
              "International Organization for Standardization.",
              "Information technology\u2014security techniques\u2014guidelines for cybersecurity (ISO/IEC 27032:2012).",
              "Retrieved from https://www.",
              "iso.",
              "org/standard/44375.",
              "html (2012).",
              "International Organization for Standardization.",
              "Information technology\u2014security techniques\u2014governance of information security (ISO/IEC 27014:2013).",
              "Retrieved from https://www.",
              "iso.",
              "org/standard/43754.",
              "html (2013).",
              "International Organization for Standardization.",
              "Information technology\u2014security techniques\u2014code of practice for information security controls (ISO/IEC 27002:2013).",
              "Retrieved from https://www.",
              "iso.",
              "org/standard/54533.",
              "html (2013).",
              "Food and Drug Administration.",
              "Postmarket surveillance under section 522 of the federal food, drug, and cosmetic act.",
              "Retrieved from https://www.",
              "fda.",
              "gov/media/81015/download (2016).",
              "International Organization for Standardization.",
              "Medical device software\u2014software life cycle processes (IEC 62304:2006).",
              "Retrieved from https://www.",
              "iso.",
              "org/obp/ui/#iso:std:iec:62304:ed-1:v1:en (2006).",
              "International Organization for Standardization.",
              "Health software - Part 1: General requirements for product safety (IEC 82304-1:2016).",
              "Retrieved from https://www.",
              "iso.",
              "org/standard/59543.",
              "html (2016).",
              "International Organization for Standardization.",
              "Medical devices\u2014application of risk management to medical devices (ISO 14971:2019).",
              "Retrieved from https://www.",
              "iso.",
              "org/standard/72704.",
              "html (2019).",
              "Food and Drug Administration.",
              "General principles of software validation.",
              "Retrieved from https://www.",
              "fda.",
              "gov/regulatory-information/search-fda-guidance-documents/general-principles-software-validation (2002).",
              "Food and Drug Administration.",
              "Off-the-shelf software use in medical devices.",
              "Retrieved from https://www.",
              "fda.",
              "gov/regulatory-information/search-fda-guidance-documents/shelf-software-use-medical-devices (2019).",
              "Food and Drug Administration.",
              "Proposed regulatory framework for modifications to artificial intelligence/machine learning (AI/ML)-based software as a medical device (SaMD).",
              "Retrieved from https://www.",
              "fda.",
              "gov/files/medical%20devices/published/US-FDA-Artificial-Intelligence-and-Machine-Learning-Discussion-Paper.",
              "pdf (2019).",
              "Hagendorff, T.",
              "The ethics of AI ethics: an evaluation of guidelines.",
              "Minds Mach.",
              "30, 99\u2013120 (2020).",
              "Article Google Scholar Jobin, A.",
              ", Ienca, M.",
              "& Vayena, E.",
              "The global landscape of AI ethics guidelines.",
              "Nat.",
              "Mach.",
              "Intell.",
              "1, 389\u2013399 (2019).",
              "Article Google Scholar Van Calster, B.",
              ", Steyerberg, E.",
              "W.",
              "& Collins, G.",
              "S.",
              "Artificial intelligence algorithms for medical prediction should be nonproprietary and readily available.",
              "JAMA Intern.",
              "Med.",
              "179, 731 (2019).",
              "Article PubMed Google Scholar Van Calster, B.",
              ", Wynants, L.",
              ", Timmerman, D.",
              ", Steyerberg, E.",
              "W.",
              "& Collins, G.",
              "S.",
              "Predictive analytics in health care: how can we know it works?",
              "J.",
              "Am.",
              "Med.",
              "Inf.",
              "Assoc.",
              "26, 1651\u20131654 (2019).",
              "Article Google Scholar Download references The authors thank Rene Aarnink, Martijn Bauer, Giovanni Cina\u0300, Andre\u0301 Dekker, Bart Geerts, Evangelos Kanoulas, Floor van Leeuwen, Wiro Niessen, Daniel Oberski, Egge van der Poel, Maarten de Rijke, Stephan Romeijn, Bart-Jan Verhoeff for providing and pointing to relevant guidance literature in the field.",
              "This research was funded by the Ministry of Health, Welfare and Sport.",
              "These authors contributed equally: Anne A.",
              "H.",
              "de Hond, Artuur M.",
              "Leeuwenberg.",
              "Department of Information Technology and Digital Innovation, Leiden University Medical Center, Leiden, The Netherlands Anne A.",
              "H.",
              "de Hond & Ilse M.",
              "J.",
              "Kant Clinical AI Implementation and Research Lab, Leiden University Medical Center, Leiden, The Netherlands Anne A.",
              "H.",
              "de Hond, Ilse M.",
              "J.",
              "Kant, Hendrikus J.",
              "A.",
              "van Os & Ewout W.",
              "Steyerberg Department of Biomedical Data Sciences, Leiden University Medical Center, Leiden, The Netherlands Anne A.",
              "H.",
              "de Hond, Ilse M.",
              "J.",
              "Kant & Ewout W.",
              "Steyerberg Julius Center for Health Sciences and Primary Care, University Medical Center Utrecht, Utrecht University, Utrecht, The Netherlands Artuur M.",
              "Leeuwenberg, Lotty Hooft, Steven W.",
              "J.",
              "Nijman, Thomas P.",
              "A.",
              "Debray, Ewoud Schuit, Maarten van Smeden, Johannes B.",
              "Reitsma & Karel G.",
              "M.",
              "Moons Cochrane Netherlands, University Medical Center Utrecht, Utrecht University, Utrecht, The Netherlands Lotty Hooft National eHealth Living Lab, Leiden, The Netherlands Hendrikus J.",
              "A.",
              "van Os, Jiska J.",
              "Aardoom & Niels H.",
              "Chavannes Department of Public Health and Primary Care, Leiden University Medical Center, Leiden, The Netherlands Jiska J.",
              "Aardoom & Niels H.",
              "Chavannes You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar You can also search for this author in PubMed Google Scholar Co-first authors A.",
              "d.",
              "H.",
              "and A.",
              "L.",
              "contributed equally to this work and so did co-last authors K.",
              "M.",
              "and N.",
              "C.",
              "A.",
              "d.",
              "H.",
              ", A.",
              "L.",
              ", S.",
              "N.",
              ", H.",
              "v.",
              "O.",
              ", N.",
              "C.",
              ", and K.",
              "M.",
              "were responsible for the conceptualization and design of the study.",
              "A.",
              "d.",
              "H.",
              ", A.",
              "L.",
              ", S.",
              "N.",
              ", and H.",
              "v.",
              "O.",
              "developed the search queries.",
              "A.",
              "d.",
              "H.",
              "and A.",
              "L.",
              "performed the search, reviewed the literature, and consulted with the experts.",
              "A.",
              "d.",
              "H.",
              ", A.",
              "L.",
              ", K.",
              "M.",
              ", N.",
              "C.",
              ", I.",
              "K.",
              ", S.",
              "N.",
              "and M.",
              "v.",
              "S.",
              "mapped the identified topics to AIPM phases.",
              "A.",
              "d.",
              "H.",
              "and A.",
              "L.",
              "drafted the original manuscript.",
              "All authors critically analyzed, reviewed, contributed to, and approved the final manuscript.",
              "All authors take shared responsibility for all aspects of the work.",
              "Correspondence to Anne A.",
              "H.",
              "de Hond or Artuur M.",
              "Leeuwenberg.",
              "The authors declare no competing interests.",
              "Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
              "Open Access This article is licensed under a Creative Commons Attribution 4.",
              "0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.",
              "The images or other third party material in this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a credit line to the material.",
              "If material is not included in the article\u2019s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.",
              "To view a copy of this license, visit http://creativecommons.",
              "org/licenses/by/4.",
              "0/.",
              "Reprints and Permissions de Hond, A.",
              "A.",
              "H.",
              ", Leeuwenberg, A.",
              "M.",
              ", Hooft, L.",
              "et al.",
              "Guidelines and quality criteria for artificial intelligence-based prediction models in healthcare: a scoping review.",
              "npj Digit.",
              "Med.",
              "5, 2 (2022).",
              "https://doi.",
              "org/10.",
              "1038/s41746-021-00549-7 Download citation Received: 26 August 2021 Accepted: 13 December 2021 Published: 10 January 2022 DOI: https://doi.",
              "org/10.",
              "1038/s41746-021-00549-7 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article.",
              "Provided by the Springer Nature SharedIt content-sharing initiative BMC Cancer (2023) Communications Medicine (2023) Pediatric Research (2023) npj Digital Medicine (2023) npj Digital Medicine (2023) Advertisement npj Digital Medicine (npj Digit.",
              "Med.)",
              "ISSN 2398-6352 (online) \u00a9 2023 Springer Nature Limited Sign up for the Nature Briefing newsletter \u2014 what matters in science, free to your inbox daily."
            ]
          }
        ],
        "Testing should be done both before and after deploying the systems in real-world settings, and any issues or feedback should be addressed promptly.": [
          {
            "title": "Root Out Bias at Every Stage of Your AI-Development Process",
            "url": "https://hbr.org/2020/10/root-out-bias-at-every-stage-of-your-ai-development-process",
            "content": [
              "Root Out Bias at Every Stage of Your AI-Development Process",
              "Bias mitigation is a fairly technical process, where certain techniques can be deployed depending on the stage in the machine learning pipeline: pre-processing, in-processing and post-processing.",
              "Each offers a unique opportunity to reduce underlying bias and create a technology that is honest and fair to all.",
              "Leaders must make it a priority to take a closer look at the models and techniques for addressing bias in each of these stages to identify how best to implement the models across their technology.",
              "Ultimately, there is no way to completely eliminate AI bias, but it\u2019s the industry\u2019s responsibility to collaborate and help mitigate its presence in future technology.",
              "With AI playing an increasing important role in our lives, and with so much promise for future innovation, it is necessary that we acknowledge and address prejudice in our technology, as well as in our society.",
              "AI has long been enabling innovation, with both big and small impacts.",
              "From AI-generated music, to enhancing the remote fan experience at the U.",
              "S.",
              "Open, to managing coronavirus patients in hospitals, it seems like the future is limitless.",
              "But, in the last few months, organizations from all sectors have been met with the realities of both Covid-19 and increasing anxiety over social justice issues, which has led to a reckoning within companies about the areas where more innovation and better processes are required.",
              "In the AI industry, specifically, organizations need to embrace their role in ensuring a fairer and less-biased world."
            ]
          },
          {
            "title": "Mitigating AI/ML Bias in Context: Establishing Practices for Testing ...",
            "url": "https://csrc.nist.gov/pubs/pd/2022/11/09/mitigating-ai-ml-bias-in-context/final",
            "content": [
              "Mitigating AI/ML Bias in Context: Establishing Practices for Testing ...",
              "This is a potential security issue, you are being redirected to https://csrc.",
              "nist.",
              "gov.",
              "You have JavaScript disabled.",
              "This site requires JavaScript to be enabled for complete site functionality.",
              "Official websites use .",
              "gov A .",
              "gov website belongs to an official government organization in the United States.",
              "Secure .",
              "gov websites use HTTPS A lock () or https:// means you've safely connected to the .",
              "gov website.",
              "Share sensitive information only on official, secure websites.",
              "Drafts for Public Comment All Public Drafts Final Pubs FIPS (standards) Special Publications (SPs) IR (interagency/internal reports) CSWP (cybersecurity white papers) ITL Bulletins Project Descriptions Journal Articles Conference Papers Books Security & Privacy Applications Technologies Sectors Laws & Regulations Activities & Products Computer Security Division Cryptographic Technology Secure Systems and Applications Security Components and Mechanisms Security Engineering and Risk Management Security Testing, Validation, and Measurement Applied Cybersecurity Division Cybersecurity and Privacy Applications National Cybersecurity Center of Excellence (NCCoE) National Initiative for Cybersecurity Education (NICE) Contact Us Documentation Topics Date Published: November 2022 Apostol Vassilev (NIST), Harold Booth (NIST), Murugiah Souppaya (NIST) Managing bias in an AI system is critical to establishing and maintaining trust in its operation.",
              "Despite its importance, bias in AI systems remains endemic across many application domains and can lead to harmful impacts regardless of intent.",
              "Bias is also context-dependent.",
              "To tackle this complex problem, we adopt a socio-technical approach to testing, evaluation, verification, and validation (TEVV) of AI systems in context.",
              "This approach connects the technology to societal values in order to develop recommended guidance for deploying AI/ML-based decision-making applications in a sector of the industry.",
              "This project will also look at the interplay between bias and cybersecurity.",
              "The project will leverage existing commercial and open-source technology in conjunction with the NIST Dioptra, an experimentation test platform for ML datasets and models.",
              "The initial phase of the project will focus on a proof-of-concept implementation for credit underwriting decisions in the financial services sector.",
              "This project will result in a freely available NIST AI/ML Practice Guide.",
              "Managing bias in an AI system is critical to establishing and maintaining trust in its operation.",
              "Despite its importance, bias in AI systems remains endemic across many application domains and can lead to harmful impacts regardless of intent.",
              "Bias is also context-dependent.",
              "To tackle this complex problem, we adopt a socio-technical approach to testing, evaluation, verification, and validation (TEVV) of AI systems in context.",
              "This approach connects the technology to societal values in order to develop recommended guidance for deploying AI/ML-based decision-making applications in a sector of the industry.",
              "This project will also look at the interplay between bias and cybersecurity.",
              "The project will leverage existing commercial and open-source technology in conjunction with the NIST Dioptra, an experimentation test platform for ML datasets and models.",
              "The initial phase of the project will focus on a proof-of-concept implementation for credit underwriting decisions in the financial services sector.",
              "This project will result in a freely available NIST AI/ML Practice Guide.",
              "None selected Publication: Project Description (pdf) Supplemental Material: Project homepage Document History: 08/18/22: Project Description (Draft) 11/09/22: Project Description (Final) risk management artificial intelligence financial services Want updates about CSRC and our publications?",
              "Subscribe Contact Us | Our Other Offices"
            ]
          },
          {
            "title": "Measuring AI Quality: Bias, Accuracy and Benchmarking for ... - Genesys",
            "url": "https://www.genesys.com/blog/post/measuring-ai-quality-bias-accuracy-and-benchmarking-for-conversational-ai",
            "content": [
              "Measuring AI Quality: Bias, Accuracy and Benchmarking for ... - Genesys",
              "Subscribe to our free newsletter and get blog updates in your inbox Welcome back Not You?",
              "Oops!",
              "We have your personal email.",
              "Don't Show This Again.",
              "Your Genesys Blog Subscription has been confirmed!",
              "Please add genesys@email.",
              "genesys.",
              "com to your safe sender list to ensure you receive the weekly blog notifications.",
              "APAC EMEA NA / LATAM Artificial intelligence (AI) practitioners are often asked to show their work.",
              "They have to prove that their AI technology works and is on par with \u2014 or better than \u2014 an alternative AI solution.",
              "It seems like a reasonable request.",
              "But measuring AI quality is difficult at best and, in some cases, it\u2019s just impossible.",
              "There are measures that are used for testing AI \u2014 error rates, recall, lift, confidence \u2014 but many of them are meaningless without context.",
              "And with AI, the real KPI is ROI.",
              "That\u2019s not to say that all AI technology is built the same or that quality is irrelevant.",
              "The quality of your AI solution has a material impact on your ability to use AI to achieve ROI.",
              "In this blog, I\u2019ll examine AI quality benchmarks and concepts as well as some best practices.",
              "This can serve as a reference point for those at any stage of the AI implementation journey.",
              "Some expect AI to be consistently accurate.",
              "The perception is that AI will correct human flaws and, since human error is inevitable and expected, AI must be its opposite.",
              "Achieving this level of perfection is an impossible standard.",
              "Expectations need to be realistic;",
              "the best way to measure AI success is business impact.",
              "There are some things that AI can in the contact center that a human can\u2019t.",
              "For example, even if a chatbot can only answer one question, it can still answer that one question 24/7 without ever stopping for a break.",
              "If that question is important to a large percentage of customers, or a small but important customer segment, then that chatbot has value well beyond its ability to accurately understand and respond conversationally to a wide set of requests.",
              "For conversational AI, expectations of perfection are sure to disappoint.",
              "Conversational AI bots are trained on data and the quality of the underlying natural language understanding (NLU) model depends on the data set used for training and testing.",
              "You might have seen some reports that show NLU benchmarks.",
              "When reviewing the numbers, make sure you understand what data was used.",
              "Let\u2019s say Vendor A used the same training and testing data for the analysis, but vendors B and C had a different training data set.",
              "The results for Vendor A will likely outperform Vendor B and C.",
              "Vendor A is (essentially) using birth year to predict age, which is a model that is 100% accurate but likely not the best use of AI.",
              "NLU models are measured using these dimensions: In the case of conversational AI, a positive prediction is a match between what a customer said and what a customer meant.",
              "Quality analysis compares how well the NLU model understands natural language.",
              "It doesn\u2019t measure how the conversational AI responds to what has been asked.",
              "Your NLU model might be able to capture what the customers want accurately.",
              "However, your framework might not be able to connect to the systems it needs to satisfy the asks, transition the call to the right channel with context preserved or identify the right answer to the question.",
              "NLU accuracy is not a proxy for customer satisfaction or first-contact resolution.",
              "Scores are typically a point-in-time evaluation and are highly dependent on the data used for the analysis.",
              "Differences in scores might be difficult to interpret (unless you\u2019re a linguistic model expert).",
              "For example, if one NLU has a score of 79% and another has a score of 80%, what does that mean?",
              "Published comparisons often don\u2019t include \u2014 or include as very fine print \u2014 the scope of the test, how many times a test was run and rarely provide the actual data used.",
              "If you\u2019re considering an off-the-shelf, pre-trained bot, then having these benchmarks could be useful.",
              "But you may need to expand your evaluation to incorporate other factors such as the ability to personalize, analyze and optimize.",
              "More about that later.",
              "For those who are using Genesys AI to build and deploy bots, NLU models are as unique as our customers.",
              "They\u2019re trained for purpose \u2014 using data that\u2019s either customer-specific or is specific to the use case the customer is trying to solve.",
              "Standard benchmark reports wouldn\u2019t do Genesys AI justice.",
              "However, teams regularly test the performance of Genesys NLU versus others using standard corpus (Figure 1).",
              "\u201cBanking bot\u201d is trained using a standard data set that represents the many self-service requests that are typical for a bank.",
              "This test shows that Genesys NLU is on par with some third-party AI and performs better on this test than others.",
              "When we expand the test to other languages, results vary.",
              "This test was performed with the same dataset translated (by a human) to the various languages represented below.",
              "If trying to compare NLU providers based on benchmarks alone, language is an important dimension.",
              "Some NLUs work particularly well with one set of languages but not as well with others (Figure 2).",
              "Consider the results: These and many other test results are used to fine-tune the underlying components to ensure that those who are creating bots can achieve the same or better level of NLU with their bot than they would through other popular NLU options.",
              "Bot are built for real-world use, not test data sets.",
              "We need to have a feel for how a model performs on data that is representative of the actual mix of incoming end-user queries for the use case that the bot has been created to address.",
              "A \u201crepresentative\u201d test set will take a snapshot of actual (anonymized) customer utterances.",
              "This can be manual or automated.",
              "The intent distribution will be very unbalanced, since there\u2019s usually a small number of intents that are the most frequent reasons customers are initially contacting your bot.",
              "And after a bot has gone through fine-tuning, the most frequent customer utterances will be part of the model;",
              "only those that aren\u2019t addressed will remain.",
              "This type of test set measures in-the-field performance and is critical for maintaining quality over time.",
              "NLU models are the foundation for conversational AI bots.",
              "An NLU model predicts the end user\u2019s intent and extracts slots (data) by being trained using a set of example utterances, which typically consist of different ways a customer asks a question.",
              "Comprehensive bot authoring tools give the bot creator the ability to add and train the intents and slots required for their bot, while also providing analytical tools to learn how the bot is performing \u2014 and tools to improve the NLU model over time.",
              "Generic NLU models might do a good job understanding basic questions, but customer service isn\u2019t generic.",
              "The specificity that bots need to be effective comes from the training data (corpus) used during the training process.",
              "The closer the training data represents actual conversations, the better the bot will perform.",
              "One way to get better data is to use the actual conversations.",
              "A best practice would be to implement conversational AI technology with a tool that can extract intents and the utterances that represent those intents from actual conversations \u2014 from voice or digital.",
              "It\u2019s important to have a way to test the bot prior to deployment and to capture any missed intent identification post-deployment.",
              "Conversational AI quality isn\u2019t static.",
              "Bots can improve over time if there\u2019s a way to optimize them post-deployment.",
              "This can be difficult with off-the-shelf bots that need custom development.",
              "A major advantage of having an integrated bot framework is that you can course correct bots that aren\u2019t performing as expected in real time without disruption.",
              "When thinking about quality, ask about optimization.",
              "Is optimization automatic?",
              "Is there a human-in-the-loop process?",
              "How can you, as the business, \u201csee\u201d what\u2019s happening?",
              "Bias should be part of the quality discussion.",
              "Bias in AI is inevitable.",
              "What\u2019s critical isn\u2019t where bias exists (it does) \u2014 but how well the bot can recognize it and trace it back to the source.",
              "If you are using a pre-trained model (an off-the-shelf bot), you likely don\u2019t know its training set.",
              "Even a large corpus drawn from a wide set of industry content can be biased if that source represents a single geography or a specific point in time.",
              "Examples of built-in bias that have derailed AI projects are out there;",
              "many AI projects are impossible to correct enough to be practical.",
              "Bias is of particular interest for those looking to use pre-trained large language models (LLMs).",
              "The advantage of having off-the-shelf, ready-to-go large models is that they\u2019re highly conversational and have been exposed to a wide variety of conversational patterns.",
              "However, the training sets are so large that they\u2019re hard to curate, and they depend on the ability of the vendors to find and use data that\u2019s truly impartial.",
              "A recent paper shows that LLMs are partisan \u2014 which you may want to consider when asking questions about politics or events.",
              "A partisan bias could alter the information you\u2019re receiving (Figure 3).",
              "When embarking on an AI project, start with the goal and the output.",
              "What is the AI project meant to accomplish (automate) and what would the impact be to that accomplishment if the data used contains bias that will materially alter the decision?",
              "For example, a model that\u2019s used to automate loan approval or employment that\u2019s biased can yield a decision that\u2019s unethical and likely illegal in most countries.",
              "The mitigation strategy is to avoid use of data such as age, gender and racial background.",
              "But often, within the data, measures exist that are related to these protected categories that could drive the output and introduce the wrong type of bias into the model.",
              "For some sensitive decisions, such as employment, the use of AI is highly regulated and monitored (as it should be).",
              "When considering the outcome of conversational AI, bias might not have a significant impact on the conversation.",
              "While it should be considered, its impact is unlikely to have the type of business impact a biased employment model would.",
              "To evaluate and consider bias, start with the outcome and work your way back to the data.",
              "The source of bias is the training data.",
              "An employment model that was trained on data from a point in time when employment practices precluded certain groups from employments or specific roles will bring this bias forward into the modern day.",
              "Making sure data is well balanced is one way to control for bias.",
              "For conversational AI and for AI-enabled customer experience automation, use actual conversational data from your own customer base as that represents the possible conversations.",
              "Built-in analytics enable users to assess whether there is bias in how the bot is responding.",
              "Watching how intents are derived from utterances, as well as which utterances are understood and which aren\u2019t, will show if there is bias potential.",
              "A built-in feedback mechanism can help capture any issues and the optimization tools enable organizations to adjust a bot.",
              "This isn\u2019t an unattended process that can run amuck.",
              "This is very much controlled, measured and optimized with a human-in-the-loop process.",
              "This means that this can work for both highly sensitive information and for general information.",
              "Some forms of AI can only do one or the other, which will limit its efficacy.",
              "Contact centers need conversational AI that provides a quality response to your customers and advances your business objectives.",
              "The temptation to search for benchmark reports and engineering specifications might yield a lot of data that could be hard to understand and is unlikely to help you meet your goals.",
              "It\u2019s important to have a solution that has the following characteristics: Learn more about the Genesys approach to conversational AI with this video.",
              "Explore how to deliver better automated and human-assisted conversational experiences.",
              "Rahul joined Genesys in 2021 as VP of Product, AI and Digital Self-Service.",
              "He\u2019s focused on building the best conversational AI products for Genesys Cloud.",
              "Prior to Genesys, he worked.",
              ".",
              ".",
              "Topics: Share: Subscribe to our free newsletter and get blog updates in your inbox.",
              "Related capabilities: 12 minutes Every year, Genesys\u00ae orchestrates billions of remarkable customer experiences for organizations in more than 100 countries.",
              "Through the power of our cloud, digital and AI technologies, organizations can realize Experience as a Service\u00ae, our vision for empathetic customer experiences at scale.",
              "With Genesys, organizations have the power to deliver proactive, predictive, and hyper personalized experiences to deepen their customer connection across every marketing, sales, and service moment on any channel, while also improving employee productivity and engagement.",
              "By transforming back-office technology to a modern revenue velocity engine Genesys enables true intimacy at scale to foster customer trust and loyalty.",
              "Copyright \u00a9 2023 Genesys.",
              "All rights reserved.",
              "Terms of Use | Privacy Policy | Email Subscription | Accessibility Plan & Feedback | Cookie Settings"
            ]
          }
        ],
        "Synthetic data, which mimics real data but doesn't contain personal or sensitive information, can help reduce bias in AI by expanding the diversity and quality of training data and protecting privacy.": [
          {
            "title": "Reducing AI bias with Synthetic data | Towards Data Science",
            "url": "https://towardsdatascience.com/reducing-ai-bias-with-synthetic-data-7bddc39f290d",
            "content": [
              "Reducing AI bias with Synthetic data | Towards Data Science",
              "Sign up Sign in Sign up Sign in Member-only story Alexander Watson Follow Towards Data Science -- 3 Share In this post, we are going to explore using synthetic data to augment a popular health dataset on Kaggle, and then train AI and ML models that perform and generalize better, while reducing algorithmic bias.",
              "The Heart Disease dataset published by University of California Irvine is one of the top 5 datasets on the data science competition site Kaggle, with 9 data science tasks listed and 1,014+ notebook kernels created by data scientists.",
              "It is a series of health 14 attributes and is labeled with whether the patient had a heart disease or not, making it a great dataset for prediction.",
              "A quick look at the dataset shows that male patient records account for 68% of the overall dataset, with female patient records at only 32%.",
              "With a 2 to 1 ratio of male to female patients, this could result in algorithms trained on the dataset over-indexing on male symptoms and performing poor diagnoses for female patients.",
              "There is no substitute for having an equal representation of groups in training data, especially with Healthcare.",
              "In absence of that, how do we reduce biases in our input data as much as possible?",
              "\u201cBy augmenting the training set with synthetic records, can we reduce the gender bias and improve ML accuracy?",
              "\u201d To test our thesis, we will use Gretel.",
              "ai\u2019s open source synthetic data library to generate additional female patient records to attempt to compensate for the biased training data.",
              "Our hope is that this will help the classifiers improve predictions for heart disease for both male and female patients, and generalize better to unknown data.",
              "We can then run the synthetic dataset through ML algorithms on Kaggle to compare results vs. the training set.",
              "A top data science notebook on Kaggle (by forks, linked below) runs a series of 6 classification algorithms on the UCI dataset and compares the.",
              ".",
              ".",
              "-- -- 3 Towards Data Science Co-Founder at Gretel.",
              "ai, previously GM at AWS.",
              "Love artificial intelligence and security.",
              "@alexwatson405 Alexander Watson in Towards Data Science -- 2 Rahul Nayak in Towards Data Science -- 39 Anthony Alcaraz in Towards Data Science -- 8 Alexander Watson in Towards Data Science -- 3 Marco Peixeiro in Towards Data Science -- 22 Timothy Lim -- Tomaz Bratanic -- 8 Berk Orbay in DataBulls -- 7 Heinrich Peters -- 1 Rick St Germaine -- Help Status About Careers Blog Privacy Terms Text to speech Teams"
            ]
          },
          {
            "title": "SMU lab creating large synthetic datasets to reduce bias, preserve ...",
            "url": "https://www.smu.edu/News/Research/AI-facial-recognition-systems",
            "content": [
              "SMU lab creating large synthetic datasets to reduce bias, preserve ...",
              "DALLAS (SMU) \u2013 The quality of any artificial intelligence (AI) model relies on the data it is given.",
              "That is why researchers at SMU are creating large datasets to address bias and fairness issues found in facial recognition (FR) technology.",
              "There are technical limitations to collecting large-scale data with the realistic variations needed to support FR systems. Additionally, the collection, maintenance and use of biometric data drawn from real people is creating legal, ethical, and privacy concerns.",
              "The alternative?",
              "By generating facial images from text descriptions with SMU\u2019s NVIDIA DGX SuperPOD, a high performance computing platform specifically designed for AI, SMU researcher Corey Clark and his team are creating ethically sourced synthetic datasets that could greatly impact how FR algorithms recognize race and gender.",
              "It is one of the first projects for SMU\u2019s Intelligent Systems and Bias Examination Lab (ISaBEL), which is located in the AT&T Center for Virtualization and charged with understanding and mitigating bias in AI systems. If an AI model is fed datasets lacking in diversity, its algorithm ends up performing better for specific demographics than others.",
              "By generating synthetic images based on text descriptions, the SMU team is producing datasets containing hundreds of thousands of facial images that include underrepresented racial groups.",
              "This methodology gives FR systems a greater chance of being fairer and more accurately balanced.",
              "\u201cThere are constraints in trying to create a real-world based dataset to train any artificial intelligence model,\u201d said Clark, assistant professor of computer science in the Lyle School of Engineering and deputy director for Research at SMU Guildhall.",
              "\u201cTo ethically source it you must solve challenges like consent, fairness, and legal compliance.",
              "Synthetic data, generated by the SuperPOD, removes those obstacles.",
              "\u201d Existing FR technology has struggled to match the same face with different angles and poses.",
              "Using an existing stable diffusion model (an open-source AI algorithm anyone can use), SMU researchers are generating a large, diverse dataset with pose variations.",
              "Their customization of the stable diffusion model is unique due to the sheer magnitude of images created \u2013 over 1 million so far \u2013 and the special tuning of the model to specifically process facial recognition.",
              "That capacity distinguishes the model from similar text-to-image AIs.",
              "\u201cFacial recognition is here and not going away,\u201d Clark said.",
              "\u201cThe demand for these larger training datasets is crucial for improving FR systems so they provide equitable results.",
              "Through our methodology and use of the of the SuperPOD, we\u2019re generating datasets not previously easy to obtain, and doing so quickly and ethically.",
              "\u201d In 2021, SMU announced its collaboration with NVIDIA, a trailblazer in the field of accelerated computing, through the University\u2019s acquisition of an NVIDIA DGX SuperPOD, which expanded SMU's supercomputer memory capacity and led to a 25-fold increase in the speed and efficiency of AI and machine learning.",
              "Clark stressed that the massive number of images created for their datasets would not be possible without the SuperPOD, and its capabilities will have a significant role to play in further FR development.",
              "Moving forward, he and his team plan to create one of the largest balanced facial recognition data sets for research use.",
              "By addressing fairness and bias issues found in FR technology, Clark and his ISaBEL colleagues also plan to create a bias certification process that could evaluate existing companies\u2019 AI and be used to develop future models specified to need.",
              "About SMU SMU is the nationally ranked global research university in the dynamic city of Dallas.",
              "SMU\u2019s alumni, faculty and more than 12,000 students in eight degree-granting schools demonstrate an entrepreneurial spirit as they lead change in their professions, communities and the world.",
              "SMU is the nationally ranked global research university in the dynamic city of Dallas.",
              "SMU\u2019s alumni, faculty and more than 12,000 students in eight degree-granting schools demonstrate an entrepreneurial spirit as they lead change in their professions, communities and the world."
            ]
          },
          {
            "title": "In machine learning, synthetic data can offer real performance ...",
            "url": "https://news.mit.edu/2022/synthetic-data-ai-improvements-1103",
            "content": [
              "In machine learning, synthetic data can offer real performance ...",
              "Suggestions or feedback?",
              "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a Creative Commons Attribution Non-Commercial No Derivatives license.",
              "You may not alter the images provided, other than to crop them to size.",
              "A credit line must be used when reproducing images;",
              "if one is not provided below, credit the images to \"MIT.\"",
              "Previous image Next image Teaching a machine to recognize human actions has many potential applications, such as automatically detecting workers who fall at a construction site or enabling a smart home robot to interpret a user\u2019s gestures.",
              "To do this, researchers train machine-learning models using vast datasets of video clips that show humans performing actions.",
              "However, not only is it expensive and laborious to gather and label millions or billions of videos, but the clips often contain sensitive information, like people\u2019s faces or license plate numbers.",
              "Using these videos might also violate copyright or data protection laws.",
              "And this assumes the video data are publicly available in the first place \u2014 many datasets are owned by companies and aren\u2019t free to use.",
              "So, researchers are turning to synthetic datasets.",
              "These are made by a computer that uses 3D models of scenes, objects, and humans to quickly produce many varying clips of specific actions \u2014 without the potential copyright issues or ethical concerns that come with real data.",
              "But are synthetic data as \u201cgood\u201d as real data?",
              "How well does a model trained with these data perform when it\u2019s asked to classify real human actions?",
              "A team of researchers at MIT, the MIT-IBM Watson AI Lab, and Boston University sought to answer this question.",
              "They built a synthetic dataset of 150,000 video clips that captured a wide range of human actions, which they used to train machine-learning models.",
              "Then they showed these models six datasets of real-world videos to see how well they could learn to recognize actions in those clips.",
              "The researchers found that the synthetically trained models performed even better than models trained on real data for videos that have fewer background objects.",
              "This work could help researchers use synthetic datasets in such a way that models achieve higher accuracy on real-world tasks.",
              "It could also help scientists identify which machine-learning applications could be best-suited for training with synthetic data, in an effort to mitigate some of the ethical, privacy, and copyright concerns of using real datasets.",
              "\u201cThe ultimate goal of our research is to replace real data pretraining with synthetic data pretraining.",
              "There is a cost in creating an action in synthetic data, but once that is done, then you can generate an unlimited number of images or videos by changing the pose, the lighting, etc.",
              "That is the beauty of synthetic data,\u201d says Rogerio Feris, principal scientist and manager at the MIT-IBM Watson AI Lab, and co-author of a paper detailing this research.",
              "The paper is authored by lead author Yo-whan \u201cJohn\u201d Kim \u201922;",
              "Aude Oliva, director of strategic industry engagement at the MIT Schwarzman College of Computing, MIT director of the MIT-IBM Watson AI Lab, and a senior research scientist in the Computer Science and Artificial Intelligence Laboratory (CSAIL);",
              "and seven others.",
              "The research will be presented at the Conference on Neural Information Processing Systems. Building a synthetic dataset The researchers began by compiling a new dataset using three publicly available datasets of synthetic video clips that captured human actions.",
              "Their dataset, called Synthetic Action Pre-training and Transfer (SynAPT), contained 150 action categories, with 1,000 video clips per category.",
              "They selected as many action categories as possible, such as people waving or falling on the floor, depending on the availability of clips that contained clean video data.",
              "Once the dataset was prepared, they used it to pretrain three machine-learning models to recognize the actions.",
              "Pretraining involves training a model for one task to give it a head-start for learning other tasks.",
              "Inspired by the way people learn \u2014 we reuse old knowledge when we learn something new \u2014 the pretrained model can use the parameters it has already learned to help it learn a new task with a new dataset faster and more effectively.",
              "They tested the pretrained models using six datasets of real video clips, each capturing classes of actions that were different from those in the training data.",
              "The researchers were surprised to see that all three synthetic models outperformed models trained with real video clips on four of the six datasets.",
              "Their accuracy was highest for datasets that contained video clips with \u201clow scene-object bias.",
              "\u201d Low scene-object bias means that the model cannot recognize the action by looking at the background or other objects in the scene \u2014 it must focus on the action itself.",
              "For example, if the model is tasked with classifying diving poses in video clips of people diving into a swimming pool, it cannot identify a pose by looking at the water or the tiles on the wall.",
              "It must focus on the person\u2019s motion and position to classify the action.",
              "\u201cIn videos with low scene-object bias, the temporal dynamics of the actions is more important than the appearance of the objects or the background, and that seems to be well-captured with synthetic data,\u201d Feris says.",
              "\u201cHigh scene-object bias can actually act as an obstacle.",
              "The model might misclassify an action by looking at an object, not the action itself.",
              "It can confuse the model,\u201d Kim explains.",
              "Boosting performance Building off these results, the researchers want to include more action classes and additional synthetic video platforms in future work, eventually creating a catalog of models that have been pretrained using synthetic data, says co-author Rameswar Panda, a research staff member at the MIT-IBM Watson AI Lab.",
              "\u201cWe want to build models which have very similar performance or even better performance than the existing models in the literature, but without being bound by any of those biases or security concerns,\u201d he adds.",
              "They also want to combine their work with research that seeks to generate more accurate and realistic synthetic videos, which could boost the performance of the models, says SouYoung Jin, a co-author and CSAIL postdoc.",
              "She is also interested in exploring how models might learn differently when they are trained with synthetic data.",
              "\u201cWe use synthetic datasets to prevent privacy issues or contextual or social bias, but what does the model actually learn?",
              "Does it learn something that is unbiased?",
              "\u201d she says.",
              "Now that they have demonstrated this use potential for synthetic videos, they hope other researchers will build upon their work.",
              "\u201cDespite there being a lower cost to obtaining well-annotated synthetic data, currently we do not have a dataset with the scale to rival the biggest annotated datasets with real videos.",
              "By discussing the different costs and concerns with real videos, and showing the efficacy of synthetic data, we hope to motivate efforts in this direction,\u201d adds co-author Samarth Mishra, a graduate student at Boston University (BU).",
              "Additional co-authors include Hilde Kuehne, professor of computer science at Goethe University in Germany and an affiliated professor at the MIT-IBM Watson AI Lab;",
              "Leonid Karlinsky, research staff member at the MIT-IBM Watson AI Lab;",
              "Venkatesh Saligrama, professor in the Department of Electrical and Computer Engineering at BU;",
              "and Kate Saenko, associate professor in the Department of Computer Science at BU and a consulting professor at the MIT-IBM Watson AI Lab.",
              "This research was supported by the Defense Advanced Research Projects Agency LwLL, as well as the MIT-IBM Watson AI Lab and its member companies, Nexplore and Woodside.",
              "Previous item Next item Read full story \u2192 Read full story \u2192 Read full story \u2192 Read full story \u2192 Read full story \u2192 Read full story \u2192 This website is managed by the MIT News Office, part of the Institute Office of Communications.",
              "Massachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA"
            ]
          }
        ],
        "Bias in AI is a complex and evolving challenge that requires ongoing research, collaboration, and vigilance from all stakeholders involved.": []
      },
      "evidence": {
        "AI can be influenced by biases that can lead to unfair or harmful outcomes for some groups of people.": [
          "Bias Analyzer helps companies proactively identify, monitor and mitigate potential bias risks in automated decision-making systems. It allows them to self-regulate and continuously improve fairness in their AI",
          "Related One form of AI bias that has rightly gotten a lot of attention is the kind that shows up repeatedly in facial recognition systems. These models are excellent at identifying white male faces because those are the sorts of faces they\u2019ve been more commonly trained on.",
          "Studies have found mortgage algorithms charging Black and Latinx borrowers higher interest rates and egregious cases of recruiting algorithms exacerbating bias against hiring women.",
          "A series of studies about various facial recognition software found that most had misidentified darker-skinned women 37% more often than those with lighter-skin tones.",
          "If we are to develop trustworthy AI systems, we need to consider all the factors that can chip away at the public\u2019s trust in AI.",
          "Many of these factors go beyond the technology itself to the impacts of the technology, and the comments we received from a wide range of people and organizations emphasized this point.",
          "\u201d Bias in AI can harm humans.",
          "AI can make decisions that affect whether a person is admitted into a school, authorized for a bank loan or accepted as a rental applicant.",
          "It is relatively common knowledge that AI systems can exhibit biases that stem from their programming and data sources;",
          "for example, machine learning software could be trained on a dataset that underrepresents a particular gender or ethnic group.",  
          "The many ways bias can enter AI models may impact automated decision applications to become systematically unfair for certain groups of people."
        ],
        "Bias in AI can arise from various sources, such as the data used to train the algorithms, the design choices made by the developers, or the context and interpretation of the results by the users.": [
          "Managing bias in an AI system is critical to establishing and maintaining trust in its operation.",
          "Despite its importance, bias in AI systems remains endemic across many application domains and can lead to harmful impacts regardless of intent.",
          "Bias is also context-dependent.",
          "To tackle this complex problem, we adopt a socio-technical approach to testing, evaluation, verification, and validation (TEVV) of AI systems in context.",  
          "First, AI will inherit the biases that are in the training data.",
          "These perpetuated AI biases have proven to be especially harmful in several cases, such as Amazon\u2019s Sexist Hiring Algorithm that inadvertently favored male candidates and the Racist Criminological Software COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) where black defendants were 45% more likely to be assigned higher risk scores than white defendants.",
          "A more complete understanding of bias must take into account human and systemic biases, which figure significantly in the new version.",
          "When human, systemic and computational biases combine, they can form a pernicious mixture \u2014 especially when explicit guidance is lacking for addressing the risks associated with using AI systems.",
          "researchers at the National Institute of Standards and Technology (NIST) recommend widening the scope of where we look for the source of these biases \u2014 beyond the machine learning processes and data used to train AI software to the broader societal factors that influence how technology is developed.",
          "Any historical data that is used would be subject to the same societal biases at the time the data was generated."
        ],
        "Data scientists and other stakeholders should be aware of how AI can be biased and how it can affect different groups of people.": [
          "Addressing bias in AI is crucial to ensure fairness and equity in all of its applications.",
          "AI can make decisions that affect whether a person is admitted into a school, authorized for a bank loan or accepted as a rental applicant.",
          "According to NIST\u2019s Reva Schwartz, the main distinction between the draft and final versions of the publication is the new emphasis on how bias manifests itself not only in AI algorithms and the data used to train them, but also in the societal context in which AI systems are used.",
          "\u201d Bias in AI can harm humans.",
          "A more complete understanding of bias must take into account human and systemic biases, which figure significantly in the new version.",
          "If the data set used is biased, software simply reinforces the human errors and biases.",
          "As AI systems are trained on large datasets, they can inherit and perpetuate biases present in the data, which can lead to unfair or discriminatory outcomes."
        ],
        "Data scientists and other stakeholders should consider the ethical and social implications of their work and follow best practices and guidelines for responsible AI development and use.": [
          "The competitive nature of AI development poses a dilemma for organizations, as prioritizing speed may lead to neglecting ethical guidelines, bias detection, and safety measures.",
          "We propose that an \u2018embedded ethics\u2019 approach can fill this gap and promote a more ethical development of AI applications.",
          "For instance, major tech corporations are laying off their AI ethics teams precisely at a time when responsible actions are needed most.",
          "Ethical concerns around artificial intelligence (AI) technology have prompted a rush towards \u2018AI ethics\u2019 to consider how AI technology can be developed and implemented in an ethical manner1,2,3,4.",
          "It is imperative that the ethical challenges of AI are addressed as early as possible during the development process to ensure the ethically, socially and legally responsible design and implementation of these applications8.",
          "Leaders, prioritizing speed to market, are driving the current AI arms race in which major corporate players are rushing products and potentially short-changing critical considerations like ethical guidelines, bias detection, and safety measures."
        ],
        "AI systems should be able to explain how they make decisions and what factors they consider.": [],
        "Increasing transparency in AI systems can help users and auditors to understand and evaluate the logic and reliability of the algorithms, as well as to identify and correct any errors or biases.": [
          "To make the situation more challenging, the skills required to audit AI individual and cross-linked systems are emerging and scarce, leading to a patchwork quilt of capabilities.",
          "For auditors and compliance personnel, the journey of AI transparency begins not with technology, but with a clear understanding of the goals and objectives of the AI solution, its touchpoints and control limits, and finally its data and technology ecosystem.",
          "As this transformation occurs and AI scales within the enterprise, organizations should adapt governance, oversight structures, and processes to promote trust and transparency in AI models.",
          "An auditor\u2019s experience with regulators and understanding of regulatory intent and investor expectations, coupled with their independent mindset, can provide valuable insight to organizations as they respond to increased public concern and scrutiny regarding transparency and functionality of AI applications."
        ],
        "There should be clear and consistent standards for measuring and managing bias in AI, as well as for ensuring accountability and governance.": [
          "The approach the authors propose for managing bias involves a conscientious effort to identify and manage bias at different points in an AI system\u2019s lifecycle, from initial conception to design to release.",
          "NIST outlines the approach in A Proposal for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270), a new publication that forms part of the agency\u2019s broader effort to support the development of trustworthy and responsible AI.",
          "The OMB memo goes on to require a range of accountability processes, including human fallback, the mitigation of new or emerging risks to rights and safety, ongoing assessment throughout a system\u2019s lifecycle, assessment for bias, and consultation and feedback from affected groups.",
          "\u201cManaging the risk of bias in AI is a critical part of developing trustworthy AI systems, but the path to achieving this remains unclear,\u201d said NIST\u2019s Reva Schwartz, one of the report\u2019s authors.",
          "The recommendation is a core message of a revised NIST publication, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270), which reflects public comments the agency received on its draft version released last summer.",
          "In what may become a model for AI governance from localities, to states, to international governing agreements, the OMB memo, Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence, requires specific AI guardrails.",
          "As a step toward improving our ability to identify and manage the harmful effects of bias in artificial intelligence (AI) systems, researchers at the National Institute of Standards and Technology (NIST) recommend widening the scope of where we look for the source of these biases \u2014 beyond the machine learning processes and data used to train AI software to the broader societal factors that influence how technology is developed."
        ],
        "These standards should be developed with input from diverse stakeholders, including experts, regulators, civil society, and affected communities.": [
          "While the NAIAC is the appropriate next step in gathering input from various stakeholders within the private and public sectors, as well as from universities and civil society stakeholders, representatives from more inclusive and affected groups are also key to developing and executing a more resilient governance approach.",
          "As AI becomes ever more consumable and its capabilities continue to evolve at breakneck speed, so too will the implications for society as a whole.",
          "In an ideal world, government, industry and civil society should work together to ensure that AI is developed and implemented ethically.",
          "But the genie is out of the bottle, so to speak, and despite growing concern from AI pioneers and thought leaders alike, there\u2019s likely no slowing it down.",
          "Even so, there\u2019s plenty that we can do to set up some guardrails around sticky ethical considerations.",
          "Cooperation between government, industry and civil society will be crucial in order to harness the power of AI for good and avoid the pitfalls of what could go wrong."
        ],
        "AI systems should be tested for bias using various methods and tools, such as statistical analysis, simulation, or human evaluation.": [
          "Bias is also context-dependent.",
          "To tackle this complex problem, we adopt a socio-technical approach to testing, evaluation, verification, and validation (TEVV) of AI systems in context.",  
          "First, AI will inherit the biases that are in the training data.",
          "Managing bias in an AI system is critical to establishing and maintaining trust in its operation."
        ],
        "Testing should be done both before and after deploying the systems in real-world settings, and any issues or feedback should be addressed promptly.": [],
        "Synthetic data, which mimics real data but doesn't contain personal or sensitive information, can help reduce bias in AI by expanding the diversity and quality of training data and protecting privacy.": [
          "By augmenting the training set with synthetic records, can we reduce the gender bias and improve ML accuracy?",
          "Their dataset, called Synthetic Action Pre-training and Transfer (SynAPT), contained 150 action categories, with 1,000 video clips per category.",
          "SMU researcher Corey Clark and his team are creating ethically sourced synthetic datasets that could greatly impact how FR algorithms recognize race and gender.",
          "By generating synthetic images based on text descriptions, the SMU team is producing datasets containing hundreds of thousands of facial images that include underrepresented racial groups.",
          "This work could help researchers use synthetic datasets in such a way that models achieve higher accuracy on real-world tasks.",
          "The ultimate goal of our research is to replace real data pretraining with synthetic data pretraining."
        ],
        "Bias in AI is a complex and evolving challenge that requires ongoing research, collaboration, and vigilance from all stakeholders involved.": []
      },
      "subconclusions": {
        "AI can be influenced by biases that can lead to unfair or harmful outcomes for some groups of people.": "supported",
        "Bias in AI can arise from various sources, such as the data used to train the algorithms, the design choices made by the developers, or the context and interpretation of the results by the users.": "partially supported",
        "Data scientists and other stakeholders should be aware of how AI can be biased and how it can affect different groups of people.": "supported",
        "Data scientists and other stakeholders should consider the ethical and social implications of their work and follow best practices and guidelines for responsible AI development and use.": "supported",
        "AI systems should be able to explain how they make decisions and what factors they consider.": "not supported",
        "Increasing transparency in AI systems can help users and auditors to understand and evaluate the logic and reliability of the algorithms, as well as to identify and correct any errors or biases.": "partially supported",
        "There should be clear and consistent standards for measuring and managing bias in AI, as well as for ensuring accountability and governance.": "supported",
        "These standards should be developed with input from diverse stakeholders, including experts, regulators, civil society, and affected communities.": "partially supported",
        "AI systems should be tested for bias using various methods and tools, such as statistical analysis, simulation, or human evaluation.": "partially supported",
        "Testing should be done both before and after deploying the systems in real-world settings, and any issues or feedback should be addressed promptly.": "not supported",
        "Synthetic data, which mimics real data but doesn't contain personal or sensitive information, can help reduce bias in AI by expanding the diversity and quality of training data and protecting privacy.": "partially supported",
        "Bias in AI is a complex and evolving challenge that requires ongoing research, collaboration, and vigilance from all stakeholders involved.": "not supported"
      }
    }
  }
}